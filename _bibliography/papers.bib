---
---

@string{aps = {American Physical Society,}}

@book{einstein1920relativity,
  title={Relativity: the Special and General Theory},
  author={Einstein, Albert},
  year={1920},
  publisher={Methuen & Co Ltd},
  html={relativity.html}
}

@book{einstein1956investigations,
  bibtex_show={true},
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation},
  preview={brownian-motion.gif}
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}

@Article{einstein1905photoelectriceffect,
  bibtex_show={true},
  abbr={Ann. Phys.},
  title="{{\"U}ber einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichtspunkt}",
  author={Albert Einstein},
  abstract={This is the abstract text.},
  journal={Ann. Phys.},
  volume={322},
  number={6},
  pages={132--148},
  year={1905},
  doi={10.1002/andp.19053220607},
  award={Albert Einstein receveid the **Nobel Prize in Physics** 1921 *for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect*},
  award_name={Nobel Prize}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schrödinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif},
  abbr={Vision}
}


@article{Kalamara2022making,
author = {Kalamara, Eleni and Turrell, Arthur and Redl, Chris and Kapetanios, George and Kapadia, Sujit},
title = {Making text count: Economic forecasting using newspaper text},
journal = {Journal of Applied Econometrics},
volume = {37},
number = {5},
pages = {896-919},
keywords = {forecasting, machine learning, text},
doi = {https://doi.org/10.1002/jae.2907},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2907},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.2907},
abstract = {Summary This paper examines several ways to extract timely economic signals from newspaper text and shows that such information can materially improve forecasts of macroeconomic variables including GDP, inflation and unemployment. Our text is drawn from three popular UK newspapers that collectively represent UK newspaper readership in terms of political perspective and editorial style. Exploiting newspaper text can improve economic forecasts both unconditionally and when conditioning on other relevant information, but the performance of the latter varies according to the method used. Incorporating text into forecasts by combining counts of terms with supervised machine learning delivers the highest forecast improvements relative to existing text-based methods. These improvements are most pronounced during periods of economic stress when, arguably, forecasts matter most.},
year = {2022}
}

@article{shapiro2022measuring,
title = {Measuring news sentiment},
author = {Shapiro, Adam Hale and Sudhof, Moritz and Wilson, Daniel},
year = {2022},
journal = {Journal of Econometrics},
volume = {228},
number = {2},
pages = {221-243},
abstract = {This paper demonstrates state-of-the-art text sentiment analysis tools while developing a new time-series measure of economic sentiment derived from economic and financial newspaper articles from January 1980 to April 2015. We compare the predictive accuracy of a large set of sentiment analysis models using a sample of articles that have been rated by humans on a positivity/negativity scale. The results highlight the gains from combining existing lexicons and from accounting for negation. We also generate our own sentiment-scoring model, which includes a new lexicon built specifically to capture the sentiment in economic news articles. This model is shown to have better predictive accuracy than existing “off-the-shelf” models. Lastly, we provide two applications to the economic research on sentiment. First, we show that daily news sentiment is predictive of movements of survey-based measures of consumer sentiment. Second, motivated by Barsky and Sims (2012), we estimate the impulse responses of macroeconomic variables to sentiment shocks, finding that positive sentiment shocks increase consumption, output, and interest rates and dampen inflation.},
url = {https://EconPapers.repec.org/RePEc:eee:econom:v:228:y:2022:i:2:p:221-243}
}

@misc{baevski2020wav2vec2,
      title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations}, 
      author={Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},
      year={2020},
      eprint={2006.11477},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.11477}, 
}

@misc{pasad2022layerwiseanalysisselfsupervisedspeech,
      title={Layer-wise Analysis of a Self-supervised Speech Representation Model}, 
      author={Ankita Pasad and Ju-Chieh Chou and Karen Livescu},
      year={2022},
      eprint={2107.04734},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2107.04734}, 
}

@misc{belrose2023elicitinglatentpredictionstransformers,
      title={Eliciting Latent Predictions from Transformers with the Tuned Lens}, 
      author={Nora Belrose and Zach Furman and Logan Smith and Danny Halawi and Igor Ostrovsky and Lev McKinney and Stella Biderman and Jacob Steinhardt},
      year={2023},
      eprint={2303.08112},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.08112}, 
}

@article{starr2012sentiment,
author = {Starr, Martha A.},
title = {Consumption, Sentiment, And Economic News},
journal = {Economic Inquiry},
volume = {50},
number = {4},
pages = {1097-1111},
doi = {https://doi.org/10.1111/j.1465-7295.2010.00346.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1465-7295.2010.00346.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1465-7295.2010.00346.x},
abstract = {This paper investigates the influence of economic news on consumer sentiment, and examines whether “news shocks”—changes in coverage that would not be expected from incoming data on economic fundamentals—have aggregate effects. Using monthly U.S. data and a structural vector autoregression, I find that (1) sentiment is affected by news shocks; (2) after filtering out effects of news shocks, shocks to sentiment still have positive effects on consumer spending; and (3) news shocks influence both spending and unemployment in significant, though transitory ways. These results are consistent with other evidence of a role of nonfundamental factors in aggregate fluctuations. (JEL E21, E32, D12)},
year = {2012}
}

@TechReport{bernanke2006price,
  author={Ben S. Bernanke},
  title={The benefits of price stability},
  year={2006},
  institution={Board of Governors of the Federal Reserve System (U.S.)},
  type={Speech},
  url={https://ideas.repec.org/p/fip/fedgsq/171.html},
  number={171},
  abstract={a speech at the Center for Economic Policy Studies and on the occasion of the Seventy-Fifth Anniversary of the Woodrow Wilson School of Public and International Affairs, Princeton University, Princeton, New Jersey},
  keywords={Prices; Monetary policy},
  doi={},
}

@TechReport{morin2004sentiment,
title = {Consumer sentiment, the economy, and the news media},
author = {Doms, Mark and Morin, Norman J.},
year = {2004},
institution = {Federal Reserve Bank of San Francisco},
type = {Working Paper Series},
number = {2004-09},
abstract = {The news media affects consumers' perceptions of the economy through three channels. First, the news media conveys economic data and the opinions of professionals to consumers. Second, consumers receive a signal about the economy through the tone and volume of economic reporting. Last, when the volume of economic news increases, consumers are more likely to update their expectations about the economy. We find evidence that all three channels affect consumer sentiment. We derive measures of the tone and volume of economic reporting, building upon the R-word index of The Economist. We find that reporting on the economy is not always consistent with actual economic events, and, consequently, there are times during which consumer sentiment is driven away from what economic fundamentals would suggest. We find evidence that consumers update their expectations about the economy much more frequently during periods of high news coverage and that \"stickiness\" in expectations is countercyclical.},
keywords = {Consumer behavior; Economic conditions},
url = {https://EconPapers.repec.org/RePEc:fip:fedfwp:2004-09}
}

@article{shiller2017narrative,
Author = {Shiller, Robert J.},
Title = {Narrative Economics},
Journal = {American Economic Review},
Volume = {107},
Number = {4},
Year = {2017},
Month = {April},
Pages = {967–1004},
DOI = {10.1257/aer.107.4.967},
URL = {https://www.aeaweb.org/articles?id=10.1257/aer.107.4.967}}

@book{Keynes1936emp,
  added-at = {2009-06-26T15:25:19.000+0200},
  author = {Keynes, J. M.},
  biburl = {https://www.bibsonomy.org/bibtex/2c27c51d717b88bdf405c38cbfdfa65ac/butz},
  description = {diverse cognitive systems bib},
  interhash = {060c8b5ac81500224415ba44501f4164},
  intrahash = {c27c51d717b88bdf405c38cbfdfa65ac},
  keywords = {imported},
  note = {14th edition, 1973},
  publisher = {Macmillan},
  timestamp = {2009-06-26T15:25:41.000+0200},
  title = {The General Theory of Employment, Interest and Money},
  year = 1936
}

@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@article{Huang2023fin,
author = {Huang, Allen H. and Wang, Hui and Yang, Yi},
title = {FinBERT: A Large Language Model for Extracting Information from Financial Text},
journal = {Contemporary Accounting Research},
volume = {40},
number = {2},
pages = {806-841},
keywords = {deep learning, large language model, transfer learning, interpretable machine learning, sentiment classification, environment, social, and governance (ESG), apprentissage profond, grand modèle de langage, apprentissage par transfert, apprentissage automatique interprétable, classification des sentiments, environnement, social et gouvernance (ESG)},
doi = {https://doi.org/10.1111/1911-3846.12832},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1911-3846.12832},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1911-3846.12832},
abstract = {ABSTRACT We develop FinBERT, a state-of-the-art large language model that adapts to the finance domain. We show that FinBERT incorporates finance knowledge and can better summarize contextual information in financial texts. Using a sample of researcher-labeled sentences from analyst reports, we document that FinBERT substantially outperforms the Loughran and McDonald dictionary and other machine learning algorithms, including naïve Bayes, support vector machine, random forest, convolutional neural network, and long short-term memory, in sentiment classification. Our results show that FinBERT excels in identifying the positive or negative sentiment of sentences that other algorithms mislabel as neutral, likely because it uses contextual information in financial text. We find that FinBERT's advantage over other algorithms, and Google's original bidirectional encoder representations from transformers model, is especially salient when the training sample size is small and in texts containing financial words not frequently used in general texts. FinBERT also outperforms other models in identifying discussions related to environment, social, and governance issues. Last, we show that other approaches underestimate the textual informativeness of earnings conference calls by at least 18\% compared to FinBERT. Our results have implications for academic researchers, investment professionals, and financial market regulators.},
year = {2023}
}

@article{hutto2014vader, 
title={VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text},
 volume={8}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/14550}, 
DOI={10.1609/icwsm.v8i1.14550}, 
abstractNote={ &lt;p&gt; The inherent nature of social media content poses serious challenges to practical applications of sentiment analysis. We present VADER, a simple rule-based model for general sentiment analysis, and compare its effectiveness to eleven typical state-of-practice benchmarks including LIWC, ANEW, the General Inquirer, SentiWordNet, and machine learning oriented techniques relying on Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) algorithms. Using a combination of qualitative and quantitative methods, we first construct and empirically validate a gold-standard list of lexical features (along with their associated sentiment intensity measures) which are specifically attuned to sentiment in microblog-like contexts. We then combine these lexical features with consideration for five general rules that embody grammatical and syntactical conventions for expressing and emphasizing sentiment intensity. Interestingly, using our parsimonious rule-based model to assess the sentiment of tweets, we find that VADER outperforms individual human raters (F1 Classification Accuracy = 0.96 and 0.84, respectively), and generalizes more favorably across contexts than any of our benchmarks. &lt;/p&gt; },
 number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media},
  author={Hutto, C. and Gilbert, Eric}, year={2014}, month={May}, pages={216-225} }

@ARTICLE{diebold1995test,
title = {Comparing Predictive Accuracy},
author = {Diebold, Francis and Mariano, Roberto},
year = {1995},
journal = {Journal of Business & Economic Statistics},
volume = {13},
number = {3},
pages = {253-63},
abstract = {The authors propose and evaluate explicit tests of the null hypothesis of no difference in the accuracy of two competing forecasts. In contrast to previously developed tests, a wide variety of accuracy measures can be used (in particular, the loss function need not be quadratic and need not even be symmetric) and forecast errors can be non-Gaussian, nonzero mean, serially correlated, and contemporaneously correlated. Asymptotic and exact finite sample tests are proposed, evaluated, and illustrated.},
url = {https://EconPapers.repec.org/RePEc:bes:jnlbes:v:13:y:1995:i:3:p:253-63}
}

@misc{kim2018interpretabilityfeatureattributionquantitative,
      title={Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)}, 
      author={Been Kim and Martin Wattenberg and Justin Gilmer and Carrie Cai and James Wexler and Fernanda Viegas and Rory Sayres},
      year={2018},
      eprint={1711.11279},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1711.11279}, 
}

@misc{schut2023bridginghumanaiknowledgegap,
      title={Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero}, 
      author={Lisa Schut and Nenad Tomasev and Tom McGrath and Demis Hassabis and Ulrich Paquet and Been Kim},
      year={2023},
      eprint={2310.16410},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.16410}, 
}

@article{mcgrath2022az,
   title={Acquisition of chess knowledge in AlphaZero},
   volume={119},
   ISSN={1091-6490},
   url={http://dx.doi.org/10.1073/pnas.2206625119},
   DOI={10.1073/pnas.2206625119},
   number={47},
   journal={Proceedings of the National Academy of Sciences},
   publisher={Proceedings of the National Academy of Sciences},
   author={McGrath, Thomas and Kapishnikov, Andrei and Tomašev, Nenad and Pearce, Adam and Wattenberg, Martin and Hassabis, Demis and Kim, Been and Paquet, Ulrich and Kramnik, Vladimir},
   year={2022}}

@misc{hammersborg2023informationbasedexplanationmethods,
      title={Information based explanation methods for deep learning agents -- with applications on large open-source chess models}, 
      author={Patrik Hammersborg and Inga Strümke},
      year={2023},
      eprint={2309.09702},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.09702}, 
}

@article{lovering2022evaluation,
  title={Evaluation beyond task performance: analyzing concepts in AlphaZero in Hex},
  author={Lovering, Charles and Forde, Jessica and Konidaris, George and Pavlick, Ellie and Littman, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25992--26006},
  year={2022}
}

@article{jenner2024evidence,
  title={Evidence of Learned Look-Ahead in a Chess-Playing Neural Network},
  author={Jenner, Erik and Kapur, Shreyas and Georgiev, Vasil and Allen, Cameron and Emmons, Scott and Russell, Stuart},
  journal={arXiv preprint arXiv:2406.00877},
  year={2024}
}

@misc{mini2023understandingcontrollingmazesolvingpolicy,
      title={Understanding and Controlling a Maze-Solving Policy Network}, 
      author={Ulisse Mini and Peli Grietzer and Mrinank Sharma and Austin Meek and Monte MacDiarmid and Alexander Matt Turner},
      year={2023},
      eprint={2310.08043},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.08043}, 
}

@misc{li2024emergentworldrepresentationsexploring,
      title={Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task}, 
      author={Kenneth Li and Aspen K. Hopkins and David Bau and Fernanda Viégas and Hanspeter Pfister and Martin Wattenberg},
      year={2024},
      eprint={2210.13382},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.13382}, 
}

@misc{nanda2023emergentlinearrepresentationsworld,
      title={Emergent Linear Representations in World Models of Self-Supervised Sequence Models}, 
      author={Neel Nanda and Andrew Lee and Martin Wattenberg},
      year={2023},
      eprint={2309.00941},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.00941}, 
}

@article{karvonen2024emergent,
  title={Emergent world models and latent variable estimation in chess-playing language models},
  author={Karvonen, Adam},
  journal={arXiv preprint arXiv:2403.15498},
  year={2024}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}

@misc{das2023state2explanationconceptbasedexplanationsbenefit,
      title={State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding}, 
      author={Devleena Das and Sonia Chernova and Been Kim},
      year={2023},
      eprint={2309.12482},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.12482}, 
}

@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

@article{cammarata2020curve,
  author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
  title = {Curve Detectors},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/curve-detectors},
  doi = {10.23915/distill.00024.003}
}

@misc{wang2022interpretabilitywildcircuitindirect,
      title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small}, 
      author={Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
      year={2022},
      eprint={2211.00593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.00593}, 
}

@misc{du2023inside,
  title={Inside the mind of a superhuman Go model: How does Leela Zero read ladders?},
  author={Haoxing, Du},
  year={2023},
  url={https://www.alignmentforum.org/posts/FF8i6SLfKb4g7C4EL/inside-the-mind-of-a-superhuman-go-model-how-does-leela-zero-2}
}

@misc{bloom2023decision,
  title={Decision Transformer Interpretability},
  author={Bloom, Joseph and Colognese, Paul},
  year={2023},
  url={https://www.alignmentforum.org/posts/bBuBDJBYHt39Q5zZy/decision-transformer-interpretability}
}

@inproceedings{rupprecht2020finding,
  title={Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents},
  author={Rupprecht, Christian and Ibrahim, Cyril and Pal, Christopher J},
  booktitle={International Conference on Learning Representations},
  year="2020"
}

@article{sequeira2020interestingness,
  title={Interestingness elements for explainable reinforcement learning: Understanding agents' capabilities and limitations},
  author={Sequeira, Pedro and Gervasio, Melinda},
  journal={Artificial Intelligence},
  volume={288},
  pages={103367},
  year={2020},
  publisher={Elsevier},
}

@inproceedings{zahavy2016graying,
  title={Graying the black box: Understanding dqns},
  author={Zahavy, Tom and Ben-Zrihem, Nir and Mannor, Shie},
  booktitle={International conference on machine learning},
  pages={1899--1908},
  year={2016},
  organization={PMLR}
}

@inproceedings{
deshmukh2023explaining,
title={Explaining {RL} Decisions with Trajectories},
author={Shripad Vilasrao Deshmukh and Arpan Dasgupta and Balaji Krishnamurthy and Nan Jiang and Chirag Agarwal and Georgios Theocharous and Jayakumar Subramanian},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=5Egggz1q575}
}

@inproceedings{simonyan2014deep,
  title={Deep inside convolutional networks: visualising image classification models and saliency maps},
  author={Simonyan, K and Vedaldi, A and Zisserman, A},
  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2014},
  organization={ICLR}
}

@inproceedings{weitkamp2019visual,
  title={Visual rationalizations in deep reinforcement learning for atari games},
  author={Weitkamp, Laurens and van der Pol, Elise and Akata, Zeynep},
  booktitle={Artificial Intelligence: 30th Benelux Conference, BNAIC 2018,‘s-Hertogenbosch, The Netherlands, November 8--9, 2018, Revised Selected Papers 30},
  pages={151--165},
  year={2019},
  organization={Springer}
}

@inproceedings{joo2019visualization,
  title={Visualization of deep reinforcement learning using grad-CAM: how AI plays atari games?},
  author={Joo, Ho-Taek and Kim, Kyung-Joong},
  booktitle={2019 IEEE conference on games (CoG)},
  pages={1--2},
  year={2019},
  organization={IEEE}
}

@inproceedings{greydanus2018visualizing,
  title={Visualizing and understanding atari agents},
  author={Greydanus, Samuel and Koul, Anurag and Dodge, Jonathan and Fern, Alan},
  booktitle={International conference on machine learning},
  pages={1792--1801},
  year={2018},
  organization={PMLR}
}

@inproceedings{iyer2018transparency,
  title={Transparency and explanation in deep reinforcement learning neural networks},
  author={Iyer, Rahul and Li, Yuezhang and Li, Huao and Lewis, Michael and Sundar, Ramitha and Sycara, Katia},
  booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={144--150},
  year={2018}
}

@inproceedings{puri2020explain,
  title={Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution},
  author={Puri, Nikaash and Verma, Sukriti and Gupta, Piyush and Kayastha, Dhruv and Deshmukh, Shripad and Krishnamurthy, Balaji and Singh, Sameer},
  booktitle={International Conference on Learning Representations},
year={2020}
}

@article{hilton2020understanding,
  author = {Hilton, Jacob and Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris},
  title = {Understanding RL Vision},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/understanding-rl-vision},
  doi = {10.23915/distill.00029}
}

@inproceedings{
atrey2020exploratory,
title={Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning},
author={Akanksha Atrey and Kaleigh Clary and David Jensen},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkl3m1BFDB}
}

@inproceedings{wang2016dueling,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle={International conference on machine learning},
  pages={1995--2003},
  year={2016},
  organization={PMLR}
}

@article{adebayo2018sanity,
  title={Sanity checks for saliency maps},
  author={Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{kindermans2019reliability,
  title={The (un) reliability of saliency methods},
  author={Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Sch{\"u}tt, Kristof T and D{\"a}hne, Sven and Erhan, Dumitru and Kim, Been},
  journal={Explainable AI: Interpreting, explaining and visualizing deep learning},
  pages={267--280},
  year={2019},
  publisher={Springer}
}

@article{silver2018alphazero,
    author = {David Silver  and Thomas Hubert  and Julian Schrittwieser  and Ioannis Antonoglou  and Matthew Lai  and Arthur Guez  and Marc Lanctot  and Laurent Sifre  and Dharshan Kumaran  and Thore Graepel  and Timothy Lillicrap  and Karen Simonyan  and Demis Hassabis },
    title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
    journal = {Science},
    volume = {362},
    number = {6419},
    pages = {1140-1144},
    year = {2018},
    doi = {10.1126/science.aar6404},
    URL = {https://www.science.org/doi/abs/10.1126/science.aar6404},
    eprint = {https://www.science.org/doi/pdf/10.1126/science.aar6404}
}

@article{valmeekam2023planning,
  title={On the planning abilities of large language models-a critical investigation},
  author={Valmeekam, Karthik and Marquez, Matthew and Sreedharan, Sarath and Kambhampati, Subbarao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={75993--76005},
  year={2023}
}

@inproceedings{guez2019investigation,
  title={An investigation of model-free planning},
  author={Guez, Arthur and Mirza, Mehdi and Gregor, Karol and Kabra, Rishabh and Racani{\`e}re, S{\'e}bastien and Weber, Th{\'e}ophane and Raposo, David and Santoro, Adam and Orseau, Laurent and Eccles, Tom and others},
  booktitle={International Conference on Machine Learning},
  pages={2464--2473},
  year={2019},
  organization={PMLR}
}

@misc{
  garriga-alonso2024planning,
  title={Pacing Outside the Box: RNNs Learn to Plan in Sokoban},
  author={Adri{\`a} Garriga-Alonso and Mohammad Taufeeque and Adam Gleave and ChengCheng},
  year={2024},
  url={https://www.alignmentforum.org/posts/CLijBSGKi6bbSHw46/pacing-outside-the-box-rnns-learn-to-plan-in-sokoban},
  rl={https://openreview.net/forum?id=DzGe40glxs}
}

@inproceedings{
  2024interpreting,
  title={Interpreting Emergent Planning in Model-Free Reinforcement Learning},
  author={Bush, Thomas and Chung, Stephen and Anwar, Usman and Garriga-Alonso, Adri{\`a} and Krueger, David},
  booktitle={The Thirteenth International Conference on Learning Representations (Oral)},
  year={2025},
  url={https://openreview.net/forum?id=DzGe40glxs},
  selected={true},
  preview={emgplanning_titlegif.gif},
  code={https://github.com/tuphs28/emergent-planning/tree/main},
  website={https://tuphs28.github.io/projects/interpplanning/},
  abstract={
  We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in agent's representations) have causal effect on agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, enabling improved diagnosis, interpretation, and control of agent planning processes.
  },
  keywords = {my-paper},
}