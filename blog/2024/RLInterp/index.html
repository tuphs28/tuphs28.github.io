<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Post-Hoc Approaches to Interpreting Reinforcement Learning Agents | Tom Bush </title> <meta name="author" content="Tom Bush"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tuphs28.github.io/blog/2024/RLInterp/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tom</span> Bush </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Post-Hoc Approaches to Interpreting Reinforcement Learning Agents</h1> <p class="post-meta"> Created in September 01, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning,</a>   <a href="/blog/tag/rl"> <i class="fa-solid fa-hashtag fa-sm"></i> rl,</a>   <a href="/blog/tag/interpretability"> <i class="fa-solid fa-hashtag fa-sm"></i> interpretability</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>The field of interpretability seeks to explain the behaviour of neural networks. Post-hoc interpretability is the sub-field of interpretability that seeks to explain the behaviour of models without altering them to make such an explanation easier (in this post, I will use interpretability to mean post-hoc interpretability though it should definitely be noted that there are many other, equally important approaches to interpretting neural networks). In the course of writing my thesis, I read a <em>lot</em> of papers on interpretting RL agents, and this post is a quick summary of work in this field. The aim of this post is to give a quick overview of some common approaches to interpretting RL agents, and examples of papers apply each approach for anyone to read if interested. As such, this post is <em>not</em> a complete literature review and I am sure I have missed out some important pieces of work.</p> <p>Most modern methods for interpreting RL agents (in a post-hoc manner) can, roughly, be seen as falling into one of four categories: (1) concept-based interpretability; (2) mechanistic interpretability; (3) example-based interpretability; and (4) attribution-based interpretability. This division isn’t perfect (it leaves out some stuff at the boundary of what counts as a post-hoc explanation e.g. decision trees and related methods) but feels like a good enough typology for the purposes of quickly getting to grips with the field.</p> <h3 id="concept-based-interpretability">Concept-Based Interpretability</h3> <p>Concept-based approaches to interpretability explain neural network behaviours in terms of the concepts that networks learn to internally represent <a class="citation" href="#kim2018interpretabilityfeatureattributionquantitative">(Kim et al., 2018)</a>. A concept is an interpretable feature of an input to a model such as, when investigating a chess-playing RL agent, a chess board having some number of rooks remaining. A concept can be discrete or continuous, though we focus on discrete-valued concepts. Concept-based interpretability approaches determine which concepts a model internally represents by investigating which input features can be reverse-engineered from model activations.</p> <p>More concretely, concept-based interpretability works as follows. Suppose we are interested in whether a model internally represents a discrete concept \(Q\) within a component \(l\) such as its \(l\)-th feedforward layer. Let \(Q\) take one of \(P\) values in the set \(\Lambda_Q=\{q_1, \cdots, q_P\}\) for every possible model input \(x \in X\). The concept \(Q\) is typically then defined as a mapping \(Q: X \rightarrow \Lambda_Q\) that maps every input \(x\) to the value taken by the concept on that input, \(Q(x) \in \Lambda_Q\). Further, denote the activations at component \(l\) of the model on a forward pass on an input \(x_i\) as \(c_l^i \in \mathbb{R}^{D_l}\). Concept-based interpretability methods determine whether the concept \(Q\) is represented in the activations by introducing a low-capacity auxiliary classifier \(p^l: \mathbb{R}^{D_l} \rightarrow \Lambda_Q\). If \(p^l\) can successfully classify concept value \(Q(x_i)\) of inputs \(x_i\) based on the activations \(c_l^i\), we say that the concept \(Q\) is represented within the model activations at component \(l\). The intuition for this is that, since \(p^l\) is low-capacity, its ability to discriminate based on activations must come from the concept being represented within those activations.</p> <p>There are different options for \(p^l\), but the most common approach is to use a supervised linear classifier <a class="citation" href="#alain2016understanding">(Alain &amp; Bengio, 2016)</a>. The linear classifier \(p^l\) is then referred to as a linear probe, and computes a distribution over concept values for a given vector of activations \(c_l^i\) by projecting the activations along learned directions and passing the resulting logits through a softmax. To obtain a trained linear probe, a dataset of model activations labelled according to the concept value of the input they correspond to is collected and the probe is trained in a standard supervised fashion. Importantly, the directions learned by a linear probe can be viewed as the distributed representations of the concept of interest. Thus, references to learned representations of a concept are references to directions in a model’s activation space that a linear probe learns when decoding that concept.</p> <p>Concept-based interpretability has been extensively applied to interpret AlphaZero-style agents. For instance, AlphaZero has been found to utilise an array of human, and super-human, chess concepts <a class="citation" href="#schut2023bridginghumanaiknowledgegap">(Schut et al., 2023; McGrath et al., 2022)</a>. Similar results have been found when interpreting the open-source LeelaZero <a class="citation" href="#hammersborg2023informationbasedexplanationmethods">(Hammersborg &amp; Strümke, 2023)</a>, and when investigating AlphaZero’s behaviour in Hex <a class="citation" href="#lovering2022evaluation">(Lovering et al., 2022)</a>. AlphaZero has additionally been found to compute concepts relating to future moves when playing chess <a class="citation" href="#jenner2024evidence">(Jenner et al., 2024)</a>. However, concept-based interpretability has been less extensively applied to model-free RL agents. An exception to this is recent work (that, rather than using linear probes manually inspected convolutional channels to locate representation) that found the concept of a goal within a maze-solving model-free RL agent <a class="citation" href="#mini2023understandingcontrollingmazesolvingpolicy">(Mini et al., 2023)</a>. Another application of concept-based interpretability to model-free RL agents investigates using concept representations decoded by probes to provide natural language explanations of agent behaviour <a class="citation" href="#das2023state2explanationconceptbasedexplanationsbenefit">(Das et al., 2023)</a>. In relevant work in the supervised learning context, concept-based interpretability has been applied to locate representations of apparent world models in language models trained to predict transcripts of board games <a class="citation" href="#li2024emergentworldrepresentationsexploring">(Li et al., 2024; Nanda et al., 2023; Karvonen, 2024)</a>.</p> <h3 id="mechanistic-interpretability">Mechanistic Interpretability</h3> <p>Mechanistic interpretability aims to reverse engineer the computations performed by specific model components with the aim of finding “circuits”, or, groups of model components responsible for certain behaviours <a class="citation" href="#olah2020zoom">(Olah et al., 2020)</a>. Whilst similar to representation engineering due to the common focus on model internals, mechanistic interpretability can be distinguished from representation engineering in that its focus is on reverse engineering individual model components - such as individual neurons or attention heads - as opposed to locating meaningful distributed representations.</p> <p>Mechanistic interpretability includes an array of different methods. A full description of all methods in mech interp would take way too long, but a method of particular interest is activation patching. In activation patching, the activations of a set of model components are modified to some counterfactual value and the change in model output is noted when patching in these counterfactual activations. If altering specific components causes the model output to consistently change on some connected set of inputs, the model’s behaviour on those inputs is attributed to the altered model components. These counterfactual activations can be drawn from a forward pass on some similar-but-importantly-different input, the mean activations from some validation set, or just zero activations.</p> <p>In supervised learning, mechanistic interpretability methods have successfully located circuits for behaviours like curve detection in vision models <a class="citation" href="#cammarata2020curve">(Cammarata et al., 2020)</a> and indirect object identification in language models <a class="citation" href="#wang2022interpretabilitywildcircuitindirect">(Wang et al., 2022)</a>. While mechanistic interpretability has primarily been applied in the supervised learning context, recent work has been successfully applied to identify model components responsible for specific behaviours model-based planners in the games of Go and Chess <a class="citation" href="#du2023inside">(Haoxing, 2023; Jenner et al., 2024)</a>, and model-free agents <a class="citation" href="#bloom2023decision">(Bloom &amp; Colognese, 2023)</a>. Applying mechanistic interpretability analysis to RL agents seems like a really cool area and is something I am (hopefully) going to look into more at some point soon.</p> <h3 id="example-based-interpretability">Example-Based Interpretability</h3> <p>Example-based interpretability methods seeks to explain the behaviour of RL agents by providing examples of trajectories, transitions or states that are particularly insightful regarding the agent’s behaviour. A popular method here is to construct some quantitative measure of the “interestingness” of a transition - defined in terms of features such as being observed incredibly frequently or being assigned an abnormally low value by a value function - which can be used this to generate examples of transitions that illustrate some informative aspect of the agent’s behaviour <a class="citation" href="#sequeira2020interestingness">(Sequeira &amp; Gervasio, 2020; Rupprecht et al., 2020)</a>. These examples can then be used to build up a qualitative description of the agent’s behaviour. Other alternative example-based approaches exist, such as determining which training tractories are crucial for some learned behaviour <a class="citation" href="#deshmukh2023explaining">(Deshmukh et al., 2023)</a> and clustering sequences of transitions to locate clusters corresponding to behaviours <a class="citation" href="#zahavy2016graying">(Zahavy et al., 2016)</a>. Note that in all of these methods, the focus is on generating a qualitative understanding of the agent’s behaviour.</p> <h3 id="attribution-based-interpretability">Attribution-Based Interpretability</h3> <p>Attribution-based interpretability seeks to determine which features in an agent’s observation are important for agent behaviour. Importance is typically characterised by the production of a saliency map <a class="citation" href="#simonyan2014deep">(Simonyan et al., 2014)</a>, which is a heat map over the input that activates strongly over input components judged as important. Saliency maps in RL can be constructed by using gradient-based methods <a class="citation" href="#weitkamp2019visual">(Weitkamp et al., 2019; Wang et al., 2016)</a> or input perturbation-based methods <a class="citation" href="#iyer2018transparency">(Iyer et al., 2018; Puri et al., 2020)</a>. Saliency maps have be used for purposes ranging from explaining policy failures by highlighting when agents perform poorly due to focusing on the “wrong” parts of the environment <a class="citation" href="#greydanus2018visualizing">(Greydanus et al., 2018; Hilton et al., 2020)</a> to explaining learned strategies by illustrating what agents focus on when performing certain actions <a class="citation" href="#iyer2018transparency">(Iyer et al., 2018; Puri et al., 2020)</a>.Attribution-based methods are very popular in interpreting RL agents. However, they have been shown to provide misleading explanations of agent behaviour in RL <a class="citation" href="#atrey2020exploratory">(Atrey et al., 2020; Puri et al., 2020)</a> and of model behaviour more generally <a class="citation" href="#adebayo2018sanity">(Adebayo et al., 2018; Kindermans et al., 2019)</a>. Thus, they are (probably) best used in for exploratory purposes to generate hypotheses about agent behaviour which can then be investigated more rigourously using alternative methods.</p> </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="jenner2024evidence" class="col-sm-8"> <div class="title">Evidence of Learned Look-Ahead in a Chess-Playing Neural Network</div> <div class="author"> Erik Jenner, Shreyas Kapur, Vasil Georgiev, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Cameron Allen, Scott Emmons, Stuart Russell' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2406.00877</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="li2024emergentworldrepresentationsexploring" class="col-sm-8"> <div class="title">Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task</div> <div class="author"> Kenneth Li, Aspen K. Hopkins, David Bau, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="karvonen2024emergent" class="col-sm-8"> <div class="title">Emergent world models and latent variable estimation in chess-playing language models</div> <div class="author"> Adam Karvonen </div> <div class="periodical"> <em>arXiv preprint arXiv:2403.15498</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="schut2023bridginghumanaiknowledgegap" class="col-sm-8"> <div class="title">Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero</div> <div class="author"> Lisa Schut, Nenad Tomasev, Tom McGrath, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Demis Hassabis, Ulrich Paquet, Been Kim' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="hammersborg2023informationbasedexplanationmethods" class="col-sm-8"> <div class="title">Information based explanation methods for deep learning agents – with applications on large open-source chess models</div> <div class="author"> Patrik Hammersborg, and Inga Strümke </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="mini2023understandingcontrollingmazesolvingpolicy" class="col-sm-8"> <div class="title">Understanding and Controlling a Maze-Solving Policy Network</div> <div class="author"> Ulisse Mini, Peli Grietzer, Mrinank Sharma, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Austin Meek, Monte MacDiarmid, Alexander Matt Turner' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="das2023state2explanationconceptbasedexplanationsbenefit" class="col-sm-8"> <div class="title">State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding</div> <div class="author"> Devleena Das, Sonia Chernova, and Been Kim </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="nanda2023emergentlinearrepresentationsworld" class="col-sm-8"> <div class="title">Emergent Linear Representations in World Models of Self-Supervised Sequence Models</div> <div class="author"> Neel Nanda, Andrew Lee, and Martin Wattenberg </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="du2023inside" class="col-sm-8"> <div class="title">Inside the mind of a superhuman Go model: How does Leela Zero read ladders?</div> <div class="author"> Du Haoxing </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="bloom2023decision" class="col-sm-8"> <div class="title">Decision Transformer Interpretability</div> <div class="author"> Joseph Bloom, and Paul Colognese </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="deshmukh2023explaining" class="col-sm-8"> <div class="title">Explaining RL Decisions with Trajectories</div> <div class="author"> Shripad Vilasrao Deshmukh, Arpan Dasgupta, Balaji Krishnamurthy, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Nan Jiang, Chirag Agarwal, Georgios Theocharous, Jayakumar Subramanian' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In The Eleventh International Conference on Learning Representations </em>, 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="mcgrath2022az" class="col-sm-8"> <div class="title">Acquisition of chess knowledge in AlphaZero</div> <div class="author"> Thomas McGrath, Andrei Kapishnikov, Nenad Tomašev, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Adam Pearce, Martin Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, Vladimir Kramnik' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>Proceedings of the National Academy of Sciences</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="lovering2022evaluation" class="col-sm-8"> <div class="title">Evaluation beyond task performance: analyzing concepts in AlphaZero in Hex</div> <div class="author"> Charles Lovering, Jessica Forde, George Konidaris, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Ellie Pavlick, Michael Littman' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang2022interpretabilitywildcircuitindirect" class="col-sm-8"> <div class="title">Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small</div> <div class="author"> Kevin Wang, Alexandre Variengien, Arthur Conmy, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Buck Shlegeris, Jacob Steinhardt' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="olah2020zoom" class="col-sm-8"> <div class="title">Zoom In: An Introduction to Circuits</div> <div class="author"> Chris Olah, Nick Cammarata, Ludwig Schubert, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Gabriel Goh, Michael Petrov, Shan Carter' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Distill</em>, 2020 </div> <div class="periodical"> https://distill.pub/2020/circuits/zoom-in </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="cammarata2020curve" class="col-sm-8"> <div class="title">Curve Detectors</div> <div class="author"> Nick Cammarata, Gabriel Goh, Shan Carter, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Ludwig Schubert, Michael Petrov, Chris Olah' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Distill</em>, 2020 </div> <div class="periodical"> https://distill.pub/2020/circuits/curve-detectors </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="sequeira2020interestingness" class="col-sm-8"> <div class="title">Interestingness elements for explainable reinforcement learning: Understanding agents’ capabilities and limitations</div> <div class="author"> Pedro Sequeira, and Melinda Gervasio </div> <div class="periodical"> <em>Artificial Intelligence</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="rupprecht2020finding" class="col-sm-8"> <div class="title">Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents</div> <div class="author"> Christian Rupprecht, Cyril Ibrahim, and Christopher J Pal </div> <div class="periodical"> <em>In International Conference on Learning Representations</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="puri2020explain" class="col-sm-8"> <div class="title">Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution</div> <div class="author"> Nikaash Puri, Sukriti Verma, Piyush Gupta, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Dhruv Kayastha, Shripad Deshmukh, Balaji Krishnamurthy, Sameer Singh' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In International Conference on Learning Representations</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="hilton2020understanding" class="col-sm-8"> <div class="title">Understanding RL Vision</div> <div class="author"> Jacob Hilton, Nick Cammarata, Shan Carter, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Gabriel Goh, Chris Olah' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Distill</em>, 2020 </div> <div class="periodical"> https://distill.pub/2020/understanding-rl-vision </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="atrey2020exploratory" class="col-sm-8"> <div class="title">Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning</div> <div class="author"> Akanksha Atrey, Kaleigh Clary, and David Jensen </div> <div class="periodical"> <em>In International Conference on Learning Representations</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="weitkamp2019visual" class="col-sm-8"> <div class="title">Visual rationalizations in deep reinforcement learning for atari games</div> <div class="author"> Laurens Weitkamp, Elise Pol, and Zeynep Akata </div> <div class="periodical"> <em>In Artificial Intelligence: 30th Benelux Conference, BNAIC 2018,‘s-Hertogenbosch, The Netherlands, November 8–9, 2018, Revised Selected Papers 30</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="kindermans2019reliability" class="col-sm-8"> <div class="title">The (un) reliability of saliency methods</div> <div class="author"> Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Maximilian Alber, Kristof T Schütt, Sven Dähne, Dumitru Erhan, Been Kim' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Explainable AI: Interpreting, explaining and visualizing deep learning</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="kim2018interpretabilityfeatureattributionquantitative" class="col-sm-8"> <div class="title">Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)</div> <div class="author"> Been Kim, Martin Wattenberg, Justin Gilmer, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Carrie Cai, James Wexler, Fernanda Viegas, Rory Sayres' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="iyer2018transparency" class="col-sm-8"> <div class="title">Transparency and explanation in deep reinforcement learning neural networks</div> <div class="author"> Rahul Iyer, Yuezhang Li, Huao Li, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Michael Lewis, Ramitha Sundar, Katia Sycara' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="greydanus2018visualizing" class="col-sm-8"> <div class="title">Visualizing and understanding atari agents</div> <div class="author"> Samuel Greydanus, Anurag Koul, Jonathan Dodge, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alan Fern' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In International conference on machine learning</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="adebayo2018sanity" class="col-sm-8"> <div class="title">Sanity checks for saliency maps</div> <div class="author"> Julius Adebayo, Justin Gilmer, Michael Muelly, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Ian Goodfellow, Moritz Hardt, Been Kim' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Advances in neural information processing systems</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="alain2016understanding" class="col-sm-8"> <div class="title">Understanding intermediate layers using linear classifier probes</div> <div class="author"> Guillaume Alain, and Yoshua Bengio </div> <div class="periodical"> <em>arXiv preprint arXiv:1610.01644</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="zahavy2016graying" class="col-sm-8"> <div class="title">Graying the black box: Understanding dqns</div> <div class="author"> Tom Zahavy, Nir Ben-Zrihem, and Shie Mannor </div> <div class="periodical"> <em>In International conference on machine learning</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang2016dueling" class="col-sm-8"> <div class="title">Dueling network architectures for deep reinforcement learning</div> <div class="author"> Ziyu Wang, Tom Schaul, Matteo Hessel, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Hado Hasselt, Marc Lanctot, Nando Freitas' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In International conference on machine learning</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="simonyan2014deep" class="col-sm-8"> <div class="title">Deep inside convolutional networks: visualising image classification models and saliency maps</div> <div class="author"> K Simonyan, A Vedaldi, and A Zisserman </div> <div class="periodical"> <em>In Proceedings of the International Conference on Learning Representations (ICLR)</em>, 2014 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Finetuning-W2V/">A Very Brief Investigation of Using Finetuning To Interpret Wav2Vec 2.0 </a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Basic-MCMC/">Basic MCMC Pt. 2: The Metropolis-Hastings Algorithm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Bandit-Algorithms/">Bandit Algorithms (&amp; The Exploration-Exploitation Tradeoff)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Dropout-In-Recurrent-Models/">Variational Dropout in Recurrent Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Exploring-Tradeoffs-Between-Safety-Metrics/">Exploring Tradeoffs Between Safety Metrics with MNIST</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Tom Bush. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"A collection of cool AI-ish projects I have worked on that seemed to big for blog posts.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-post-hoc-approaches-to-interpreting-reinforcement-learning-agents",title:"Post-Hoc Approaches to Interpreting Reinforcement Learning Agents",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/RLInterp/"}},{id:"post-a-very-brief-investigation-of-using-finetuning-to-interpret-wav2vec-2-0",title:"A Very Brief Investigation of Using Finetuning To Interpret Wav2Vec 2.0",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Finetuning-W2V/"}},{id:"post-basic-mcmc-pt-2-the-metropolis-hastings-algorithm",title:"Basic MCMC Pt. 2: The Metropolis-Hastings Algorithm",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Basic-MCMC/"}},{id:"post-bandit-algorithms-amp-the-exploration-exploitation-tradeoff",title:"Bandit Algorithms (&amp; The Exploration-Exploitation Tradeoff)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Bandit-Algorithms/"}},{id:"post-variational-dropout-in-recurrent-models",title:"Variational Dropout in Recurrent Models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Dropout-In-Recurrent-Models/"}},{id:"post-exploring-tradeoffs-between-safety-metrics-with-mnist",title:"Exploring Tradeoffs Between Safety Metrics with MNIST",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Exploring-Tradeoffs-Between-Safety-Metrics/"}},{id:"post-basic-mcmc-pt-1-an-intro-to-monte-carlo-methods",title:"Basic MCMC Pt. 1: An Intro to Monte Carlo Methods",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Monte-Carlo-Methods/"}},{id:"news-i-submitted-my-mphil-thesis",title:"I submitted my MPhil thesis!",description:"",section:"News"},{id:"projects-improve-inflation-forecasts-with-bert",title:"Improve Inflation Forecasts With BERT",description:"Using a fine-tuned language model to extract measures of economic activity in order to improve inflation forecasts.",section:"Projects",handler:()=>{window.location.href="/projects/transfer-learning-inflation/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%6F%75@%65%78%61%6D%70%6C%65.%63%6F%6D","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>