<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://tuphs28.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tuphs28.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-01T07:14:27+00:00</updated><id>https://tuphs28.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Very Brief Investigation of Using Finetuning To Interpret Wav2Vec 2.0</title><link href="https://tuphs28.github.io/blog/2024/Finetuning-W2V/" rel="alternate" type="text/html" title="A Very Brief Investigation of Using Finetuning To Interpret Wav2Vec 2.0"/><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>https://tuphs28.github.io/blog/2024/Finetuning-W2V</id><content type="html" xml:base="https://tuphs28.github.io/blog/2024/Finetuning-W2V/"><![CDATA[<p>In recent years there has been an explosion in the use of foundation models in automatic speech recognition (ASR). In this post, I overview some brief experiments involving finetuning a specific foundation model – Wav2Vec 2.0 – for the task of ASR on the LibriSpeech-10hour dataset. The aim of this post is to show basic methods can be used as a first step in interpretting large models.</p> <h3 id="a-brief-overview-of-wav2vec-20">A Brief Overview of Wav2Vec 2.0</h3> <p>Foundation models are models that are trained using self-supervised learning on huge amounts of unlabelled data in a process known as pre-training. Self-supervised learning refers to the fact that the objectives foundation models are trained on during pre-training do not require human annotations of the training data. In the context of ASR-related tasks – where human annotation is notably expensive relative to other deep learning areas – this is hugely useful as it allows foundation models to be trained on many orders of magnitude more data than is the case with standard supervised learning of ASR systems. The idea is that by pre-training foundation models on huge amounts of audio data, the foundation models will learn useful, general representations. We can then further train foundation models on a downstream application of interest in a process called fine-tuning. Fine-tuning, therefore, can be seen as finding a way to utilise (and perhaps slightly modify) these powerful representations for the task at hand.</p> <p>Wav2Vec2.0-Base (which I refer to as W2V for the remainder of this post) is a popular foundation model in the ASR community that was pre-trained on the full LibriSpeech 960-hour dataset of unlabelled speech data <a class="citation" href="#baevski2020wav2vec2">(Baevski et al., 2020)</a>. W2V has a specific structure and pre-training scheme that aims to allow it to learn useful representations. Structurally, the core W2V model consists of a CNN encoder that maps from raw waveforms to a sequence of latent speech representations and 12 masked transformer layers that turns these latent speech representations into contextual representations. W2V’s success as a foundation model is due to its training scheme. The model includes a quantizer that converts outputs from the encoder to discrete speech tokens from a learned codebook of speech tokens. This allows the model to be trained using a unique self-supervised loss. This loss combines a contrastive loss (whereby the model seeks to distinguish masked segments in the input speech waveform from distractor samples from an alternative speech waveform) and a diversity loss (that avoids codebook collapse by encouraging diversity in the learned codebook). The figure below is taken from the <a href="https://proceedings.neurips.cc//paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf">original W2V paper</a> and illustrates this architecture.</p> <p align="center"> <img src="/assets/img/posts/2024-08-20/w2v.png" style="width: 100%; max-width: 100%; height: auto;"/> </p> <p>An output layer to project into the desired output space can then be placed on top of W2V such that we can fine-tune it for a downstream application of interest. For instance, when fine-tuning W2V for the task of ASR, this output projection will project into the space of possible tokens (such as characters, phones or morphs).</p> <h3 id="fine-tuning-methodology">Fine-tuning Methodology</h3> <p>LibriSpeech is a dataset consisting of utterances and associated word-level transcripts. LibriSpeech contains both “clean” and “other” data, with “other” data consisting of, on average, more challenging utterances. In the following, I investigate fine-tuning W2V on a 10-hour subset of LibriSpeech consisting of 5 hours of “clean” data and 5 hours of “other” data. My validation set similarly mixes the two types of data and I report test word error rates (WERs) on a “clean” and an “other” test set separately. I train all models to output characters from a 30-character vocabulary consisting of the 26 standard characters, an ellipse, a <code class="language-plaintext highlighter-rouge">&lt;space&gt;</code> token to denote space between words, a <code class="language-plaintext highlighter-rouge">&lt;unk&gt;</code> token to denote an unseen character and a <code class="language-plaintext highlighter-rouge">&lt;blank&gt;</code> token for <a href="https://distill.pub/2017/ctc/">CTC</a> decoding. Decoded strings of characters are turned into strings of words by taking the <code class="language-plaintext highlighter-rouge">&lt;space&gt;</code> token to denote word boundaries</p> <p>In the following, all fine-tuning runs use Adam for 10 epochs with a tri-state learning rate scheduler that warmed up over the first epoch and decayed to 0 from the start of the 6th epoch while freezing the W2V module for the first 3 epochs. The learning rate warmed up to a maximum of 1e-4 and I used gradient clipping with a clipping value of 1. All following results are the average of three fine-tuning runs with different random seeds. Using this fine-tuning regime resulted in an average test WER (word error rate) of 10.71% on the clean utterances and 19.47% on the other utterances when fine-tuning all parameters in the W2V model. I refer to this as my baseline result. That we have achieved such a low WER despite only having 10 hours of supervised training data again speaks to the power of fine-tuning foundation models. Clearly, W2V has learned representations of input speech waveforms that are hugely useful in classifying spoken utterances.</p> <h3 id="re-initialising-layers">Re-Initialising Layers</h3> <p>How, then, can fine-tuning help us interpret the internal computations performed by the W2V model? Well, we can fine-tune the model in a manner designed to isolate the role played by specific W2V components! For instance, it has been argued that that the pre-training scheme of W2V causes W2V to effectively function as a large autoencoder-style model <a class="citation" href="#pasad2022layerwiseanalysisselfsupervisedspeech">(Pasad et al., 2022)</a>. This is to say that during pre-training, W2V’s early transformer layers learn at first to abstract away from inputs towards general representations before its later layers learn then to partially re-construct inputs in order to perform well at its contrastive pre-training task. If this hypothesis is true, we should expect re-initialising the final W2V transformer layers prior to fine-tuning to have minimal effect on the performance of the fine-tuned W2V model.</p> <p>To test this, I followed <a href="https://homepages.inf.ed.ac.uk/htang2/sigml/mlslp2021/MLSLP2021_paper_15.pdf">Pasad et al (2021)</a> and investigated re-initialising the final three W2V transformer layers. The table below shows my results.</p> <table style="width:100%; border-collapse: collapse;"> <thead> <tr> <th style="border: 1px solid black; padding: 8px;">Layers Re-Initialised</th> <th style="border: 1px solid black; padding: 8px;">Average Validation WER</th> <th style="border: 1px solid black; padding: 8px;">Average Test WER – Clean</th> <th style="border: 1px solid black; padding: 8px;">Average Test WER – Other</th> </tr> </thead> <tbody> <tr> <td style="border: 1px solid black; padding: 8px;">None (Baseline)</td> <td style="border: 1px solid black; padding: 8px;">13.87%</td> <td style="border: 1px solid black; padding: 8px;">10.71%</td> <td style="border: 1px solid black; padding: 8px;">19.47%</td> </tr> <tr> <td style="border: 1px solid black; padding: 8px;">12</td> <td style="border: 1px solid black; padding: 8px;">14.30%</td> <td style="border: 1px solid black; padding: 8px;">10.93%</td> <td style="border: 1px solid black; padding: 8px;">19.58%</td> </tr> <tr> <td style="border: 1px solid black; padding: 8px;">11, 12</td> <td style="border: 1px solid black; padding: 8px;">13.80%</td> <td style="border: 1px solid black; padding: 8px;">10.82%</td> <td style="border: 1px solid black; padding: 8px;">19.45%</td> </tr> <tr> <td style="border: 1px solid black; padding: 8px;">10, 11, 12</td> <td style="border: 1px solid black; padding: 8px;">15.34%</td> <td style="border: 1px solid black; padding: 8px;">11.49%</td> <td style="border: 1px solid black; padding: 8px;">21.18%</td> </tr> </tbody> </table> <p>This table shows that the WERs resulting from re-initialising just the twelfth W2V layer and the eleventh and twelfth W2V layers are sufficiently close to the baseline WERs that it is hard to argue that the WERs are meaningfully distinct . This suggests that these final two pre-trained layers contain barely any useful information for ASR, and we can view fine-tuning them on Librispeech as “overwriting them”. The table also illustrates that WERs increase by a small but meaningful amount when re-initialising the tenth, eleventh and twelfth layer. The fact that test WERs increase in this case implies that the pre-trained 10th layer does contain some ASR-relevant information, albeit only to a certain degree given that test WERs have only worsened slightly. The upshot of these results is that the final 3 transformer layers all must change heavily during fine-tuning (especially the final 2 layers that appear to be almost completely overwritten) else we would see large increases relative to the baseline when we re-initialise them. These results are consistent with the aforementioned hypothesis and illustrate how some easy fine-tuning can be used to quickly empirically test an interpretablity hypothesis.</p> <h3 id="fine-tuning-using-w2v-as-a-feature-extractor">Fine-Tuning Using W2V As A Feature Extractor</h3> <p>Often, it is of interest to fine-tune a completely frozen foundation model. One reason we might want to do this is to reduce the number of parameters we need to fine-tune. If we only have a small amount of data for our downstream task, for instance, reducing the number of tuneable parameters is crucial to avoid overfitting. When freezing a whole foundation model, we can view the foundation model as a feature extractor that has learned in pre-training how to extract features from input speech waveforms that are generically useful for downstream applications. In an interpretability setting, using a foundation model as a feature extractor allows us to investigate the applicability of the model’s representations to particular tasks of interest, providing insight into what is represented at specific layers of the model. Note that this idea is related to the idea of the <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">logit lens</a> and its variations in language modelling <a class="citation" href="#belrose2023elicitinglatentpredictionstransformers">(Belrose et al., 2023)</a>.</p> <p>I investigated fine-tuning a model consisting of a frozen W2V module and a 3-layer bi-directional LSTM with a hidden dimensionality of 1024. This corresponds to teaching the LSTM to recognise spoken utterances using the features produced by the frozen W2V. I investigated fine-tuning such an LSTM that took as inputs the representations at each of the layers of a frozen W2V model. The figures below show the resulting word error rates.</p> <p align="center"> <img src="/assets/img/posts/2024-08-20/fig2.png" style="width: 100%; max-width: 100%; height: auto;"/> </p> <p>These figures provide additional evidence for the hypothesis that W2V has learned autoencoder-style behaviour. Specifically, they illustrate that using the final transformer layer outputs as features results in poor WER which we expect given that these final layers are adapted to the specific contrastive pre-training task. Likewise, using outputs from the early transformer layers results in poor WERs which we would expect given that these layers are still building towards useful generalised representations of speech waveforms. Both clean-WER and other-WER are minimised when using outputs from layer 8. This makes sense when we consider that this is the rough layer at which general representations have been constructed by, but before these general representations are modified for the pre-training task.</p> <h3 id="summary">Summary</h3> <p>In this blog post I have illustrated how fine-tuning can be used as a basic tool when interpretting foundation models. Specifically, I have shown how some basic fine-tuning experiments can be used to perform rudimentary empirical tests of the hypothesis that Wav2Vec 2.0 leans to function as an autoencoder-style model. Note that these fine-tuning experiments primarily seek to rapidly falsify hypotheses, and that positively validating them would require the use of tools such as probing.</p>]]></content><author><name></name></author><category term="deep-learning,"/><category term="asr,"/><category term="interpretability"/><summary type="html"><![CDATA[In recent years there has been an explosion in the use of foundation models in automatic speech recognition (ASR). In this post, I overview some brief experiments involving finetuning a specific foundation model – Wav2Vec 2.0 – for the task of ASR on the LibriSpeech-10hour dataset. The aim of this post is to show basic methods can be used as a first step in interpretting large models.]]></summary></entry><entry><title type="html">Basic MCMC Pt. 2: The Metropolis-Hastings Algorithm</title><link href="https://tuphs28.github.io/blog/2024/Basic-MCMC/" rel="alternate" type="text/html" title="Basic MCMC Pt. 2: The Metropolis-Hastings Algorithm"/><published>2024-02-09T00:00:00+00:00</published><updated>2024-02-09T00:00:00+00:00</updated><id>https://tuphs28.github.io/blog/2024/Basic-MCMC</id><content type="html" xml:base="https://tuphs28.github.io/blog/2024/Basic-MCMC/"><![CDATA[<p>In machine learning, we often face computations over random variables that are analytically intractable and are hence forced to use <a href="https://tuphs28.github.io/Monte-Carlo-Methods/">Monte Carlo (MC) approximations</a>. However, using MC approximations is only possible if we have actually have samples from the distribution of interest, something which is often not the case. In this post I will outline the idea of “Markov Chain Monte Carlo” (MCMC) methods that allow us to compute Monte Carlo approximations in such settings. Specifically, I will focus on the most basic MCMC algorithm - the Metropolis-Hastings algorithm.</p> <h3 id="motivation">Motivation</h3> <p>We often want to perform computations such as computing expectations of the form:</p> <p>\[I = \mathbb{E}_{X \sim p}[f(X)] = \int{f(x)p(x)dx}\]</p> <p>As mentioned, if we can easily sample from the distribution \(p(x)\) we can easily approximate such computations through the use of MC approximations of the form:</p> <p>\[\hat{I}=\frac{1}T\sum_{t=1}^Tf(X_T), \text{where }X_1,…,X_T \sim \pi\]</p> <p>Such approximations are provably unbiased and consistent estimators and will converge to the true quantity of interest as \(N\) becomes arbitarily large.</p> <p>However, this obviously assumes we can easily generate samples \(X_1,…,X_T \sim \pi\) which often isn’t trivially the case. For instance, in Bayesian ML we will often run up against distributions \(\pi\) which are known only up to a normalising constant. When the distribution \(\pi\) is intractable in this sense, we cannot rely on standard sampling procedures (e.g. <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">inverse transform sampling</a> with a known CDF) to generate the samples we need in order to construct MC approximations.</p> <p>It is in this kind of setting where MCMC comes in. MCMC methods work by constructing a chain of samples \({X_1,…X_T}\) (called a “Markov chain” for reasons outlined below) that will converge towards being samples from the distribution of interest \(\pi\) as the length of the chain \(T\) goes to infinity. We can then use these samples to evaluate the MC approximation.</p> <h3 id="markov-chains">Markov Chains</h3> <p>So, what is this Markov chain \({X_1,…X_T}\) and how can we be sure that it will tend towards being samples from the distribution of interest if we cannot directly sample from said distribution? Well, A Markov chain is a series of sequential samples where the next sample depends on the previous sample and the previous sample alone. This is known as the chain satisfying the first order Markov property.</p> <p>The chain is then constructed using a Markov transition kernel. This is a function that governs where the next position of the chain \(X_{t}\) will be give the previous position \(X_{t-1}=x_{t-1}\). Specifically, it functions as a probability measure that governs the probability that the chain will end up in any region (i.e. the probability that \(X_t \in A\)) over the domain given its current position \(X_{t-1}=x_{t-1}\). So, the transition kernel is a function of the following form:</p> <p>\[\mathcal{P}(x, A) = \mathbb{P}(X_t \in A|X_{t-1}=x)\]</p> <p>To be precise, the transition kernel is a measurable function in its first argument and a probability measure over the corresponding probability space in its second argument. We then draw samples at each timestep according to this kernel such that the:</p> <p>Importantly, we can choose how to construct the transition kernel such that if we keep drawing samples according to it we will eventually end up drawing samples from a distribution of choice. The Metropolis-Hastings (MH) algorithm is, at its core, really just a clever choice for this kernel such that we can ~magically~ draw samples from a distribution that we might not fully know. In the next section I will outline how the MH algorithm designs the kernel to do this, but feel free to skip straight to the following section since the construction itself is maybe not massively illustrative.</p> <h3 id="constructing-the-mh-transition-kernel">Constructing the MH Transition Kernel</h3> <p>The aim here, then, is to construct a transition kernel such that if we repeatedly sample according to it we will end up sampling from the distribution \(\pi\) that we care about. This is where a nice property of Markov chains comes in - Markov chains (under certain conditions) have “stationary distributions” which are distributions that the chains will converge to in the sense that running the chain for arbitrarily long will results in the samples being distributed according to the stationary distribution.</p> <p>A sufficient condition for \(\pi\) being a stationary distribution for the chain created by the kernel \(\mathcal{P}\) is that it satisfies the following equation:</p> <p>\[\int_A{\mathcal{P}(x, B)\pi(x)dx}=\int_B{\mathcal{P}(x, A)\pi(x)dx}\]</p> <p>This is called the Detailed Balance Equation and states that the probability of observing a transition from a region A of the domain to a region B should be the same as observing the reverse transition if we assume we begin by sampling from \(\pi\). Thus, we want to construct a kernel to satisfy the Detailed Balance Equation, since this guarantees that the chain will eventually converge to \(\pi\).</p> <p>Now, consider the following transition kernel:</p> <p>\[\mathcal{P}_{MH}(x, A) = r(x) \delta_x(A) + \int_A{p(x,y)dy}\]</p> <p>where \(r(x)=1-\int{p(x,y)dy}\) is the probability that the chain remains at its current position \(x\) and \(p(x, y)\) is a density that generates proposed moves of the chain. Given that the Markov chain is at some position \(x\) this kernel corresponds to proposing that the new position of the chain be \(y\) (drawn according to the density \(p(x, y)\)) and rejecting the move and remaining at \(x\) with probability \(r(x)\). We will hence call the density \(p(x, y)\) the proposal density since, at each step of the chain, we will propose the next step of the chain by drawing from this density. For instance, a common choice is to set \(p(x, y)=N(y;x,1)\).</p> <p>It turns out that if the density \(p(x, y)\) satisfies the following reversibility equation</p> <p>\[\pi(x)p(x, y) = \pi(y)p(y,x)\]</p> <p>then \(\mathcal{P}_{MH}\) satisfies detailed balance w.r.t \(\pi\) and \(\pi\) will be the stationary distribution of the associated Markov chain! This means we similarly need to ensure that transitioning from \(x\) to \(y\) when initially sampling from \(\pi\) is just as likely as transitioning from \(y\) to \(x\) when initially sampling from \(\pi\).</p> <p>However, we now face a problem, since any arbitrary proposal isn’t going to satisfy this requirement for all moves between \(x\) and \(y\). For instance, given that our chain will be proposing moves according to the target density, we will often be making moves into regions of higher probability that are more likely than the reverse moves. To ensure that we satisyf the requirement, we can introduce a scaling probability factor \(\alpha(x,y)\) that controls the probability with which we accept proposed moves. We then let our our proposed density be \(q(x, y) = p(x, y)\alpha(x,y)\) such that we draw proposed moves from \(p\) and then accept then with probability \(\alpha\).</p> <p>The point of this scaling factor is to reduce the chances of making a move in the more likely direction and improve the chances of making a move in the less likely direction. In this way, the chain should become reversible, we should satisfy detailed balance, and we should converge to the target distribution \(\pi\). Thus, we want:</p> <p>\[\pi(x)\alpha(x,y)p(x, y) = \pi(y)\alpha(y,x)p(y,x)\]</p> <p>Let’s assume that in general we are more likely to observe transitions from \(x\) to \(y\) so that \(\pi(x)p(x, y) &gt; \pi(y)p(y,x)\). We then say we want to accept all proposed moves from \(y\) to \(x\) (so that we set \(\alpha(y,x)=1\)). Plugging this in and re-arranging, we get:</p> <p>\[\alpha(x,y) = \frac{\pi(y)p(y,x)}{\pi(x)p(x, y)}\]</p> <p>for the proposed move. This fraction is sometimes called the “Hastings ratio”. However, since we want this to hold for all moves and want to use it as a probability to limit the number of overly-likely moves we accept, we need to cap this at 1. Thus, we set:</p> <p>\[\alpha(x,y) = min(1, \frac{\pi(y)p(y,x)}{\pi(x)p(x, y)})\]</p> <p>With \(\alpha\) set like this, the new proposal density \(q(x, y) = p(x, y)\alpha(x,y)\) satisfies the reversibility requirement. If we sub this into \(\mathcal{P}_{MH}\) we hence get a transition kernel that will target our distribution \(\pi\) as a stationary distribution! So, our transition kernel is:</p> <p>\[\mathcal{P}_{MH}(x, A) = r(x) \delta_x(A) + \int_A{ p(x, y)\alpha(x,y)dy}\]</p> <p>with \(r(x)=1-\int{p(x, y)\alpha(x,y)dy}\).</p> <p>Crucially, the form of \(\alpha\) allows us to sample from \(\pi\) without being able to completely able to specify it. This is because if we can write</p> <p>\[\pi(x)=\frac{\phi(x)}{\int{\phi(x)}dx}\]</p> <p>\(\alpha\) becomes:</p> <p>\[\alpha(x,y) = min(1, \frac{\phi(y)p(y,x)}{\phi(x)p(x, y)})\]</p> <p>such that running this chain only requires knowing the distribution up to a normalising constant (which is something we often do know in Bayesian ML etc.). Furthermore, if we choose \(p\) to be symmetric such that \(p(x,y)=p(y,x)\), \(\alpha\) simplifies even further to become:</p> <p>\[\alpha(x,y) = min(1, \frac{\phi(y)}{\phi(x)})\]</p> <h3 id="the-metropolis-hastings-algorithm">The Metropolis-Hastings Algorithm</h3> <p>So, what does the above construction of the MH transition kernel actually <em>mean</em> for the construction of an algorithm? Well, it gives us the following algorithm known as the Metropolis-Hastings algorithm to sample from some distribution of interest \(\pi\):</p> <ul> <li>First, initialise the value of the chain \(x_t\) to be some arbitrary \(x_0\) and initialise the chain to be an empty list C</li> <li>Then, for t = 1, …, T, do: <ul> <li>Sample a proposed move for the chain: \(y_t \sim p(x_t, y_t)\)</li> <li>Sample a gating variable: \(u_t \sim U[0,1]\)</li> <li>If \(u_t &lt; \alpha(x_t, y_t)\), do: <ul> <li>Set \(x_t = y_t\)</li> </ul> </li> <li>Append \(x_t\) to the chain C</li> </ul> </li> <li>Return the chain C</li> </ul> <p>where (assuming that the distribution we wish to sample from can be written as \(\pi(x) \propto \phi(x)\) and our proposal density is symmetric such that \(p(x,y)=p(y,x)\)) we have the acceptance probability given by:</p> <p>\[\alpha(x,y) = min(1, \frac{\phi(y)}{\phi(x)})\]</p> <p>Running this algorithm leads to a Markov chain of samples corresponding to the Markov chain produced using the transition kernel \(\mathcal{P}_{MH}\) described above. Since this kernel is in detailed balance w.r.t. the distribution \(\pi\), this chain will, as we run the chain for an infinite amount of time, converge to samples drawn from \(\pi\)! So, we have an algorithm that allows us to sample from \(\pi\) even if we only know it up to some normalising constant.</p> <h3 id="the-gaussian-random-walk-mh-algorithm">The Gaussian Random Walk MH Algorithm</h3> <p>A comman instantiation of this algorithm is to set the proposal density to be a Gaussian centered at the current value of the chain:</p> <p>\[p(x,y) = N(x; y, \sigma^2)\]</p> <p>This is used due to its symmetry and due to the fact that the \(\sigma\) parameter functions as a fairly standard (quasi) stepsize parameter that allows us to control how close we want successive proposed moves of the chain to be. Using this choice of proposal, the Python code for the MH algorithm (i.e. the “Gaussian random walk Metropolis-Hastings algorithm”, or, GRW-MH) is shown below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">grw_metropolis_hastings_1D</span><span class="p">(</span><span class="n">target_density</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">n_iters</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="n">xt</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="n">chain</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>
        <span class="n">yt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
        <span class="n">hastings_ratio</span> <span class="o">=</span> <span class="nf">target_density</span><span class="p">(</span><span class="n">yt</span><span class="p">)</span> <span class="o">/</span> <span class="nf">target_density</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">hastings_ratio</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;</span> <span class="n">alpha</span><span class="p">:</span>
            <span class="n">xt</span> <span class="o">=</span> <span class="n">yt</span>
        <span class="n">chain</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">chain</span>
</code></pre></div></div> <p>This is a very popular and powerful algorithm. To give an example of how it works, suppose we want to use it to draw samples from a standard Gaussian. The below figure shows the true density for the standard Gaussian and the (normalised) empirical distribution of samples from running the GRW-MH algorithm for 1000 iterations and intialising the chain at 0. Clearly, the chain has succesfully approximated the distribution.</p> <p align="center"> <img src="/assets/img/posts/2024-02-09/basic_mh.png"/> </p> <p>Having outlined the algorithm, we can now turn attention to some practical considerations we face when actually running this algorithm.</p> <h3 id="chain-length">Chain Length</h3> <p>The first factor to consider when running a MH algorithm is the length of the chain. While the MH algorithm guarantees that the chain will <em>eventually</em> converge to the distribution we care about, it may not do so quickly. This can motivate the use of other algorithms in practice (i.e. if the MH algorithm would only converge in the infinite limit), but the key takeaway here is that <em>if</em> the chain converges in a sensible amount of time, its approximation to the true distribution will improve as we run it for longer.</p> <p>This is illustrated in the figure below where I plot the true distribution (here, a standard Gaussian at 0) relative to the empirical distributions generated by chains of different lengths. Clearly, as we run the chain for longer, the approximation to the distribution improves. This matters since it means any MC estimates we compute with the chain will suffer from less approximation error.</p> <p align="center"> <img src="/assets/img/posts/2024-02-09/chain_length.png" style="width: 100%; max-width: 100%; height: auto;"/> </p> <h3 id="step-size">Step Size</h3> <p>Another key consideration is step size. The step size of the chain controls how far or close proposed moves of the chain are to the current value. With the GRW-MH algorithm (i.e. MH using a Gaussian proposal) the step size is just controlled by the \(\sigma\) parameters - a larger \(\sigma\) means we will, on average, propose larger steps while a smaller \(\sigma\) means the reverse.</p> <p>I have plotted the empirical distributions from the chains generated by the GRW-MH algorithm for different \(\sigma\) below (where we again target the standard Gaussian and the chain is run for 250 iterations).</p> <p align="center"> <img src="/assets/img/posts/2024-02-09/sigma.png" style="width: 100%; max-width: 100%; height: auto;"/> </p> <p>Clearly, the approximation worsens for both overly small \(\sigma\) and overly large \(\sigma\).</p> <p>The approximations worsens for overly small \(\sigma\) since this corresponds to each step in the chain being very close to the prior steps. This means the chain takes way longer to explore the whole distribution and so our empirical samples will be concentrated in the regions of the domain that the chain has focused on taking small steps around.</p> <p>A large \(\sigma\) causes the approximation to worsen since this corresponds to taking huge steps which will often end up being in regions of low density ; this means the Hastings ratio will often be very low and so most steps will be rejected. This then means that the chain will be “sticky” and remain at the same place for many steps. This means it will not generate a diverse array of samples from the desired distribution, leading to a poor empirical approximation. This will then lead to worse MC estimates.</p> <h3 id="burn-in">Burn-In</h3> <p>When running the MH algorithm we also have to consider that we might be initialising the chain somewhere far away from the regions of high density under the target distribution. If this is the case, the beginning samples will NOT be drawn from the distribution that we care about as they will merely represent steps taken by the chain on its way towards the distribution we care about. We hence ought to discard these samples as they are not representative of our target distribution. This discarding of the first samples is called “burn-in” in MCMC algorithms.</p> <p>To illustrate the need to track burn-in, I ran the GRW-MH algorithm for 250 iterations to again target the standard Gaussian. However, I initialised the chain at 0, 10 and -10 to show that when we initialise far outside regions of high density (i.e. 10 and -10) we end up with some samples that are clearly not drawn from the distribution we care about and hence worsen our empirical approximation to this distribution. This is shown in the figure below, with there clearly being samples that are drawn while the chain is converging to the distribution of interest.</p> <p align="center"> <img src="/assets/img/posts/2024-02-09/mh_burnin.png" style="width: 100%; max-width: 100%; height: auto;"/> </p> <p>How can we track burn-in? Well, we can create traceplots of the variables we are running the chain over (or, in the multivariate case we can create traceplots for summary statistics). A traceplot just tracks the value of a variable at each iteration. I created traceplots to track the the three chains described above and these are shown below. These traceplots clearly show that the chains initialised far from the target distribution exhibit burn-in as they move towards the target. I have also included the MC estimates correspond to the three chains. Clearly, the two chains that have a long burn-in time produce worse MC estimates (since the true mean of the distribution is 0).</p> <p align="center"> <img src="/assets/img/posts/2024-02-09/mh_trace.png" style="width: 100%; max-width: 100%; height: auto;"/> </p> <h3 id="multi-modal-distributions">Multi-Modal Distributions</h3> <p>Finally, let’s consider a problem with the MH algorithm as presented. Namely, let’s consider the fact that the vanilla MH algorithm struggles with multimodal target distributions.</p> <p>Imagine our target distribution is the following Gaussian mixture model:</p> <p>\[\pi(x) = 0.4N(x;0,1) + 0.3N(x;7, 1) + 0.3N(x; -10,1)\]</p> <p>This distribution is characterised by having multiple modes separated by sizable regions of negligible density as show by the plot of its density below:</p> <p align="center"> <img src="/assets/img/posts/2024-02-09/gmm_pdf.png" style="width: 100%; max-width: 100%; height: auto;"/> </p> <p>The GRW-MH algorithm faces a problem here: in order for the chain to moves between the different regions of high density, it needs to step through the regions of low density. However, the Hastings ratio for any proposed step into a region of low density will be very small such that the vast majority of these needed proposed steps will be rejected. This means that the chain will be stuck in one region of high density until it is lucky enough to get a move to one of the other regions (through a low density region) accepted.</p> <p>To illustrate this effect, I ran the GRW-MH algorithm for 250 iterations (initialised at \(x_0=0\) with \(\sigma=1\)) to target this multimodal distribution. The resulting empirical distribution is shown below, and clearly illustrates that we have failed to move from the central region of high density to the other two regions.</p> <p align="center"> <img src="/assets/img/posts/2024-02-09/mm_250.png" style="width: 100%; max-width: 100%; height: auto;"/> </p> <p>Now, if we run the chain for longer we end up being able to eventually move over to the “nearer” mode centered at 7. However, even if we run the chain for a very long time, we seem unlikely to ever get to the further mode (centered at -10). This is because getting to the further mode requires many repeated steps that are likely to be rejected while getting to the closer mode only requires a few such steps. This effect is illustrated below as I have plotted the empirical distributions of chains run for different amounts of time:</p> <p align="center"> <img src="/assets/img/posts/2024-02-09/mm_long.png" style="width: 100%; max-width: 100%; height: auto;"/> </p> <p>So, what can we do to overcome this problem? Well, a partial solution is to increase \(\sigma\) such that it is very large. This reduces the number of unlikely steps required to move between regions of high density, making such moves far more likely. However, doing this is problematic since a large \(\sigma\) will, as explained above, still lead to many proposed moves getting rejected (e.g. when the chain tries to step beyond the modes).</p> <p>A better solution is to use a mixture transition kernel. The idea here is that if we select two transition kernels \(\mathcal{P}_A\) and \(\mathcal{P}_B\)that both have our target distribution as their stationary distribution, then linearly interpolating between then will yield another transition kernel that also has our target as the stationary distribution. That is, for \(\gamma \in [0,1]\), the kernel</p> <p>\[\mathcal{P} = \gamma \mathcal{P}_A + (1-\gamma) \mathcal{P}_B\]</p> <p>will be a valid MH transition kernel that will target out desired distribution!.</p> <p>In practice, using this kernel leads to a MH implementation where our proposal distribution is just the corresponding linear interpolation between the two proposal distributions corresponding to the two densities. The modified mixture GRW-MH algorithm is shown below in Python:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mixture_grw_metropolis_hastings_1D</span><span class="p">(</span><span class="n">target_density</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">n_iters</span><span class="p">,</span> <span class="n">sigma_low</span><span class="p">,</span> <span class="n">sigma_high</span><span class="p">,</span> <span class="n">mixture_weight</span><span class="p">):</span>
    <span class="n">xt</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="n">chain</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>
        <span class="n">yt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">+</span> <span class="n">mixture_weight</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma_low</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mixture_weight</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma_high</span><span class="p">)</span>
        <span class="n">hastings_ratio</span> <span class="o">=</span> <span class="nf">target_density</span><span class="p">(</span><span class="n">yt</span><span class="p">)</span> <span class="o">/</span> <span class="nf">target_density</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">hastings_ratio</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;</span> <span class="n">alpha</span><span class="p">:</span>
            <span class="n">xt</span> <span class="o">=</span> <span class="n">yt</span>
        <span class="n">chain</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">chain</span>
</code></pre></div></div> <p>This algorithm is then nice since we can get the benefits of having a large \(\sigma\) (e.g. being able to freely move between modes) without incurring all of the costs (e.g. many moves being rejected and the chain being sticky) by mixing between a proposal Gaussian with a low variance and a proposal Gaussian with a high variance.</p> <p>To illustrate this, I ran this modified algorithm for 2000 iterations with the \(\sigma\) set to 1 for the low-variance Gaussian anf 5 for the high variance Gaussian. The resulting empirical distribution is shown below and clearly illustrates that this modified algorithm can better explore multi-modal distributions.</p> <p align="center"> <img src="/assets/img/posts/2024-02-09/mixture.png" style="width: 100%; max-width: 100%; height: auto;"/> </p> <h3 id="summary">Summary</h3> <p>In this blog post I have outlined the msot simple MCMC algorithm (the MH algorithm) and detailed some pratical concerns that come up when using it (e.g. step size, burn-in and mutlimodality). While this has been considered in the simple uni-dimensional setting, these ideas can easily be scaled up to more complex settings. Indeed, with some minor <a href="https://en.wikipedia.org/wiki/Preconditioned_Crank%E2%80%93Nicolson_algorithm">modifications</a> the MH algorithm can be defined for infinite dimensional spaces such that that we can perform MCMC in Hilbert spaces. This then lets us get Markov chains of (approximations to) infinite-dimensional objects like functions which is pretty cool. The idea of generating Markov chains is also crucial to advances in modern ML such as in diffusion models.</p>]]></content><author><name></name></author><category term="mcmc,"/><category term="statistics"/><summary type="html"><![CDATA[In machine learning, we often face computations over random variables that are analytically intractable and are hence forced to use Monte Carlo (MC) approximations. However, using MC approximations is only possible if we have actually have samples from the distribution of interest, something which is often not the case. In this post I will outline the idea of “Markov Chain Monte Carlo” (MCMC) methods that allow us to compute Monte Carlo approximations in such settings. Specifically, I will focus on the most basic MCMC algorithm - the Metropolis-Hastings algorithm.]]></summary></entry><entry><title type="html">Bandit Algorithms (&amp;amp; The Exploration-Exploitation Tradeoff)</title><link href="https://tuphs28.github.io/blog/2024/Bandit-Algorithms/" rel="alternate" type="text/html" title="Bandit Algorithms (&amp;amp; The Exploration-Exploitation Tradeoff)"/><published>2024-01-24T00:00:00+00:00</published><updated>2024-01-24T00:00:00+00:00</updated><id>https://tuphs28.github.io/blog/2024/Bandit-Algorithms</id><content type="html" xml:base="https://tuphs28.github.io/blog/2024/Bandit-Algorithms/"><![CDATA[<p>RL problems are unique in that RL agents face much greater uncertainty (e.g. about rewards, the environment) than is faced by models in supervised learning. This gives rise to many problems, one of which is that RL agents are forced to confront the so-called “exploration-exploitation tradeoff”. In this post, I will explore this tradeoff in the simplified context of multi-armed bandit problems. Specifically, I will investigate how different algorithms navigate this tradeoff in this setting. I will also provide the code for these algorithms so that anyone interested can play about with them further.</p> <p>Note (24/08/2024): It should be noted that this blog post is one of the posts I copied over from my old blog. As such, it is a pretty word explanation of some faily simple ideas. However, I decided to copy this post over to my new blog (despite viewing it as being a fairly crappy post) since these ideas - like value functions, UCB exploration and policy gradient algorithms - are <em>really</em> important in understanding RL, and exploring them in the context of multi-armed bandits seems like a simple way to grok some of the core insights.</p> <h3 id="multi-armed-bandits">Multi-Armed Bandits</h3> <p>In RL, we think of an agent that is interacting with an environment. At each timestep, the agent takes some action that influences the environment and then recieves some reward. The overall aim of the agent is to maximise its return, or the (discounted) sum of its rewards. Crucially, the agent does not know its reward function and hence has to perform actions and see what rewards it gets in order to make inferences about what kinds of actions provide it with high rewards.</p> <p>The agent hence faces a tradeoff - it can either keep performing the actions that it already knows provide it with high rewards, or it can choose to perform actions for which it is uncertain what the reward will be. The idea here is that the agent may “know” that some actions will lead to a good reward but may have some other actions for which it is highly uncertain what the return would be. Such uncertain actions could, then, potentially lead to massive long-run returns. Hence, the agent faces the so-called “exploration-exploitation tradeoff”, since it must balance exploiting the actions it knows will yield a good reward and exploring new actions which could possible yield an even better reward.</p> <p>Now, this explanation is a bit simplistic relative to how RL actually works. This is because RL agents can take actions that end up influencing their environments, such that they have to overcome problems like building models of the environment and so on. These topics are really cool, but we can simplify the setting to focus in on the exploration-exploitation tradeoff. We can do this by focusing in on multi-armed bandit problems.</p> <p>Multi-armed bandit settings are settings where the environment only has a single state. This makes RL <em>way</em> easier, since it means agents can ignore the effect of the actions thet take on the environment. In a bandit setting, agents must choose which of a set of actions to perform at each timestep. They will then recieve a (random) reward associated with the action they performed. Thus, the environment in this setting is just:</p> <p>\[E=\{ p(r|a) | a \in \mathcal{A}\}\]</p> <p>where \(\mathcal{A}\) is the set of possible actions the agent can perform and \(p(r|a)\) are the distributions of rewards associated with the agent performing action \(a\).</p> <p>Importantly, the “problem” we face in bandit setting is that the agent does not know what the distributions of rewards are. Thus, it has to have some strategy for exploring actions and then performing those that yield a high reward. This means we face having to trade off exploring different arms whilst exploiting our knowledge of arms that we know to be “good”. We can think of this as sitting on a casino floor surrounded by slot machines. Each machine has an arm which we can pull and each machine has its own distribution of rewards. Our goal, then, is to come up with some plan for winning money from these machines.</p> <h3 id="a-toy-bandit-problem">A Toy Bandit Problem</h3> <p>There are many different algorithms - which I am refering to as “bandit algorithms” - to guide an agent in this setting. I will explore three basic such algorithms in the context of a toy problem. Specifically, I will use a bandit problem where an agent has 11 actions to choose from. The reward distributions of these 11 actions are merely shifted standard Gaussian centered at the integers from -5 to 5. So, our “actions” are basically picking one of these 11 integers such that \(\mathcal{A} = \{-5,4,…,4,5\} \) and our “environment” is merely:</p> <p>\[E=\{ \mathcal{N}(a, 1) | a \in \mathcal{A} \}\]</p> <p>Clearly, the optimal action in this toy set-up is to merely keep pulling the final arm (i.e. the one corresponding to +5). However, our agents do not know this and so our algorithms need to let them act in some way as to “figure it out”. The goal of the algorithms I consider is hence to learn a policy \(\pi(a)\)- a distribution over actions - that achieves a high reward over the long run.</p> <h3 id="action-value-estimates">Action-Value Estimates</h3> <p>The first three algorithms I will consider all work by estimating the expected value of each actions. The idea here is that the rewards associated with each action have some true expected value. Call this \(q(a)\). If the agent new these rewards, the problem would be trivial as the agent could merely just keep performing the action with the highest expected reward. However, in practice, \(q(a)\) is unkown to the agent. These first three algorithms - which we can call action-value algorithms - work by building some estimate \(Q_{t}(a)\) of the true expected value for each action at each time step. They then use these estimates to decide how to act. The t subscript here is important since these estimates will change over time.</p> <p>So, how do agents build these action value estimates? Well, in the simplest case we can merely let the estimated value for action a at time t be equal to the sample average reward recieved whenever the agent performed action a at any time up to time t. Using indicator functions, we can write this as :</p> <p>\[Q_{t}(a)=\frac{1}{N_{t}(a)}\sum_{i=0}^t\unicode{x1D7D9}(A_i=a)R_i, \text{where } N_{t}(a)=\sum_{i=0}^t\unicode{x1D7D9}(A_i=a)\]</p> <p>where \(A_i \in \mathcal{A}\) denotes the action taken at timestep \(i\) and \(R_i \sim p(r|A_i)\) is a reward drawn from the associated reward distribution. However, calculating expected rewards using the above requires us to store the entire history of actions and rewards. While possible in a toy problem like this, such a requirement is clearly infeasible for actual problems. Thus, we can update action value counts and estimates in an online fashion by updating the estimate for the action \(A_t\) performed at each time step as shown below:</p> <p>\[Q_{t}(A_t)=\frac{N_{t-1}(A_{t-1})Q_{t-1}(A_t) + R_t}{N_{t-1}(A_t)+1}\] \[N_t(A_t) = N_{t-1}(A_t) + 1\]</p> <p>and leaving all other estimates unchanged.</p> <p>With this outlined, we can now turn to the specific algorithms and how they approach the tradeoff between exploration and exploitation. All the algorithms have a common form of choosing some action, recieiving a reward and then (optionally) updating action value estimates. As such, I use the following wrapper for all the algorithms:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BanditAlgorithm</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">algorithm</span><span class="p">,</span> <span class="n">reward_dists</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">arm_value_estimates</span> <span class="o">=</span> <span class="p">{</span><span class="n">arm</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">arm</span> <span class="ow">in</span> <span class="n">reward_dists</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">arm_counts</span> <span class="o">=</span> <span class="p">{</span><span class="n">arm</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">arm</span> <span class="ow">in</span> <span class="n">reward_dists</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">reward_dists</span> <span class="o">=</span> <span class="n">reward_dists</span>
        <span class="n">self</span><span class="p">.</span><span class="n">algorithm</span> <span class="o">=</span> <span class="n">algorithm</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">):</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>

            <span class="c1"># policy - choose arm to pull and get reward
</span>            <span class="n">action</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">algorithm</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">arm_value_estimates</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">arm_counts</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_dists</span><span class="p">)</span>

            <span class="c1"># perform updates    
</span>            <span class="n">self</span><span class="p">.</span><span class="n">arm_value_estimates</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">arm_counts</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">arm_value_estimates</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">reward</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">arm_counts</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">arm_counts</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># track actions and rewards
</span>            <span class="n">actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">rewards</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span>      
</code></pre></div></div> <h3 id="greedy-algorithm">Greedy Algorithm</h3> <p>The simplest possible algorithm in the face of the bandit problem is to merely adopt a greedy policy. Adopting such a policy merely means selecting the action with the highest value estimate at each timestep. Thus, the greedy policy is merely:</p> <p>\[ \pi_t(a) = \begin{cases} 1 &amp;\text{if } a = argmax_bQ_t(b)\\\<br/> 0 &amp;\text{otherwise} \end{cases} \]</p> <p>In Python, we can implement this as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Greedy</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arm_value_estimates</span><span class="p">,</span> <span class="n">arm_counts</span><span class="p">,</span> <span class="n">reward_dists</span><span class="p">):</span>
        <span class="n">action_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">arm_value_estimates</span><span class="p">.</span><span class="nf">values</span><span class="p">())).</span><span class="nf">argmax</span><span class="p">()</span>
        <span class="n">action</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">arm_value_estimates</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="n">action_idx</span><span class="p">]</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">reward_dists</span><span class="p">[</span><span class="n">action</span><span class="p">]()</span>
        <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span>
</code></pre></div></div> <p>On the face of it, this might seem like a sensible course of action - if we believe some action to be more rewarding in expectation, it perhaps makes sense to perform it. However, in practice this is a pretty terrible strategy since it corresponds to putting full weight on the exploitation side of the exploration-exploitation tradeoff. As such, it means that our agent may never perform many actions that have huge return. This is because the moment the estimate value for an action exceeds the value at which estimated values are initialised the greedy algorithm will just keep repeating that same action and may never even try other actions with a higher reward.</p> <p>In our toy bandit setup, we initialised all value estimates to zero. Thus, once an action achieves a higher estimated value than 0, the agent will just keep performing that action and may nevery get a chance to explore other actions at all. Since I have configured the algorithm in such a way as to perform the action corresponding to the lowest integer when there is a tie, this means that the greedy policy will explore actions only until it arrives at action 1 at which point it will (in all probability) keep performing that action ad infinitum. The rewards achieved by this algorithm for 250 timesteps are shown below, and the behaviour is clearly illustrated (e.g. the rewards are clealy draws from a Gaussian at 1 after a certain number of steps).</p> <p align="center"> <img src="/assets/img/posts/2024-01-22/greedy.png" style="width: 80%; max-width: 80%; height: auto;"/> </p> <p>This, though, is clearly sub-optimal since we know that the best action the agent can do is to recieve rewards from the Gaussian centered at 5 instead. What has gone wrong here is that the greedy policy is merely exploiting what it knows to be a good action and fails to explore sufficiently. This means it fails to realise that there are other actions that have higher possible returns.</p> <p>Now, of course, the “failure” of the greedy approach here follows from the fact that we initialised our value estimates to zero. So, one could argue that the algorithm would work well if we were merely incredibly optimistic with our initial value estimates (say, initialising all values as 1000). This is because such an initialisation would encourage the agent to explore all actions since all actions would seem inferior to its prior expectations. However, this is not a scalable approach. This is mainly because it relies on the assumption that we know the rough range in which rewards will fall, which is a pretty unrealistic assumption in many cases. It is also an incredibly ad-hoc solution that doesn’t get to the core of the failure here - that the algorithm seems to put too much weight on exploitation over exploration.</p> <h3 id="epsilon-greedy-algorithm">Epsilon Greedy Algorithm</h3> <p>So, the problem with the above greedy approach was that it relied too much on exploiting its knowledge of what it considered good actions and hence failed to explore actions which it hadn’t tried but that might have been better. Basically, we can see the above failure mode as the agent commiting to exploit one action which seems “okay” despite being very uncertain about the values of other actions. Intuitively, we want an agent to reduce the uncertainty about the rewards of all actions to at least a cetain extent so that we are justified in believing that there are not huge unrealised gains from actions we haven’t explored sufficiently.</p> <p>This, then, motivates the epsilon greedy algorithm. This algorithm is very similar to the simple greedy approch but introduces a parameter \(\epsilon\). This then leads to a policy whereby, with probability \(\epsilon\), the agents selects some action at random and, with probability \(1-\epsilon\) the agent selects the action with the highest estimated value. The epsilon-greedy policy is hence given by:</p> <p>\[ \pi_t(a) = \begin{cases} 1-\epsilon + \frac{1}{|\mathcal{A}|}\epsilon &amp;\text{if } a = argmax_bQ_t(b)\\\<br/> \frac{1}{|\mathcal{A}|}\epsilon &amp;\text{otherwise} \end{cases} \]</p> <p>and my Python implementation is given by:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EpsilonGreedy</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arm_value_estimates</span><span class="p">,</span> <span class="n">arm_counts</span><span class="p">,</span> <span class="n">reward_dists</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">arm_value_estimates</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">arm_value_estimates</span><span class="p">.</span><span class="nf">keys</span><span class="p">()))]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">arm_value_estimates</span><span class="p">.</span><span class="nf">values</span><span class="p">())).</span><span class="nf">argmax</span><span class="p">()</span>
            <span class="n">action</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">arm_value_estimates</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="n">action_idx</span><span class="p">]</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">reward_dists</span><span class="p">[</span><span class="n">action</span><span class="p">]()</span>
        <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span>   
</code></pre></div></div> <p>The intuition underlying this algorithm is that we are just trying to create a form of the greedy algorithm that properly explores the entire space of actions and hence reduces sufficiently uncertainty about action rewards. This policy does this by sometimes selecting a random policy. This should lead to better estimates of all action values and should, hence, not miss out on actions with high expected rewards.</p> <p>The below figure illustrates the rewards achieved by the algorithm over 500 iterations for 3 different values of epsilon (0.001, 0.01 and 0.1).</p> <p align="center"> <img src="/assets/img/posts/2024-01-22/eps_greedy.png" style="width: 80%; max-width: 80%; height: auto;"/> </p> <p>Clearly, this is an improvement over the simple greedy approach for all epsilon. However, it is also clear that the largest epsilon manages to converge to recieving rewards from the optimal distributon fairly quickly while the middle epsilon takes longer and the smallest epsilon fails to achieve this at all over the 500 runs. What is happening here is that epsilon controls how much we trade off exploration for exploitation - a larger epsilon means we place more weight on exploring and less on exploiting our knowledge of what we think is a good action.</p> <p>However, a large epsilon has its drawbacks as shown by the graph for the 0.1 case. This is because, even once the agent has sufficiently explored the environment to find the best action, it keeps exploring at the same rate. This is because epsilon is constant and hence every 10th action will, on average, be random even once we know what actions are good and what actions are bad. This means we can improve further on the epsilon greedy algorithm.</p> <h3 id="upper-confidence-bounds">Upper Confidence Bounds</h3> <p>So, how can we make such an improvement? Well, the epsilon greedy approach failed since it kept exploring even after we had reduced uncertainty enough to be relatively confident about which actions had the highest action values. Thus, what we broadly want is an algorithm that explores actions a lot when our uncertainty over their values is high and then explores less and less as this uncertainty reduces.</p> <p>This is sometimes called “optimism in the face of uncertainty” and represents a theoretically well-founded approach to the exploration-exploitation tradeoff. The idea here is that when we are very uncertain about the value of actions we should be optimistic and perform them. This is because they may have a high expected value and so performing them gives us a chance to investigate whether this is the case.</p> <p>Optimisim in the face of uncertainty gives rise to the UCB algorithm, where UCB stands for “upper confidence bounds”. The UCB algorithm is similar to the original greedy algorithm except that, rather than being greedy with estimated values, we are greedy with respect to upper bounds on what we think these values could be. That is, we come up with some upper confidence \(U_t(a)\) for each action such that the true value \(q(a)\) is less than our upper confidence bound \(Q_t(a) + U_t(a)\) with a high probability. That is, we select \(U_t(a)\) such that with a high probability it holds that:</p> <p>\[q(a) &lt; Q_t(a) + U_t(a) \]</p> <p>We then select actions to be the actions with the highest upper confidence bound:</p> <p>\[a_t = argmax_{a \in \mathcal{A}}(Q_t(a) + U_t(a))\]</p> <p>The intuition here is that we are selecting the action for which it is plausible that the expected value is the largest. Since large expected values will be plausible for actions which haven’t been tried much, this will encourage exploration for uncertain actions. However, unlike the epsilon greedy approach, this epxploration will reduce once we are certain of what actions are good.</p> <p>So, how do we select \(U_t(a)\)? Well, it can be shown that a good choice is to set it such that:</p> <p>\[U_t(a) = \sqrt{\frac{2log(t)}{N_t(a)}}\]</p> <p>Why is this a good choice? Without going into too much theory its kind of hard to explain rigourously but a broad outline is as follows. We can judge bandit algorithms in terms of how the total “regret” of agent grows over time, where the regret at each timestep is the difference between the expected value of the optimal action and the expected value of the performed action. We can hence see the goal of the agent as being to minimise total regret. It turns out that regret growth for the UCB algorithm with this \(U_t(a)\) is exactly logarithmic. This is good since both the prior algorithms (and the following one I outline shortly) have linear regret growth. Additionally, it can also be shwon that regret will grow at least logarithmically. Thus the UCB with this \(U_t(a)\) leads to agents achieving the optimlal rate of regret growth.</p> <p>In practice we can replace the \(\sqrt{2}\) with a parameter \(\alpha\). This still allows us to achieve our bound but having the paramter allows us to explicitly control how much we wish to trade off exploration and exploitation. A larger \(\alpha\) corresponds to placing a greater weight one exploring to equally reduce the uncertainties of all action values whilst a lower alpha corresponds to placing a greater weight on exploiting actions that are known to have good action values.</p> <p>Having explained all of that, the policy provided by the UCB algorithm is then:</p> <p>\[ \pi_t(a) = \begin{cases} 1 &amp;\text{if } a = argmax_{b \in \mathcal{A}}(Q_t(b) + \alpha \sqrt{\frac{log(t)}{N_t(b)}})\\\<br/> 0 &amp;\text{otherwise} \end{cases} \]</p> <p>and my Python implementation is:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">UCB</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arm_value_estimates</span><span class="p">,</span> <span class="n">arm_counts</span><span class="p">,</span> <span class="n">reward_dists</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">arm_counts</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
        <span class="n">ns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">arm_counts</span><span class="p">.</span><span class="nf">values</span><span class="p">()))</span>
        <span class="n">ns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">ns</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
        <span class="n">us</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">ns</span><span class="p">))</span>
        <span class="n">upper_bounds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">arm_value_estimates</span><span class="p">.</span><span class="nf">values</span><span class="p">()))</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">us</span> <span class="o">**</span> <span class="mf">0.5</span>
        <span class="n">action_idx</span> <span class="o">=</span> <span class="n">upper_bounds</span><span class="p">.</span><span class="nf">argmax</span><span class="p">()</span>
        <span class="n">action</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">arm_value_estimates</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="n">action_idx</span><span class="p">]</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">reward_dists</span><span class="p">[</span><span class="n">action</span><span class="p">]()</span>
        <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span>
</code></pre></div></div> <p>To illustrate the performance of UCB, I ran the algorithm with \(\alpha = \sqrt{2}\) for 500 steps and have displayed the resulting rewards at each iteration in the graph below.</p> <p align="center"> <img src="/assets/img/posts/2024-01-22/ucb.png" style="width: 80%; max-width: 80%; height: auto;"/> </p> <p>Clearly, we get good results with the algorithm. Not only do we rapidly converge to the optimal action, but we also keep recieving rewards drawn from the optimal distribution rather than keeping exploring sub-optimal actions for too long.</p> <h3 id="comparing-action-value-algorithms">Comparing Action-Value Algorithms</h3> <p>We have hence considered an array of different action value algorithms that all work in various ways by estimating the action value of the different actions \(a \in \mathcal{A}\). To illustrate how these algorithms differ in performances, I ran all algorithms again for 1000 iterations, 1000 times over (e.g. I ran each algorithm for 1000 steps 1000 seperate times) and averaged the rewards each algorithm achieved at each timestep. Running the algorithms 1000 times over and averaging rewards should reduce sampling variability and better allow us to inspect the trends. The results are shown in the figure below.</p> <p align="center"> <img src="/assets/img/posts/2024-01-22/actvalue.png" style="width: 80%; max-width: 80%; height: auto;"/> </p> <p>From this figure we can see all of the theoretical points explained above:</p> <ul> <li>The plain greedy algorithm is just bad - it leans too far on the exploit side of the explore-exploit tradeoff and doesn’t realise there are actions with higher possible values</li> <li>The epsilon greedy algorithm is okay, but we face a tradeoff in setting the \(\epsilon\) parameters. For large \(\epsilon\) rewards increase quickly but bottom out below the optimal possible value since this corresponds to randomly exploring with a high probability even once we know what actions are good. For small \(\epsilon\) we explore slower but experience less suboptimal exploration once the optimal is reached.</li> <li>The UCB algorithm converges rapidly and strictly dominates ; this makes sense given the afforementioned theoretical guarantees we can provide.</li> </ul> <h3 id="policy-gradient-algorithms">Policy Gradient Algorithms</h3> <p>All of the above algorithms work by estimating action values. This corresponds to a more general strategy in broader RL of estimating value functions. An alternative strategy in RL is to parameterise the policy of an agent and then converge to an optimal policy by taking steps of gradients ascent. This is the so-called “policy gradient approach”.</p> <p>In a a simple form, let us have a vector of “preferences” as follows:</p> <p>\[\boldsymbol{h_t} = [h_t(a_1), …, h_t(a_k)]^T\]</p> <p>where each the relative size of the preference for action \(a\), denoted \(h_t(a)\), represents how much more likely that action should be under the policy \(\pi_T\). The policy can then be viewed as a distribution over actions by passing the preferences vector through a softmax such that the policy is given by:</p> <p>\[\pi_t(a|\boldsymbol{h_t}) = \frac{exp(h_t(a))}{\sum_{i=1}^k exp(h_t(a_k))}\]</p> <p>The nice thing about this is that, since we have parameterised the policy with the preferences, we can update the preferences such that the expected value of the reward at the next time step increases. This is because the expected value of the reward \(\mathbb{E}[R_t|A_t=a]\) is just a function of our new paramters. We can then update like this with standard gradient ascent such that we update preferences according to:</p> <p>\[\boldsymbol{h_{t+1}} = \boldsymbol{h_t} + \eta \nabla_{\boldsymbol{h_t}}\mathbb{E}[R_t|\pi_t]\]</p> <p>where \(\eta\) is a standard stepsize parameter.</p> <p>What then, is \(\nabla_{\boldsymbol{h_t}}\mathbb{E}[R_t|\pi_t]\)? Well, using the REINFORCE trick that is common in RL it turns out that:</p> <p>\[\nabla_{\boldsymbol{h_t}}\mathbb{E}[R_t|\pi_t] = \mathbb{E}[R_t\nabla_{\boldsymbol{h_t}}log\pi_t(a|\boldsymbol{h_t})]\]</p> <p>which we can approximate using a <a href="https://tuphs28.github.io/Monte-Carlo-Methods/">Monte Carlo approximation</a> with a sample size of 1 at each step such that our gradient ascent update rule is:</p> <p>\[\boldsymbol{h_{t+1}} = \boldsymbol{h_t} + \eta R_t \nabla_{\boldsymbol{h_t}}log(\pi_t(A_t|\boldsymbol{h_t}))\]</p> <p>Using the earlier-described preference set-up (e.g. passing through a softmax to get the policy), this corresponds to the following update rule in practice:</p> <p>\[ h_{t+1}(a) = \begin{cases} h_t(a) + \eta R_t(1-\pi_t(a|\boldsymbol{h_t})) &amp;\text{if } a= A_t\\\<br/> h_t(a) -\eta R_t\pi_t(a|\boldsymbol{h_t}) &amp;\text{if } a \neq A_t \end{cases} \]</p> <p>Intuitively, this means that whenever we select an action, we will increase the magnitude of its preference (while decreasing those for other actions such that we retain a valid PMF). Crucially, we increase the magnitude more for actions that yield larger rewards so that, eventually, the preferences for the largest rewards will dominate.</p> <p>My code for this algorithm is provided below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PolicyGradient</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">n_arms</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="n">self</span><span class="p">.</span><span class="n">preferences</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_arms</span><span class="p">,))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">preferences</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">preferences</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arm_value_estimates</span><span class="p">,</span> <span class="n">arm_counts</span><span class="p">,</span> <span class="n">reward_dists</span><span class="p">):</span>
        <span class="n">action_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">probs</span><span class="p">).</span><span class="nf">argmax</span><span class="p">()</span>
        <span class="n">action</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">arm_value_estimates</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="n">action_idx</span><span class="p">]</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">reward_dists</span><span class="p">[</span><span class="n">action</span><span class="p">]()</span>
        <span class="n">update</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">reward</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">probs</span>
        <span class="n">update</span><span class="p">[</span><span class="n">action_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">reward</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">probs</span><span class="p">[</span><span class="n">action_idx</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">preferences</span> <span class="o">+=</span> <span class="n">update</span>
        <span class="n">self</span><span class="p">.</span><span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">preferences</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">preferences</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span>
</code></pre></div></div> <p>To compare this to the value algorithms, I again ran it, the epsilon-greedy algorithm and the UCB algorithm for 1000 timesteps 1000 times over and averaged the rewards at each timestep for each algorithm. The results are shown in the figure below:</p> <p align="center"> <img src="/assets/img/posts/2024-01-22/pg_comp.png" style="width: 80%; max-width: 80%; height: auto;"/> </p> <p>We can see that the policy gradient algorithm performs equally well as the UCB algorithm. This is a kind of cool result since the policy gradient approach handles the exploration-exploitation tradeoff automatically, without us having to hard-code it in!</p> <h3 id="summary">Summary</h3> <p>In this post, I have outlined how various algorithms interact with the need to tradeoff exploring vs exploiting in the simplified RL setting of multi-armed bandits. This is, I think, a pretty interesting topic, especially when we scale up to full RL problems where actions impact environment states. A notebook I made corresponding to this post can be found <a href="https://github.com/tuphs28/MLMI-Notes/blob/main/RL/bandit_algorithms.ipynb">here</a> if anyone wants to play around with the algorithms further.</p>]]></content><author><name></name></author><category term="rl"/><summary type="html"><![CDATA[RL problems are unique in that RL agents face much greater uncertainty (e.g. about rewards, the environment) than is faced by models in supervised learning. This gives rise to many problems, one of which is that RL agents are forced to confront the so-called “exploration-exploitation tradeoff”. In this post, I will explore this tradeoff in the simplified context of multi-armed bandit problems. Specifically, I will investigate how different algorithms navigate this tradeoff in this setting. I will also provide the code for these algorithms so that anyone interested can play about with them further.]]></summary></entry><entry><title type="html">Variational Dropout in Recurrent Models</title><link href="https://tuphs28.github.io/blog/2024/Dropout-In-Recurrent-Models/" rel="alternate" type="text/html" title="Variational Dropout in Recurrent Models"/><published>2024-01-16T00:00:00+00:00</published><updated>2024-01-16T00:00:00+00:00</updated><id>https://tuphs28.github.io/blog/2024/Dropout-In-Recurrent-Models</id><content type="html" xml:base="https://tuphs28.github.io/blog/2024/Dropout-In-Recurrent-Models/"><![CDATA[<p>A workhorse of deep learning is dropout which is typically thought to help limit the extent to which models overfit to training data. However, the question of how to apply dropout to recurrent models that process inputs sequentially lacks a trivial answer. In this blog post, I will explore the problem of applying dropout to recurrent models and outline the idea of “variational dropout” as introduced in <a href="https://proceedings.neurips.cc/paper/2016/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf">Gal &amp; Ghahramani (2015)</a>. As such, this post is more or less a shoddy re-explanation of that paper, so the interested reader is encouraged to go read that. I have also implemented this as part of my <a href="https://github.com/tuphs28/BadTorch">BadTorch</a> project which I will probably write about at some point soon.</p> <h3 id="dropout">Dropout</h3> <p>A key capability that we wish for models to acquire is the ability to generalise from inputs they have learnt to correctly handle training to unseen inputs they encounter “out in the wild” when deployed. Generalisation requires training models on training datasets in such a way that they learn the generalisable patterns inherent to the data without fitting to the noise specific to the training data. This problem of fitting to the noise in the data is the so-called problem of overfitting.</p> <p>A common technique for encouraging models to avoid overfitting to the data is dropout. I assume the reader is somewhat familiar with dropout since it is pretty ubiquitous but will give a brief overview now in case this is not the case.</p> <p>Dropout, as introduced in <a href="https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Srivastava et al. (2014)</a>, is motivated by the idea that overfitting can arise when individual hidden units in a model co-adapt to specific quirks of the training data. The worry here is that, if we train a model for an overly long time on some training data, we may end up minimising loss be encouraging many hidden units to work together to represent noise in the training data that is unnrelated to the patterns we wish for it to learn. This is because the models will be able to better predict training labels by, effectively, memorising the training data rather than trying to infer the underlying pattern that explains the link between training examples and their labels.</p> <p>To avoid this from happening, Dropout “drops” hidden units at random while training. What this means is that whenever we pass a batch of training data through the model, we randomly set some hidden units to be zero before passing on to the next layer. A nice visual illustration of this is the below figure taken from the afforementioned original dropout paper.</p> <p align="center"> <img src="/assets/img/posts/2024-01-16/dropout_example.png"/> </p> <p>The idea here is that, by randomly “turning off” hidden units during training, hidden units cannot collectively learn to represent quirks of the training data since units can never depend on other units being present.</p> <p>More specifically, we choose a dropout strength \(p \in [0,1]\) and, with independent probabilities \(p\), randomly set each individal hidden unit to zero. In other words, dropout introduces a \(Bernoulli(p)\) random variable for each hidden unit that masks out that hidden unit when it is turned on (which, of course, happens with independent probability \(p\)).</p> <p>Now, since we are randomly zeroing out hidden units during training, the obvious question to ask is what do we do at test time? One option is to view dropout in Bayesian terms (more on this below) such that the dropout masks applied at each training pass correspond to drawing a sample of network weights from a distribution (since zeroing out hidden units leads to the corresponding weights not contributing to predictions). Then, we can provide a Monte Carlo approximation of the expected output of the model by running the model multiple times on each input and averaging the outputs. While nice in terms of its theoretical justification, this technique is not that popular in practice. Instead, people tend to multiply all weights by \(1-p\) at test time, with the thought being here that this corresponds fixing to the expected value of the hidden unit times any weight.</p> <p>The type of dropout described above is what is commonly meant when dropout is referred to in deep learning and has been found to be massively succesfull at preventing overfitting such that models can spend longer learning generalisable patterns in training data. As such, dropout is a form of regularisation. For the rest of this post, I shall refer to this of dropout that randomly masks out hidden units whenever data is passed through as being “standard dropout”.</p> <h3 id="applying-dropout-to-recurrent-models">Applying Dropout To Recurrent Models</h3> <p>We can then turn attention to regularising recurrent models like GRUs and LSTMS. While such recurrent models are maybe less popular than they were a few years ago given the advent of transformers, this is, I think, still a pretty interesting topic for two reasons:</p> <ul> <li>First, it relates to the broad question of generalisation in deep learning which is both hugely important and still lacking a completely solid theory. Investigating generalisation in the context of recurrent models is then of interest since it helps build a vague sense of intuition about how this kind of thing ends up working in practice.</li> <li>Second, while recurrent models aren’t quite as powerful as transformers given unlimited access to compute, I have personally found them to be better given limited compute.</li> </ul> <p>Whether or not standard dropout would be able to succesfully regularise a recurrent model is an interesting question. This is because, in a recurrent model like an LSTM, we pass each element in an input sequence through a model sequentially. Thus, standard dropout would involve applying a new dropout mask to the hidden units at each step of the sequence. Now, this intuitively seems like it might be a cause for concern since it conceptually leads to a different form of regularisation than when standard dropout is applied to a non-recurrent model. This is since we are now applying many different dropout masks over the course of a single forward pass of training. Thus, the gradient updates that will be being performed on the backward pass will lead to intuitively different kinds of updates as we will be backpropogating through combinations of multiple different dropout masks at once.</p> <p>Why might this be as problem? Well, intuitively, it means we are effectively adding more noise into the network at each step of the input sequence rather than just adding one injection of noise as with dropout in non-recurrent models. Hence, while the noise in standard dropout as applied to non-recurrent models is helpful for regularising, we might worry that standard dropout would inject sufficiently loud noise into a recurrent model that the model would no longer be able to infer the patterns hidden in the data through it. This then motivates the question of how to apply dropout to regularise a recurrent model.</p> <h3 id="a-brief-overview-of-bayesian-neural-networks">A Brief Overview of Bayesian Neural Networks</h3> <p>Before turning to a proposed solution to this problem, it is useful to first very briefly overview Bayesian neural networks. The core idea here is that we want to somehow approximate the predictive distribution \(p(y^*|x^*, X, Y)\) over the label \(y^*\) for some unseen input \(x^*\) given our training data \((X, Y)\). For classification tasks like those being considered here, this predictive distribution will be a distribution over the labels. If we call the parameters of a network \(\omega\), we can do this by introducing a posterior \( p(\omega|X, Y) \) over these parameters and then marginalising them out:</p> <p>\[p(y^*|x^*, X, Y)=\int{p(y^*|x^*, \omega)p(\omega|X, Y)d\omega}\]</p> <p>We often then place a standard Gaussian prior over the weights of the network and the likelihood \(p(y^*|x^*, \omega)\) is just the distribution over class labels for the given parameters \(\omega\).</p> <p>But, what actually is the posterior over network parameters? Well, as is the case a lot of the time in Bayesian statistics, this kind of distribution ends up being analytically intractable. However, we can approximate it using variational inference.</p> <h3 id="an-even-briefer-overview-of-variational-inference">An Even Briefer Overview of Variational Inference</h3> <p>What, then, is variational inference? The general idea here is that there is some distribution \(p(x)\) that is of interest but that is unknown. Variational inference then involves introducing some known distribution \(q(x)\) - called the “variational distribution” and then minimising the KL divergence (which measures how “far apart” two distributions are in an abstract sense) between the variational distribution and the distribution of interest. The resulting variational distribution then ought to be a good approximation to the original, unknown distribution.</p> <p>We can then use variational inference to find a variational approximation \(q(\omega)\) to the posterior over the network weights \( p(\omega|X, Y) \). If we are predicting class labels from a set of K possible labels, this KL divergence then looks like:</p> <p>\[KL(q(\omega)||p(\omega|X, Y)) \propto - \int{q(\omega)log(p(Y|X, \omega))d\omega} + KL(q(\omega)||p(\omega)) \]</p> <p>where \(p(\omega)\) is our prior over the network parameters. Now, say we have N training examples in the training dataset \((x_i, y_i)\) and let the logits produced by our model for the i-th training sequence when our parameters are \(\omega\) be denoted \(f^\omega(x_i)\). We can then decompose the KL further as:</p> <p>\[KL(q(\omega)||p(\omega|X, Y)) \propto - \sum_{i=1}^N\int{q(\omega)log(p(y_i|f^\omega(x_i)))d\omega} + KL(q(\omega)||p(\omega)) \]</p> <h3 id="variational-dropout">Variational Dropout</h3> <p>So, how does this help us with the problem of applying dropout to recurrent neural networks? Well, the basic idea is that we want to find some variational approximation to our posterior over the weights and then use this posterior to evaluate the predictive distribution for any test input sequence.</p> <p>To do this, though, we need to be able to evaluate the KL above. We do this by using Monte Carlo approximations (each with a sample size of 1) to the integral terms so that the expected log likelihood for each training sequence is just given by the empirical log-likelihood:</p> <p>\[\int{q(\omega)log(p(y_i|f^\omega(x_i)))d\omega} \approx log(p(y_i|f^\omega(x_i))), \omega \sim q(\omega)\]</p> <p>which then gives an unbiased estimator for each term of the sum. We then put this back into the original equation to get the loss that we optimise with respect to:</p> <p>\[L = - \sum_{i=1}^Nlog(p(y_i|f^\omega(x_i))) + KL(q(\omega)||p(\omega))\]</p> <p>Finally, we need to choose a form for the variational approximation. Gal &amp; Ghahramani choose to do this by putting a mixture of Gaussians variational approximation over each row of each weight matrix. So, the variational approximation associated with some row of some weight matrix is:</p> <p>\[q(w_j) = pN(w_j; 0, \sigma^2I) + (1-p)N(w_j; m_k, \sigma^2I)\]</p> <p>Crucially, this then links us back to dropout - in sampling model parameters to evaluate the log-likelihood, we are sampling weights from a mixture that puts probability \(p\) on zeroing out rows of a weight matrix! Note that we can then appoximate the KL between the variational approximation and the prior as L2 weight decay on the variational parameters \(m_k\). Having put this all in place, we now have a tractable optimisation objective! To evaluate the loss we merely have to pass our training sequences through the model to evaluate the empirical log likelihoods, and then perform L2 weight decay.</p> <p>Finally, we need to be able to evaluate the original predictive distribution at test time in some way. As mentioned at the beginning, we can do this using another Monte Carlo approximation (e.g. sampling many times and averaging out to get final predictions) but performance seems just as good if we merely scale weights by \(1-p\) so that we fix the expectation (think of this as being analogous to approximating the predictive distribution with the MAP estimate for a parameter).</p> <h3 id="putting-it-all-together">Putting It All Together</h3> <p>This all basically a lot of maths to say the following: if we perform dropout with the <em>same</em> dropout mask applied at each step of a sequence in a recurrent model (e.g. we drop out the same hidden units and input units at each sequence step) we are effectively performing approximate inference in the above sense. This means we have a principled way to perform dropout in recurrent models.</p> <p>More precisely: by randomly dropping the same hidden units at each time step, we are effectively sampling rows of each weight matrix from the mixture of Gaussians approximatng distribution. We can then perform approximate inference and generate samples from the predictive distribution at test time! (NB - this is actually using a “tied” version of the algorithm in the sense that zeroing out hidden and input units ties dropout masks across multiple weight matrices but this isn’t hugely important for getting what is going on)</p> <p>The idea here is nicely illustrated by the below figure taken from Gal &amp; Ghahramani’s paper. The core takeaway that it is trying to get across is that we are applying the same dropout masks to the respective weight matrices at each time step rather than randomly sampling a new mask at each step.</p> <p align="center"> <img src="/assets/img/posts/2024-01-16/variational.png"/> </p>]]></content><author><name></name></author><category term="deep-learning"/><summary type="html"><![CDATA[A workhorse of deep learning is dropout which is typically thought to help limit the extent to which models overfit to training data. However, the question of how to apply dropout to recurrent models that process inputs sequentially lacks a trivial answer. In this blog post, I will explore the problem of applying dropout to recurrent models and outline the idea of “variational dropout” as introduced in Gal &amp; Ghahramani (2015). As such, this post is more or less a shoddy re-explanation of that paper, so the interested reader is encouraged to go read that. I have also implemented this as part of my BadTorch project which I will probably write about at some point soon.]]></summary></entry><entry><title type="html">Exploring Tradeoffs Between Safety Metrics with MNIST</title><link href="https://tuphs28.github.io/blog/2024/Exploring-Tradeoffs-Between-Safety-Metrics/" rel="alternate" type="text/html" title="Exploring Tradeoffs Between Safety Metrics with MNIST"/><published>2024-01-08T00:00:00+00:00</published><updated>2024-01-08T00:00:00+00:00</updated><id>https://tuphs28.github.io/blog/2024/Exploring-Tradeoffs-Between-Safety-Metrics</id><content type="html" xml:base="https://tuphs28.github.io/blog/2024/Exploring-Tradeoffs-Between-Safety-Metrics/"><![CDATA[<p>Despite huge advances in their capabilities when measured along standard performance dimensions (e.g. recognising images, producing language, forecasting weather patterns etc…), many deep learning models are still surprisingly “brittle”. While there are many dimensions along which this “brittle-ness” shows itself, three interesting ones are:</p> <ol> <li>some models are surprisingly <strong>non-robust</strong> to inputs perturbed in specific, minimal ways not seen during training</li> <li>some models are frequently unable to tell when an input is <strong>out-of-distribution</strong> or unlike other inputs it has seen</li> <li>some models are still <strong>poorly calibrated</strong>, in the sense that their internal representations of uncertainty do not accurately reflect reality</li> </ol> <p>In this post I will outline these issues - which we can view as being dimensions along which a model can be “safe” - with respect to a simple classifier trained on MNIST in the context of a project I did a while back. In retrospect, I think the project left a lot to be desired (I’ll get into this in the empirical section below), but I still think writing up this summary is worth it. For one, this was the first ever ML project that I did and I am definitely nostalgic about it since it kinda set me on the path I am on now. Additionally, I still think the topic (of empirical “AI safety” in the broad sense) is absolutely fascinating.</p> <h3 id="1---background">1 - Background</h3> <h5 id="11---model-robustness">1.1 - Model Robustness</h5> <p>One ways in which models can be “brittle” is that they can suffer from a lack of robustness to minor changes in inputs. In the context of an image classifier, this lack of robustness exhibits itself as models being unable to accurately classify images that have undergone pertubations that are irrelavant / unnoticable to humans but that cause the model to fail.</p> <p>One kind of such image are distribution-shifted images. We can roughly see these as being images that, to a human, seem just as easy to classify as the images the model was trained on, but that have some visual effect - like snow or jitter - applied to them. Thus, we can think of them as being images drawn from a different distribution to the training examples, but from a distribution that seems qualitatively similar enough that the broad semantics remain unchanged and so model performance should transfer fairly well.</p> <p>For example, the following are some distortions from the MNIST-C dataset (Mu and Gilmer, 2019; figure 1 from their paper). In all of these cases, despite looking like an easy “3” to classify, baseline MNIST classifiers often struggle hugely.</p> <p align="center"> <img src="/assets/img/posts/2024-01-08/MNIST_C.png.png"/> </p> <p>Another kind of image that image classifiers are often non-robust to are adversarially distorted images. These are images that an “adversary” has distorted in such a way as to make it hard for a model to classify accurately.</p> <p>A common type of adversarial attack is a gradient-based adversarial attack. The basic idea here is that an adversary works out the gradient of the loss with respect to each pixel in an image and then takes mini steps in the direction that increases loss. There are many types of gradient-based attacks but I will just consider two popular variants.</p> <p>The first attack is the Fast Sign Gradient Method (FGSM) as introduced in (Goodfellow, Schlens, and Szegedy 2014). If \(x\) is an image, \(y\) is its label and \(l(x, y)\) is the loss associated with the model for whatever the model predicts for this image, then the FGSM-attacked image is given by: \[x_{FGSM}=x+\varepsilon \times sign(\nabla_xl(x, y))\]</p> <p>Thus, FGSM distorts each pixel by an amount \(\varepsilon\) (which we call the “attack budget” and is a hyperparameter set by the user) in a direction that increases model loss. This ensures that the \(l_{\infty}\) norm of the distortion is trivially bounded by \(\varepsilon\). Basically, FGSM just performs a single bounded step of gradient ascent. In practice, this results in images that seem very similar to the human eye but that models struggle to classify accurately. The below example illustrates a 4 from the MNIST data before and after an FGSM adversarial attack is applied:</p> <p align="center"> <img src="/assets/img/posts/2024-01-08/FGSM.png"/> </p> <p>The second kind of adversarial attack I considered was projected gradient descent (PGD), which we can kind of think of as being a more powerful form of FGSM. PGD is very similar to FGSM except that it performs many small steps of gradient ascent rather than one big one as FGSM does. The broad steps of a PGD attack with an attack budget of \(\varepsilon\) and step count of \(N\) are as follows:</p> <ul> <li>First, randomly jitter the image \(x\) \[x_{PGD}=x+\eta, \eta \sim U[-\varepsilon, \varepsilon]\]</li> <li>Then, for \(t=1,…,N\), perform a step of gradient ascent to distort each pixel in a direction that increases loss \[x_{PGD}=x_{PGD} + \beta \times sign(\nabla_{\delta}l(x_{PGD}+\delta, y))\]</li> <li>After each step, if the \(l_{\infty}\) norm of the total distortion exceeds the attack budget, clip the distortion back into the \(l_{\infty}\) ball of size \(\varepsilon\) around the original image</li> </ul> <p>So, this PGD attack performs multiple steps of gradient ascent to find a distortion that remains in a \(\varepsilon\)-ball (in terms of \(l_{\infty}\) norm) around the original image (though we can equally well use PGD with other norms). Thus, PGD can be seen as trying to find a “better” distortion than FGSM while remaining within the same attack budget. An example where I have applied PGD to an MNIST digit is shown below:</p> <p align="center"> <img src="/assets/img/posts/2024-01-08/PGD.png"/> </p> <h5 id="12---detecting-out-of-distribution-inputs">1.2 - Detecting Out-Of-Distribution Inputs</h5> <p>Alongside robustness, another dimension along which a model can be “safe” is its ability to recognise out-of-distribution (OOD), or, anomalous, inputs. The idea here is that we want a model to realise when it is fed an input that is radically different from other inputs it has seen. This will become an increasingly important consideration as models are deployed in more and more sensitive real-world applications.</p> <p>The most common kind of technique for endowing models with the ability to detect anomalies involves anomaly scoring. Anomaly scoring techniques extract from models an “anomaly score” for each input, classifying that input as an anomaly if the score crosses some threshold. A common scoring technique involve using of maximum softmax or logit values for an input (Hendrycks and Gimpel 2017).</p> <p>Two examples of inputs that we might want an MNIST classifier to flag as OOD are shown below. The examples are, respectively, from the F-MNIST dataset (Xiao, Rasul, and Vollgraf 2017) and K-MNIST dataset (Clanuwat et al. 2018)</p> <p align="center"> <img src="/assets/img/posts/2024-01-08/OOD_Examples.png"/> </p> <p><img src="/assets/images/mnist_robustness/OOD_Examples.png" alt="OOD Example"/></p> <h5 id="13---calibration">1.3 - Calibration</h5> <p>The final dimension of model safety I considered was calibration. The idea here is that we want models to be able to quantify how certain / uncertain they are in their outputs since there is a lot of difference between an output that a model is 50.1% confident in as opposed to one it is 99.9% confident in. As such, we want to train our models not just to be accurate, but also to be well calibrated such that, for instance, when it makes a prediction with 60% confidence, it will be correct in that prediction 60% of the time</p> <h5 id="14---techniques-for-improving-safety">1.4 - Techniques for Improving Safety</h5> <p>I considered two commonly-proposed general techniques for improving model safety along the outlined dimensions. The first of these is the use of data augmentation during the training process, while the second is architectural changes.</p> <p>Many commonly-proposed solutions to our safety issues involve using specific techniques to augment the data on which a model is trained on in some way. There are, broadly, two rough categories of such techniques.</p> <p>The first of these is adversarial training (Madry et al. 2017), in which a certain portion of the data on which a model is trained on is adversarially distorted using some adversarial attack method. The hope here is that such adversarially distorted training data can act as a sort of regulariser. I only considered PGD and FGSM adversarial training, although other types do exist and this is something I would definitely change if I re-did this work.</p> <p>The second category of such techniques involves augmenting the training data in non-adversarial ways but similarly aiming to mimic regularisation. Many such techniques exist, but I focused on the data augmentation techniques of CutOut (DeVries and Taylor 2017) and MixUp (Zhang et al. 2017). In CutOut, a specified number of squares of specified size are “cut out” of the image in order to hide certain parts of the image from the model, and encourage the learning of more robust features (DeVries and Taylor 2017). In MixUp, input images are combined in a convex way such that the label which the model ought to assign to them is the respective convex combination of labels (Zhang et al. 2017). Both of the data augmentation techniques are illustrated below.</p> <p align="center"> <img src="/assets/img/posts/2024-01-08/Data_aug.png"/> </p> <p>The final category of techniques I consider in this report are modifications to models that we could group under the label of “architectural”. The first of these modifications I consider is model size, since larger models are frequently found to perform better with regards to many safety metrics (Hendrycks et al. 2021). The second of these modifications I consider is model ensembling. Model ensembling involves combining the outputs of many separately trained models in order to produce a final output</p> <h3 id="2---methodology">2 - Methodology</h3> <h5 id="21---model-implementations">2.1 - Model Implementations</h5> <p>The baseline model I used for comparison was the modification of the standard Le-Net 5 that is frequently used to illustrate the power of convolutional networks on the MNIST digit recognition problem. As expected, this model achieves a high accuracy rate (0.993) on the standard MNIST problems. I modified this network with various forms of the above techniques for improving safety.</p> <h5 id="22---safety-metrics">2.2 - Safety Metrics</h5> <p>To benchmark robustness to adversarial attacks, I used as a metric the accuracy of each model in the face of the PGD and FGSM adversarial attacks over different attack budgets.</p> <p>To benchmark robustness To benchmark robustness to distribution shift, I measure model accuracy on the MNIST-C dataset I discussed above.</p> <p>To explore the ability of models to detect OOD inputs in our MNIST case, I used the F-MNIST and K-MNIST datasets as examples of OOD inputs that our MNIST classifier ought to recognise as OOD. I then tested the ability of different methods to successfully detect OOD inputs, using the AUROC scores as a metric for success.</p> <p>To test calibration, I tracked the expected calibration error, RMS calibration error and brier score for each model on the original MNIST dataset. These metrics are all different measures of the same underlying concept, which is the ability of models to make well calibrated predictions such that an event X will happen 60% of the time when the models predict X with a confidence of 0.6</p> <h3 id="3---results">3 - Results</h3> <h5 id="31---adversarial-robustness">3.1 - Adversarial Robustness</h5> <p>The first area I explored was adversarial robustness, which I measured using the accuracy of models in the face of adversarial attacks of increasing strength. An interesting pattern emerges here. Across all models and all attack budgets, there appears to be three qualitatively different types of models. The figures below, which outline the adversarial accuracy for all models across all attack budgets with select models labelled, illustrates this pattern.</p> <p align="center"> <img src="/assets/img/posts/2024-01-08/AR_main.png"/> </p> <p>The pattern that is illustrated in these figures is that all the models tested fall into one of three categories.</p> <p>First, we have a set of models that perform consistently worse than all other models. This set consists of the baseline model, and all models trained purely using those data augmentation techniques that aim to improve OOD robustness (e.g. Cutout and Mixup). Indeed, further inspection of this set of models reveals that the models trained using these OOD robustness enhancing techniques actually perform worse with regard to adversarial robustness than the baseline model. One interesting point here is that CutOut seems to be the superior data augmentation technique of the two from the point of view of adversarial robustness since it results in less of a decline in accuracy relative to the baseline.</p> <p>The next overall set of models is the set of models that consistently achieve high accuracy in the face of both attacks across a range of attack budgets. This set consists of three types of models. It consists of those models that were trained solely using adversarial training, with FGSM and PGD adversarial training achieving similar results. It also contains models trained with both adversarial training and augmented training examples.</p> <p>The finally qualitatively distinct set of models is those models that outperform the baseline models and augmentation models, and yet underperform the models trained using some form of adversarial training. This set consists of ensemble models, and the large version of the baseline model.</p> <h5 id="32---out-of-distribution-robustness">3.2 - Out-Of-Distribution Robustness</h5> <p>The idea that there is a tradeoff between adversarial robustness and OOD robustness is, somewhat at least, supported by the experiments I performed regarding the robustness of models to distribution shift.</p> <p>From a high-level perspective, two conclusions arise when considering robustness to distribution shift. Firstly, many models follow a similar pattern such that they all suffer precipitous declines in accuracy when faced with the fog and impulse noise distortions. Secondly, while most models seem to follow similar overall patterns, there is much variability in accuracy with regard to each individual distortion.</p> <p>The pattern with respect to ensembles remains similar to, albeit less pronounced than, with adversarial robustness. Specifically, all ensemble models slightly outperform the baseline model, with large ensembles doing better. However, the pattern for larger models is reversed, with larger models being less robust to distribution-shifted inputs. This provides mixed evidence for the hypothesis that there is a tradeoff between adversarial and OOD robustness. On one hand, larger models seem to improve adversarial robustness whilst hurting OOD robustness, while on the other large ensembles seem to improve both.</p> <p>However, more evidence in favour of the tradeoff hypothesis comes from inspection of the data regarding adversarially trained models. All models trained purely with adversarial training (which happened to be the best performing models in terms of adversarial robustness) are less accurate on average over the OOD MNIST-C dataset. Another point of interest here is that those models that were adversarially trained with a stronger attack budget (which happened to be the most adversarially robust) have, on average, lower OOD robustness than those models trained with a smaller attack budget. Finally, it should be noted that adversarial training actually seems to improve model accuracy with respect to a specific distortions. This is perhaps interesting as it points towards adversarial training encouraging the learning of specific representations that are helpful in some contexts but harmful in others</p> <p>Finally, we can turn attention to models trained using both data augmentation and adversarial training. I found little evidence that the order of the two processes made much difference in terms of OOD robustness.</p> <h5 id="33---anomaly-detection">3.3 - Anomaly Detection</h5> <p>As previously discussed, another important “safety” property that ML models can have is the ability to detect inputs that are anomalous compared to their training data. We can measure the ability of models to do this by providing them with a dataset consisting of both examples similar to those they were trained on and examples representing a distribution shift relative to those they were trained on, and then tracking their ability to use whatever anomaly scoring method we decide on to successfully detect the anomalous examples. We track this using the AUROC, where an AUROC of 1 corresponds to perfect classification of anomalous inputs, and an AUROC of 0.5 represents random chance.</p> <p>Some of the key results here are as follows. Firstly, all models achieve relatively high AUROCs on both datasets, with most models achieving AUROCs of at least 0.90 across all datasets.</p> <p>Turning our attention to specific families of models, more definitive conclusions arise. Let’s first consider ensemble models and the large version of our baseline model. A notable finding is that all ensemble models achieve higher AUROCs than the baseline model, with the improvement seeming to grow as the size of the ensemble increases. This is notable since this is the same pattern that emerges when considering both adversarial robustness and OOD robustness, implying that we can increase model safety metrics without incurring tradeoffs by using model ensembles.</p> <p>However, this finding does not replicate for the larger version of the baseline model. Whilst this large model achieves a somewhat comparable AUROC for the K-MNIST data, it significantly underperforms the baseline model for F-MNIST. As such, given the improvements to adversarial robustness from this larger model, larger models seem to increase robustness at the cost of reducing their ability to detect anomalies.</p> <p>Shifting focus to purely adversarially trained models, few conclusions seem to arise. For models adversarially trained using both PGD and using FGSM, no consistent pattern in AUROCs seems to arise and the overall effect remains ambiguous. In both cases, all models seem to achieve roughly comparable AUROCs as the baseline model.</p> <p>More interesting findings emerge when we consider the performance of models trained using data augmentations. Specifically, I found that models trained using Mixup were notably less able to detect OOD examples than the baseline model, implying a tradeoff between OOD robustness and anomaly detection.</p> <p>However, I found that the opposite was true for all models trained using some form of cutout. Specifically, I found that models trained using Cutout (either in isolation, or combined with some form of adversarial distortion) were significantly better at detecting anomalous inputs than the baseline mode, scoring higher AUROCs</p> <h5 id="34---calibration">3.4 - Calibration</h5> <p>Finally, we can turn attention to the performance of the different families of models with regards to calibration. Like with adversarial robustness, there are again three broad tiers of models.</p> <p>The first tier is the worst overall tier that scores notably higher than the other models (including the baseline) on all calibration metrics. Interestingly, this tier of models consists solely of those models that were trained by first applying Mixup and then applying adversarial training. This is a notable finding, since it is the only case (other than that of ensemble models) where models of a single type are in a category of their own.</p> <p>Skipping a tier, the third and best tier of models consists of ensemble models. Ensemble models achieve better results than all other models and significantly outperform the baseline model. Additionally, the larger the size of the ensemble model, the better calibrated it is</p> <p>Finally, the middle tier of models consist of all other models I considered, with pretty much all of them achieving worse calibration results than the baseline. This includes all models trained using adversarial attacks, data augmentations and any mix of the two except mixup followed by adversarial distortions</p> <h3 id="4---conclusion">4 - Conclusion</h3> <p>Having considered all of the above experiments, we can finally return to the question of whether or not there are tradeoffs between different safety metrics that it might be desirable for a model to have. I would contend that the experiments conducted here are mixed in their answer to this question.</p> <p>On one hand, they imply that yes, we can achieve good performance on all these safety metrics if we use ensemble models. This is because ensemble models outperformed the baseline model with regard to all four of the safety properties tested here (adversarial robustness, OOD robustness, anomaly detection and calibration).</p> <p>However, this affirmative response must be qualified. This is because ensemble models do not always achieve the highest performance on any single calibration metric, and those models that do achieve this best-in-class performance on any single safety metric often end up underperforming the baseline model on another metric. For instance, the models that achieved the best adversarial robustness were those models that were trained using some form of adversarial training. For example, many adversarially trained models outperformed all ensemble models by &gt;50% in terms of adversarial accuracy, yet end up being incomparably worse when it comes to OOD robustness. As such, in the sense of achieving the best result possible on any single metric, it does appear that there is sometimes a tradeoff</p> <h3 id="5---reflections-on-these-experiments">5 - Reflections on these Experiments</h3> <p>Looking back, there’s a lot in this work I am not happy with. A few key things that stand out are:</p> <ul> <li>I trained models on the same adversarial attacks as I tested their adversarial robustness using - this goes in the face of work that has found that robustness to one type of attack does not guarantee robustness to others</li> <li>I only considered very weak adversarial attacks - the literature has advanced considerably since PGD and FGSM came out, and much more powerful variants now exist</li> <li>All experiments were done with MNIST - some work has found that robustness on MNIST doesn’t correlate with robustness on other datasets due to its almost “toy” nature</li> </ul> <p>At some point I’d like to re-do this kind of research on something like ImageNet with an eye to the current state of the literature. This is because not only do I think this topic is really interesting, but I also think this “pragmatic view of AI safety” will become increasingly important as ML models are embedded in the world around us in the future.</p> <h3 id="references">References</h3> <p>The following is a full list of reference I cited in the report that might be of varying degrees of interest:</p> <p>Borowski, Judy, Roland S. Zimmermann, Judith Schepers, Robert Geirhos, Thomas S. Wallis, Matthias Bethge, and Wieland Brendel. 2021. “Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization.”</p> <p>Bostrom, Nick. 2014. Superintelligence: Paths, Dangers, Strategies. Oxford: Oxford University Press.</p> <p>Clanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazauki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.”</p> <p>Cunn, Yann L., Leom Bottou, Yoshua Bengio, and Patrick Haffner. 1998. “Gradient-Based Learning Applied to Document Recognition.” PROCEEDINGS OF THE IEEE.</p> <p>Deng, Li. 2012. “The mnist database of handwritten digit images for machine learning research.”</p> <p>DeVries, Terrance, and Graham W. Taylor. 2017. “Improved Regularisation of Convolutional Neural Networks with Cutout.”</p> <p>Goodfellow, Ian J., Jonathon Schlens, and Christian Szegedy. 2014. “Explaining and Harnessing Adversarial Examples.”</p> <p>Hendrycks, Dan, Steve Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, et al. 2021. “The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization.”</p> <p>Hendrycks, Dan, Nicolas Carlini, John Schulman, and Jacob Steinhardt. 2022. “Unsolved Problems in ML Safety.”</p> <p>Hendrycks, Dan, and Kevin Gimpel. 2017. “A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks.” ICLR 2017.</p> <p>Hendrycks, Dan, Mantas Mazeika, and Thomas Dietterreich. 2018. “Deep Anomaly Detection with Outlier Exposure.” ICLR 2019.</p> <p>Kingma, Diederik P., and Jimmy L. Ba. 2015. “Adam: A Method for Stochastic Optimization.” ICLR 2015.</p> <p>Lipton, Zachary. 2016. “The Mythos of Model Interpretability.” 2016 ICML Workshop on Human Interpretability in Machine Learning.</p> <p>Madry, Aleksander, Aleksander Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017. “Towards Deep Learning Models Resistant to Adversarial Attacks.”</p> <p>Mu, Norman, and Justin Gilmer. 2019. “MNIST-C: A Robustness Benchmark for Computer Vision.”</p> <p>Olah, Chris, Alexander Mordvintsev, and Ludwig Schubert. 2017. “Feature Visualization.”</p> <p>Ovadia, Yaniv, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. 2019. “Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift.” 33rd Conference on Neural Information Processing Systems (NeurIPS 2019).</p> <p>Russel, Stuart. 2019. Human Compatible: Artificial Intelligence and the Problem of Control. N.p.: Viking.</p> <p>Schott, Luks, Jonas Rauber, Mathias Bethge, and Wieland Brendel. 2018. “Towards the first adversarially robust neural network model on MNIST.”</p> <p>Strauss, Thilo, Markus Hanselmann, Andrej Junginger, and Holger Ulmer. 2018. “Ensemble Methods as a Defence to Adversarial Perturbations Against Deep Neural Networks.”</p> <p>Wang, Haoqi, Zhizhong Li, Litong Feng, and Wayne Zhang. 2022. “ViM: Out-Of-Distribution with Virtual-logit Matching.”</p> <p>Xiao, Han, Kashif Rasul, and Roland Vollgraf. 2017. “Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms.”</p> <p>Yun, Sangdoo, Dongyoon Han, Seong J. Ooh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. 2019. “CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features.”</p> <p>Zhang, Hongyi, Moustapha Cisse, Yann Dauphin, and David Lopez-Paz. 2017. “Mixup: Beyond Empirical Risk Minimisation.”</p>]]></content><author><name></name></author><category term="robustness,"/><category term="ai-safety"/><summary type="html"><![CDATA[Despite huge advances in their capabilities when measured along standard performance dimensions (e.g. recognising images, producing language, forecasting weather patterns etc…), many deep learning models are still surprisingly “brittle”. While there are many dimensions along which this “brittle-ness” shows itself, three interesting ones are: some models are surprisingly non-robust to inputs perturbed in specific, minimal ways not seen during training some models are frequently unable to tell when an input is out-of-distribution or unlike other inputs it has seen some models are still poorly calibrated, in the sense that their internal representations of uncertainty do not accurately reflect reality]]></summary></entry><entry><title type="html">Basic MCMC Pt. 1: An Intro to Monte Carlo Methods</title><link href="https://tuphs28.github.io/blog/2024/Monte-Carlo-Methods/" rel="alternate" type="text/html" title="Basic MCMC Pt. 1: An Intro to Monte Carlo Methods"/><published>2024-01-07T00:00:00+00:00</published><updated>2024-01-07T00:00:00+00:00</updated><id>https://tuphs28.github.io/blog/2024/Monte-Carlo-Methods</id><content type="html" xml:base="https://tuphs28.github.io/blog/2024/Monte-Carlo-Methods/"><![CDATA[<p>A common computation we face in machine learning is computing expectations. When working in \(\mathbb{R}^d\), for instance, we often face integrals of the form: \[\mathbb{E}_{X \sim p}[f(X)] = \int{f(x)p(x)dx}\] Sometimes we can compute these analytically (i.e. through the use of rules of integration). However, many such integrals we face in ML are such that providing an analytic solution is intractable or impossible. It is this exact situation that motivates Monte Carlo (MC) methods.</p> <h3 id="a-deterministic-approximation">A Deterministic Approximation</h3> <p>Imagine we have some random variable X that is distributed according to a truncated exponential with a rate of 1. \[X \sim p(x)\text{ where } p(x) = \frac{1}Ze^{-x}, x\in[0,10] \] Z here is just a normalising constant to ensure that the support has measure 1 and the PDF integrates to 1. The density of X looks as follows:</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/pdf_exp.png"/> </p> <p>Now, say that we want the expected value of X. For reference, this is ~1 (or, to be precise, it is \(1-e^{10}=0.9999546\)). However, say that we cannot, for whatever reason, perform the below integral analytically. What do we do? \[I = \mathbb{E}_{X \sim p}[X] = \frac{1}Z\int_0^{10}{xe^{-x}dx}\]</p> <p>One idea might be to try and use some numerical method to evaluate the integral deterministically. Basically, this means dividing the domain of integration up into blocks, approximating the value of the integrand for each block and summing these approximations. For instance, we might try dividing [0,10] up into unit-length intervals [0,1), [1,2) and so on, taking the value of \(xe^{-x}\) at the midpoint of each interval and then approximating the value of the whole integral as the sum of these midpoint approximations. Doing this yields an approximate value for the expectation of ~0.959. This seems like a pretty good guess for using such wide interval in our approximation, and we might think the best thing to do would just be to repeat this but using increasing narrow intervals.</p> <p>This approach to evaluating integrals (roughly) corresponds to the idea of “Riemann discretization”. However, it leaves much to be desired. There are a few reasons for this:</p> <ol> <li>The approximation we generate will be biased for any finite number of subdivisions of the domain of integration. This means any downstream use of the approximation will be incorrect in a systematic way.</li> <li>As we have used a deterministic method, the nature of this bias is hard to characterise.</li> <li>What happens when we are working in \(\mathbb{R}^d\) and d is large? We will need exponentially many subdivisions of the domain of integration to generate a reasonably accurate approximation. We can see this as an instance of the “curse of dimensionality”. Importantly, d often is huge in ML applications.</li> </ol> <h3 id="monte-carlo-approximations">Monte Carlo Approximations</h3> <p>Importantly, the Monte Carlo approximation of an expectation overcomes all three of these shortcomings. An MC approximation works of the expected value of a distribution works as follows:</p> <ul> <li>First, we sample N independent draws from the PDF p \[X_1\sim p(x_1),…X_N\sim p(x_n) \]</li> <li>Then, we take generate our approximation the expected value, I, by summing them and dividing by N \[\hat{I}=\frac{1}N\sum_{n=1}^NX_n\]</li> </ul> <p>Doing this for N=1000 (i.e. averaging over 1000 samples) for the above exponential example, we get an approximation of 0.975. This is very close to the true value!</p> <p>More generally, if we wish to evaluate an expectation of the form \(I=\mathbb{E}_{X \sim p}[f(X)] = \int{f(x)p(x)dx}\), then the corresponding (vanilla) Monte Carlo estimator is defined as follows in \(\mathbb{R}^d\):</p> <ul> <li>Suppose we can generate N independent and identically distributed (i.i.d) samples from the target measure p \[X_1\sim p(x_1),…X_N\sim p(x_n) \]</li> <li>The, the vanilla MC is defined as: \[\hat{I}=\frac{1}N\sum_{n=1}^Nf(X_n)\]</li> </ul> <p>Of course, this assumes that we can actually sample from the distribution of interest (which we rarely can in practice in Bayesian ML), but ignoring this we have a powerful estimator!</p> <p>Importantly, as a function of random variables, the MC estimator \(\hat{I}\) is itself a random variabe. This means it has the properties of a random variable (such as a mean, a variance and a distribution) that allow us to characterise it. Importantly, it is therefore provably unbiased such that, unlike the deterministic approximation, it will not yield systematically wrong estimates. It is easy to show this: \[\mathbb{E}[\frac{1}N\sum_{n=1}^Nf(X_n)]=\frac{1}N\sum_{n=1}^N\mathbb{E}[f(X_n)]=\frac{N}N\mathbb{E}[f(X)]=\mathbb{E}[f(X)]\]</p> <p>Additionally, unlike the deterministic method, Monte Carlo estimators are such that we can characterise their error by looking at their variance: \[Var(\hat{I})=Var(\frac{1}N\sum_{n=1}^Nf(X_n))=\frac{Var(f(X))}N\]</p> <p>So, what does this all mean?</p> <ul> <li>Well, as already stated it means that the estimator \(\hat{I}\) is an unbiased estimator of the expectation we seek to evaluate!</li> <li>And, it doesn’t rely on some deterministic partition of the domain/support of the random variable like the Riemann discretization - this means we don’t suffer from the curse of dimensionality and can easily scale this method up to random variables living in higher dimensions!</li> <li>Even better we can quantify the level of error expected in our approximation by considering the variance of the estimator around the true value of the expectation - this lets us be confident of better and better approximations as we increase the number of samples N and decrease the variance of the estimator!</li> <li>Also, we can tell how “difficult” an expectation will be to approximate well (in terms of the number of samples we require to get a reasonably accurate approximation) since the variance of the estimator also depends on the variance of the function \(Var(f(X))\) under the sampling distribution. Hence, even if N is huge, if this variance is large we can tell a priori that our Monte Carlo approximation will leave much to be desired.</li> <li>Finally, it can pretty easily be shown that this estimator obeys the Central Limit Theorem, and hence converges in distribution to the normal: \(\hat{I} \sim N(I, \frac{Var(f(X))}N)\)</li> </ul> <p>Now, let’s see some of these properties in action for the truncated exponential example. First, let’s see that the MC approximation of the mean of this distribution improves as N increases. This is shown below, with the graph demonstrating the true value of the expectation (the dotted line) and the MC approximation for samples of different N:</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/basic_mc_est.png"/> </p> <p>Clearly, as predicted by the fact that variance around the true value of the expectation decreases in N, as N increases, we get better and better approximations. We can see this “shrinking variance” effect by considering multiple different MC approximations. The graph below shows the MC approximations generated by 5 different, randomly-initialised MC approximations for different sample sizes:</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/multi_mc_est.png"/> </p> <p>Clearly, as N decreases, we can see that the variance of these approximations decreases.</p> <h3 id="problematic-functions">Problematic Functions</h3> <p>However, as mentioned above, some functions can be problematic in the sense that vanilla MC estimators struggle to converge to their true value for any reasonable finite value of N. This happens when the variance - or, “energy” - of the function under the sampling distribution is sufficiently high that increasing N doesn’t reduce the variance of the MC estimator.</p> <p>For instance, say we want to work out the expectation of \(f(x)=0.000001e^{2x}\) under our truncated exponential distribution. I have plotted both the function and the PDF of the distribution below - what do we see?</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/high_energy_pdf.png"/> </p> <p>Well, what we see is that the region of high density under our distribution (which is the region in which most samples will be drawn) is on the left of the support while the important, non-zero region of f(x) is on the right. This means that the vast, vast majority of samples drawn from p will not contribute at all to the value of the expectation, and samples that do contribute to the value of the integral (e.g. samples that are near 10) will be exceedingly rare. In other words, the variance of the function under the distribution will be huge.</p> <p>This means that the variance of the MC estimator will remain large even for large N, and will hence struggle to converge for reasonable sample sizes. This is illustrated in the graph below, where I plot the MC estimates for samples up to size 25,000 and we can see that the MC estimate still clearly hasn’t converged to the true value (dotted line).</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/high_energy_mc_est.png"/> </p> <p>Indeed, this graph illustrate the problem well - most samples are nearly zero under f(x) (causing the estimate to decay toward zero most of the time), but very occasionally we get a sample near the right-hand side of the support that yields a huge value of f(x), causing the estimate to skyrocket. We can see that this causes high variance by considering 5 independent runs of MC estimates for this function:</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/high_energy_var.png"/> </p> <p>So, what can we do when we want to take expectations of such functions?</p> <h3 id="importance-sampling">Importance Sampling</h3> <p>One option we can take here is called importance sampling (IS). The idea of IS is, rather than sampling from p which is not in good alignment with f, to sample from some other distribution that is better aligned with f. In other words, we want to sample from some alternative distribution q that is more likely to produce samples from regions of the domain that are important for evaluating the expectation of f.</p> <p>Now, if we can produce samples from q, we can use this to construct an IS Monte Carlo estimate of \(\mathbb{E}_{X \sim p}[f(X)]\) since:</p> <p>\[\mathbb{E}_{X \sim p} [f(X)]=\int{f(x)p(x)dx}=\int{\frac{f(x)p(x)}{q(x)}q(x)dx} = \mathbb{E}_{X \sim q}[\frac{f(x)p(x)}{q(x)}]\]</p> <p>Hence, we can define the IS Monte Carlo estimator as follows:</p> <ul> <li>Suppose we want to estimate the expectation: \[I=\mathbb{E}_{X \sim p}[f(X)] = \int{f(x)p(x)dx}\]</li> <li>Further, suppose we have some alternative probability measure q (called the “importance distribution”) such that \(q(x) \neq 0 \) whenever \(p(x) \neq 0 \) from which we can generate N i.i.d samples: \[X_1\sim q(x_1),…X_N\sim q(x_n) \]</li> <li>Then, the IS MC estimator is defined as: \[\hat{I}_{IS}=\frac{1}N\sum_{n=1}^N\frac{f(X_n)p(X_n)}{q(X_n)}\]</li> </ul> <p>Importantly, the previous line guarantees that this will be an unbiased estimator for the original expectation! Additionally, if we select a “good” q this estimator will have a significantly lower variance. A nice bit of intuition here for those that know some measure theory is that we are really just performing a change of probability measure by rescaling the integrand by the Raydon-Nikodym derivative of p with respect to q.</p> <p>Now, it can be shown that the “optimal” (in the sense of maximally reducing variance) importance distribution q has a density given by: \[q(x) = \frac{|f(x)|p(x)}{\int{|f(x)|p(x)dx}}\]</p> <p>I’m not including this proof to stop this post from getting too long, but proving this just requires showing that the new integrand’s variance has a lower bound (via Jensen’s inequality) and then showing that this lower bound is achieved if we choose q as our proposal distribution.</p> <p>This optimal q makes a lot of intuitive sense - it says we want an importance distribution that assigns a high probability to samples being drawn from regions where both p(x) is high and f(x) is large in magnitude, since these are the regions that are important for evaluating expectations. Basically, if we use this optimal q we focus the samples we draw on important regions of the support and hence converge to the true value much faster.</p> <p>In practice, this optimal q can be pretty annoying to find, and we can often just use a distribution that boradly meets this desiderata instead.</p> <p>Now, let’s illustrate this with our truncated exponential distribution and the problematic f from above. Recall that the problem was that p placed most of its density on the left of the domain while the important regions for f where on the right. So, let’s try using an importance distribution that puts more density on the important regions of the domain. For instance, let’s set q to a trucated unit-scale Gaussian centered at 8: \[q(x)=N_{[0,10]}(x;8,1)\]</p> <p>We can see why this is a broadly sensible q by considering f, p and q all on one graph and noticing the better alignement of q to the important region of the domain.</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/is_dist.png"/> </p> <p>Given this better alignment, we expect faster convergence and reduced variance. This faster convergence is shown in the graph below, where we seem to have converged to the true value with 2,500 samples using the IS MC estimator. Recall that we hadn’t achieved this even within 25,000 samples for the vanilla MC estimator.</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/is_conv.png"/> </p> <p>For reference, here is the analogous graph for said 25,000 samples with the vanilla MC estimator:</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/high_energy_mc_est.png"/> </p> <p>Additionally, this estimator has a much lower variance. The graph below illustrates this by showing 5 independent runs of the IS estimator</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/is_var.png"/> </p> <p>Again, for reference, here is the associated graph for 5 independent runs of the vanilla MC graph, clearly illustrating that IS has reduced variance massively:</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/high_energy_var.png"/> </p> <p>In conclusion, importance sampling is pretty powerful and is fairly widely used, even in conjunction with more realistic methods (i.e. in MCMC contexts).</p> <h3 id="control-variates">Control Variates</h3> <p>Another option for dealing with “problematic” functions in the context of basic Monte Carlo is to use control variates. The idea here is that we find some function f for which we know both its expectation under our distribution p <em>and</em> that its vanilla MC estimator is correlated with the vanilla MC estimator of our function of interest f. Then, we use this known correlation to “correct” our approximation of f. We do this since, for example, if the MC estimator of g is less than its known true value then the positive correlation with the MC estimator of f means f’s estimate will also be an under-estimate. Hence, it is sensible to “scale it up”.</p> <p>More precisely, we define a control variate (CV) MC estimator as follows:</p> <ul> <li>Suppose we want to estimate: \[I_f=\mathbb{E}_{X \sim p}[f(X)] = \int{f(x)p(x)dx}\]</li> <li>Define the vanilla MC estimator \(\hat{I}_f\) as:</li> </ul> <p>\[\hat{I}_f=\frac{1}N\sum_{n=1}^Nf(X_n)\]</p> <ul> <li>Assume we have some function g for which we know the value of:</li> </ul> <p>\[I_g=\mathbb{E}_{X \sim p}[g(X)] = \int{g(x)p(x)dx}\]</p> <ul> <li>Further, define the associated vanilla MC estimator as \(\hat{I}_g\)</li> </ul> <p>\[\hat{I}_g=\frac{1}N\sum_{n=1}^Ng(X_n)\]</p> <ul> <li>Then, the control variate estimator of \(I_f\) is given by:</li> </ul> <p>\[\hat{I}_{CV} = \hat{I}_f + \beta[\hat{I}_g - I_g]\]</p> <p>So, why does this make sense as way of reducing the variance? Well, consider the variance of this new estimator:</p> <p>\[Var(\hat{I}_{CV}) = Var(\hat{I}_f) + \beta^2Var(\hat{I}_g) + 2\beta Cov(\hat{I}_f,\hat{I}_g) \]</p> <p>This is just a quadratic in \(\beta\). Minimising this gives the optimal \(\beta\) as:</p> <p>\[\beta=-\frac{Cov(\hat{I}_f,\hat{I}_g)}{Var(\hat{I}_g)}\]</p> <p>This formula makes sense - if the two estimators are positive correlated, the coefficient will be negative which corresponds to increasing the MC estimate of f whenever the mMC estimate of g is less than its true value. Plugging this back into the formula, we get a variance of the CV MC estimator of:</p> <p>\[Var(\hat{I}_{CV}) = Var(\hat{I}_f) - \frac{Cov(\hat{I}_f,\hat{I}_g)^2}{Var(\hat{I}_g)} \]</p> <p>Therefore, we get a reduction that is proportional to the covariance between \(\hat{I}_f\) and \(\hat{I}_g\)!</p> <p>Let’s illustrate this with the truncated exponential and the problematic f again with \(g(x)=10-x\) as our control variate. To get a sense of the covariance between \(\hat{I}_f\) and \(\hat{I}_g\), I have plotted them (alongside p) on the graph below:</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/cv_pdf.png"/> </p> <p>Now, this graphs tells us that \(\hat{I}_f\) and \(\hat{I}_g\) should be negatively correlated. This is because the “important regions” for f and g (or, the regions of f and g that take extreme values and hence contribute most to the expectation) are at opposite ends of the support of the distribution. For example, a sample that meaningfully increases \(\hat{I}_f\) will be a higher x value and hence will cause a proportionally smaller increase in \(\hat{I}_g\). This is all to say that samples that lead to over-estimates of the former MC estimator will be associated with under-estimates of the latter. Thus, we want to use a positive \(\beta\). Using \(\beta=0.5\), for example, yields rapid convergence within 2,000 samples as shown below:</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/cv_conv.png"/> </p> <p>Repeating the corresponding graph for 25,000 samples with the vanilla MC estimator again shows how this is a boost in performance:</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/high_energy_mc_est.png"/> </p> <p>And, using control variates yields a reduction in variance as shown in the figure below:</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/cv_var.png"/> </p> <p>Repeating the corresponding graph for the vanilla MC estimator again shows how this is huge reduction in variance:</p> <p align="center"> <img src="/assets/img/posts/2024-01-07/high_energy_var.png"/> </p> <h3 id="summary">Summary</h3> <p>In summary, Monte Carlo methods are extremely powerful methods for calculating expectations that are either impossible or intractable to compute analytically. Additionally, techniques such as importance sampling and control variates allow us to use MC estimators even when we have problematic functions.</p> <p>So, for the interested, why is understanding all of this so relevant to ML?</p> <ul> <li>In much of ML we cannot fully specify distributions of interest and so cannot analytically calculate expectations with respect to them. Monte Carlo methods can be used to overcome this problem if we can still sample from the distributions.</li> <li>Monte Carlo methods are combined with Markov chain methods in the suite of tools that are called “Markov Chain Monte Carlo” methods (MCMC). Markov chains allow us to sample from such distributions (even when we do not know their full explicit form), and we can then use Monte Carlo methods to marginalise out through the operation of taking expectations.</li> <li>Such MCMC methods are <em>very</em> important in modern in machine learning. Diffusion models, for instance, are underpinned by theory relating to MCMC through the use of “Langevin dynamics”.</li> <li>Finally, Monte Carlo methods are used heavily in modern RL due to the intractabality of analytically calculating the objective that we seek to maximise in RL. Control variates and importance sampling become <em>very</em> important when applying Monte Carlo methods to RL.</li> </ul>]]></content><author><name></name></author><category term="mcmc,"/><category term="statistics"/><summary type="html"><![CDATA[A common computation we face in machine learning is computing expectations. When working in \(\mathbb{R}^d\), for instance, we often face integrals of the form: \[\mathbb{E}_{X \sim p}[f(X)] = \int{f(x)p(x)dx}\] Sometimes we can compute these analytically (i.e. through the use of rules of integration). However, many such integrals we face in ML are such that providing an analytic solution is intractable or impossible. It is this exact situation that motivates Monte Carlo (MC) methods.]]></summary></entry></feed>