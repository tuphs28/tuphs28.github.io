<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Model-Free Planning | Thomas Bush </title> <meta name="author" content="Thomas Bush"> <meta name="description" content="Interpreting planning in model-free RL"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tuphs28.github.io/projects/interpplanning/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Model-Free Planning",
            "description": "Interpreting planning in model-free RL",
            "published": "April 05, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Thomas</span> Bush </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Research <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Model-Free Planning</h1> <p>Interpreting planning in model-free RL</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#0-tl-dr">0 - TL;DR</a> </div> <div> <a href="#1-introduction">1 - Introduction</a> </div> <div> <a href="#2-probing-for-planning-relevant-concepts">2 - Probing For Planning-Relevant Concepts</a> </div> <div> <a href="#3-does-the-agent-plan">3 - Does The Agent Plan?</a> </div> <div> <a href="#4-intervening-on-the-agent-s-plans">4 - Intervening on The Agent's Plans</a> </div> <div> <a href="#5-discussion">5 - Discussion</a> </div> <div> <a href="#6-conclusion">6 - Conclusion</a> </div> <div> <a href="#appendix">Appendix</a> </div> </nav> </d-contents> <p>In this post, I summarise my forthcoming paper in which we present evidence that a model-free reinforcement learning agent can learn to internally perform planning. Our post is organised as follows. In Section 0, I provide a high-level TL;DR. After this, in Section 1, I provide a brief introduction to our work. Sections 2, 3 and 4 then detail the three primary steps of our analysis. Finally, Sections 5 and 6 conclude with some discussion and high-level takeaways.</p> <h2 id="0---tldr">0 - TL;DR</h2> <ul> <li>Within modern AI, “planning” is typically associated with agents that have access to an explicit model of their environment. This naturally raises a question: can an agent learn to plan without such a world model?</li> <li>Guez et al. (2019) introduced Deep Repeated ConvLSTM (DRC) agents<d-cite key="guez2019investigation"></d-cite>. Past work has shown that, despite lacking an explicit world model, DRC agents <em>behave</em> in a manner that suggests they perform planning <d-cite key="garriga-alonso2024planning"></d-cite><d-cite key="chung2024predicting"></d-cite>. However, it was not previously known why DRC agents exhibit this behavior.</li> <li>In our paper, we seek to understand why DRC agents exhibit behavioural evidence of planning. Specifically, we provide evidence that strongly indicates that a Sokoban-playing DRC agent <em>internally</em> performs planning.</li> <li>We do this by using linear probes to locate representations of planning-relevant concepts within the agent’s activations. These concepts correspond to predictions made by the agent regarding the impact of its future actions on the environment.</li> <li>The agent appears to use its internal representations of these concepts to implement a search-based planning algorithm.</li> <li>We demonstrate that these internal plans influence the agent’s long-term behaviour in the way that would be expected. For example, we intervene on the agent’s activations to cause it to formulate and execute specific plans.</li> </ul> <h2 id="1---introduction">1 - Introduction</h2> <p>In the context of modern deep learning, “decision-time planning” – that is, the capacity of selecting immediate actions to perform by predicting and evaluating the consequences of different actions – is conventionally associated with model-based, AlphaZero-style agents. <d-footnote> Note that "planning" refers to two different phenomena in RL. Sometimes, "planning" refers to decision-time planning, that is, the capacity of selecting actions by predicting and evaluating the consequences of future actions. However, "planning" in RL can also refer to background planning. Background planning refers to an agent learning a better policy and/or value function by interacting with a world model during training. A classic example of background planning is Dyna. </d-footnote><d-footnote>In past work, decision-time planning has usually been defined as the process of interacting with an explicit world model to select actions associated with good long-term consequences. However, this definition presupposes that an agent has a world model. Thus, we pragmatically introduce the above characterization of decision-time planning for the purposes of studying planning in model-free agents. Note that this pragmatic characterisation mirrors model-based definitions of planning but relaxes the requirement for an explicit world model to the requirement that an agent predict consequences of future actions, regardless of the method used. </d-footnote> These agents predict and evaluate the consequences of different actions by interacting with an explicit model of their environment. <d-footnote> We understand an "explicit world model" as anything introduced for the purpose of approximating the dynamics of an environment. This covers both simulators that agents explicitly interact with to predict the consequences of their actions (e.g. AlphaZero), and inductive biases whereby network topologies are structured to reflect the application of some a planning algorimth to a world model (e.g. MCTSNets<d-cite key="guez2018learning"></d-cite>). </d-footnote> However, this naturally raises a question: can an agent learn to plan without relying on an explicit world model?</p> <p>We investigate this question in the context of a Deep Repeated ConvLSTM (DRC) agent – a type of generic model-free agent introduced by Guez et al. (2019)<d-cite key="guez2019investigation"></d-cite> – that is trained to play the game of Sokoban <d-footnote> Specifically, we focus on DRC agents trained in an actor-critic setting with IMPALA <d-cite key="espeholt2018impala"></d-cite> </d-footnote>. DRC agents are parameterized by a stack of ConvLSTM layers<d-cite key="shi2015convolutional"></d-cite> (i.e. LSTM layers with 3D recurrent states and convolutional connections) that perform multiple internal ticks of recurrent computation for each real time step. <d-footnote>At each time step $t$, a DRC agent passes the observed state $x_t$ through a convolutional encoder to produce an encoding $i_t \in \mathbb{R}^{H_0 \times W_0 \times G_0}$. This is then processed by $D$ ConvLSTM layers. At time $t$, the $d$-th ConvLSTM has a cell state $g_t^d \in \mathbb{R}^{H_d \times W_d \times G_d}$. Here, $H_d = W_d = 8$ and $G_d = 32$. Unlike standard recurrent networks, which perform a single tick of recurrent computation per time step, DRC agents perform $N$ ticks of recurrent computation per step. Here, $N=3$.</d-footnote> <d-footnote> The DRC architecture actually makes a few minor modifications to the basic ConvLSTM. Specifically, the DRC architecture (1) has a bottom-up skip connection that passes the observation encoding as an additional input to all layers; (2) has a top-down skip connection that passes the final-layer output at the prior tick as an additional input to the bottom layer at each tick; (3) uses spatial mean- and max-pooling.</d-footnote> The DRC agent we focus on in our work has three ConvLSTM layers that each perform three recurrent ticks for each time step in the environment. The computation performed by this agent is illustrated in Figure 1 below.</p> <p align="center"> <img src="/assets/img/projects/planning/drc_pic.png" style="width: 80%; height: auto;"> </p> <p style="font-size: 0.75em; font-style: italic; text-align: center;"> Figure 1: llustration of the DRC architecture. For each time step, the architecture encodes the input $x_t$ as a convolutional encoding $i_t$, passes it to a stack of 3 ConvLSTMs which perform three ticks of recurrent computation. The output of the final ConvLSTM after the final internal tick is then flattened, passed through an MLP and projected to produce policy logits $\pi_t$ and a value estimate $v_t$. </p> <p>Sokoban is a deterministic, episodic environment in which an agent must navigate around an 8x8 grid to push four boxes onto four targets. The agent can move a box by stepping onto the square it inhabits. If, for example, the agent steps up onto a square containing a box, the box is pushed one square up. Sokoban allows agents to push boxes in such a way that levels become permanently unsolvable. It is hence a hard domain<d-footnote>Sokoban is PSPACE-complete.<d-cite key="Culberson1997SokobanIP"></d-cite></d-footnote> and is a common benchmark environment when studying planning<d-cite key="hamrick2020role"></d-cite> An illustration of an agent playing Sokoban can be seen in Figure 2 below.<d-footnote>We use a version of Sokoban in which the agent observes a symbolic representation of the environment. In this representation, each square of a Sokoban board is represented as a one-hot vector denoting which of 7 possible states that square is in. However, for ease of inspection, we present all figures using pixel representations of Sokoban. </d-footnote></p> <p align="center"> <img src="/assets/img/projects/planning/example_sokoban.gif" style="width: 30%; height: auto;"> </p> <p style="font-size: 0.75em; font-style: italic; text-align: center;"> Figure 2: Illustration of Sokoban </p> <p>Despite lacking an explicit world model, DRC agents have been shown to behave as though they are performing decision-time planning when playing Sokoban. For example, DRC agents have been shown to:</p> <ul> <li>rival the performance of model-based agents like MuZero in strategic environments <d-cite key="chung2024predicting"></d-cite> </li> <li>perform better when given additional “thinking time” (i.e. extra test-time compute) prior to acting <d-cite key="guez2019investigation"></d-cite> </li> <li>perform actions that serve no purpose other than giving the agent extra “thinking” time <d-cite key="garriga-alonso2024planning"></d-cite>.</li> </ul> <p>An example of the agent solving a level it otherwise cannot solve when given “thinking time” is shown below. Specifically, in the below example, the agent (1) fails to solve the level by default, but (2) solves the level when given 3 steps of “thinking time” prior to acting.</p> <p align="center"> <img src="/assets/img/projects/planning/agent_comparison.gif" style="width: 50%; height: auto;"> </p> <p style="font-size: 0.75em; font-style: italic; text-align: center;"> Figure 3: Example of a DRC agent solving a level it otherwise cannot solve when given 3 steps of "thinking time" prior to acting. </p> <p>Yet, it was not previously known why DRC agents behave in this way: is this behaviour merely the result of complex learned heuristics, or do DRC agents truly learn to internally plan?</p> <p>In our work, we take a concept-based approach to interpreting a Sokoban-playing DRC agent and demonstrate that this agent is indeed internally performing planning. Specifically, we perform three steps of analysis:</p> <ol> <li>First, we use linear probes to decode representations of planning-relevant concepts from the agent’s activations (Section 2).</li> <li>Then, we investigate the manner in which these representations emerge at test-time. In doing so, we find qualitative evidence of the agent internally implementing a process that appears to be a form of search-based planning (Section 3).</li> <li>Finally, we confirm that these representations influence the agent’s behaviour in the manner that would be expected if they were used for planning. Specifically, we show that we can intervene on the agent’s representations to steer it to formulate and execute sub-optimal plans (Section 4).</li> </ol> <p>In performing our analysis, we provide the first non-behavioural evidence that it is possible for agents to learn to internally plan without relying on an explicit world model or planning algorithm.</p> <h2 id="2---probing-for-planning-relevant-concepts">2 - Probing For Planning-Relevant Concepts</h2> <p>How might an agent learn to plan in the absence of an explicit world model? We hypothesised that a natural way for such an agent to learn to plan would be for it to internally represent a collection of planning-relevant concepts. Note that we understand a “concept” to simply be a minimal piece of task-relevant knowledge <d-cite key="schut2023bridging"></d-cite>. By planning-relevant concepts we mean concepts corresponding to potential future actions the agent could take, and concepts relating to the consequences of these future actions on the environment.</p> <p>Two aspects of Sokoban are dynamic: the location of the agent, and the locations of the boxes. As such, we hypothesise that planning-capable agent in Sokoban would learn the following two concepts relating to individual squares of the 8x8 Sokoban board<d-footnote>In this paper, we specifically consider `multi-class' concepts, which can formally be defined as mappings from input states (or parts of input states) to some fixed classes.</d-footnote><d-footnote>Both of the concepts we study map each grid square of the agent's observed Sokoban board to the classes $\{\texttt{UP}, \texttt{DOWN}, \texttt{LEFT}, \texttt{RIGHT}, \texttt{NEVER}\}$. The directional classes correspond to the agent's movement directions. If the next time the agent steps onto a specific square, the agent steps onto that square from the left, the concept $C_A$ would map this square to the class $\texttt{LEFT}$. If the next time the agent pushes a box off of specific square, the box is pushed to the left, the concept $C_B$ would map this square to the class $\texttt{LEFT}$. Finally, the class $\texttt{NEVER}$ corresponds to the agent not stepping onto or pushing a box off of a square again for the remainder of the episode</d-footnote>:</p> <ul> <li> <strong>Agent Approach Direction (\(C_A\))</strong>: A concept that captures (i) whether an agent will step onto a square at any point in the future; if the agent will step onto a square this concept also captures (ii) which direction the agent will step onto this square from.</li> <li> <strong>Box Push Direction (\(C_B\)):</strong> A concept that captures (i) whether a box will be pushed off of this square at any point in the future; if a box will be pushed off of a square, this concept also captures (ii) which direction this box will be pushed.</li> </ul> <p>These concepts are illustrated in Figure 4 below.</p> <p align="center"> <img src="/assets/img/projects/planning/concept_explanation.gif" style="width: 70%; height: auto;"> </p> <p style="font-size: 0.75em; font-style: italic; text-align: center;"> Figure 4: Illustration of the two concepts we hypothesise that a planning-capable agent might Sokoban learn. </p> <p>We use linear probes – that is, linear classifiers trained to predict these concepts using the agent’s internal activations – to determine whether the Sokoban-playing DRC agent we investigate indeed represents these concepts<d-footnote>Linear probes are just standard linear classifiers. So, when predicting linear probe will compute a logit $l_k= w^T_kg$ for each class $k$ by projecting the associated activations $g \in \mathbb{R}^d$ along a class-specific vector $w_k \in \mathbb{R}^d$.</d-footnote>. Specifically, we hypothesise that the agent will learn a spatial correspondence between its 3D recurrent state and the Sokoban board. As such, we train linear probes that predict the concepts \(C_A\) and \(C_B\) for each square \((x,y)\) of a Sokoban board using the agent’s cell state activations at position \((x,y)\). We call these “1x1 probes”. We also trained probes that recieve predict concepts for each square \((x,y)\) using a 3x3 patch of the agent’s cell state activations around position \((x,y)\). We call this latter type of probes “3x3 probes”. Finally, as a baseline we trained 1x1 and 3x3 probes to predict square level concepts when recieving the agent’s observation as input.</p> <p>We measure the extent to which linear probes can accurately decode these concepts from the agent’s cell state using the Macro F1 score they achieve. <d-footnote>The Macro-F1 score is a multi-class generalisation of the F1 score. To calculate it, you calculate F1 scores when viewing each class as the positive class and then take the unweighted mean of these class-specific F1 scores. The F1 score for any binary classification task is the harmonic mean of the precision and recall. </d-footnote> The Macro F1 scores of 1x1 and 3x3 probes are shown below.</p> <p align="center"> <img src="/assets/img/projects/planning/proberes.png" style="width: 90%; height: auto;"> </p> <p style="font-size: 0.75em; font-style: italic; text-align: center;"> Figure 5: Macro F1s achieved by 1x1 and 3x3 probes when predicting (a) Agent Approach Direction and (b) Box Push Direction using the agent's cell state at each layer, or, for the baseline probes, using the observation. Error bars show ±1 standard deviation. </p> <p>Our linear probes are able to accurately predict both (1) agent approach direction and (2) box push direction for squares of Sokoban boards. For instance, when probing the agent’s cell state at all layers for these concepts, all macro F1 scores are greater than 0.8 (for agent approach direction) and 0.86 (for box push direction). In contrast, probes trained to predict agent approach direction and box push direction based on the agent’s observation of the Sokoban board, only get macro F1s of 0.2 and 0.25. We take this as evidence that the agent indeed (linearly) represents the two hypothesised planning-relevant concepts. Similarly, we take the relatively minimal increase in performance when moving from 1x1 to 3x3 probes as evidence that the agent indeed represents these concepts at localised spatial positons of its cell state.</p> <h2 id="3---does-the-agent-plan">3 - Does The Agent Plan?</h2> <p>So, the Deep Repeated ConvLSTM (DRC) agent internally represents the aformentioned planning-relevant, square-level concepts. How, then, does the agent use these representations to engage in planning?</p> <h3 id="31---internal-plans">3.1 - Internal Plans</h3> <p>We find that the agent uses its internal representations of Agent Approach Direction and Box Push Direction for each individual Sokoban square to formulate coherent planned paths to take around the Sokoban board, and to predict the consequences of taking these paths on the locations of boxes. That is, when we apply 1x1 probes to the agent’s cell state to predict \(C_A\) and \(C_B\) for every square of observed Sokoban boards, we find what appear to be (1) paths the agent expects to navigate along and (2) routes the agent expects to push boxes along. The examples in the below figure illustrate the agent’s internal plans (at each of its 3 layers) for six example levels.</p> <p align="center"> <img src="/assets/img/projects/planning/planning_ex.png" style="width: 100%; height: auto;"> </p> <p style="font-size: 0.75em; font-style: italic; text-align: center;"> Figure 6: Examples of internal plans decoded from the agent's cell state by a probe. Teal and purple arrows respectively indicate that a probe applied to the agent's cell state decodes that the agent expects to next step on to, or push a box off, a square from the associated direction. </p> <p>This, however, raises a question: how does the agent arrive at these internal plans? Is the agent merely remembering past sequences of actions that leads to good outcomes – that is, is it performing some form of heuristic-based lookup - or has the agent learned something more akin to a generalisable planning algorithm?</p> <h3 id="32---iterative-plan-refinement">3.2 - Iterative Plan Refinement</h3> <p>One way to go about answering this question is to look at what happens if we force the agent to pause and “think” prior to acting at the start of episodes. If the agent is merely performing something akin to heuristic-based lookup we would not expect the agent’s internal plan to necessarily get any more accurate when given extra “thinking time”. In contrast, if the agent were indeed performing some form of iterative planning, this is exactly what we would expect.</p> <p>To test this, we forced the agent to remain stationary and not perform any actions for the first 5 steps of 1000 episodes. Since the agent performs 3 internal ticks of computation for each real time step, this corresponds to giving the agent 15 additional computational ticks of “thinking time” before it has to act in these episodes. We then used 1x1 probes to decode the agent’s internal plan (in terms of both \(C_A\) and \(C_B\)) at each tick, and measured the correctness of the agent’s plan at each tick by measuring the macro F1 when using that plan to predict the agent’s actual future interactions with the environment. Figure 7 below shows that, as would be expected if the agent planned via an iterative search, the agent’s plans iteratively improve over the course of the 15 extra internal ticks of computation it performs when forced to remain still prior to acting. This is all to say that, when given extra time to think, the agent seems to iteratively refine its internal plan despite nothing in its training objective explicitly encouraging this!</p> <p align="center"> <img src="/assets/img/projects/planning/planrefinement.png" style="width: 70%; height: auto;"> </p> <p style="font-size: 0.75em; font-style: italic; text-align: center;"> Figure 7: Macro F1 when using 1x1 probes to decode $C_A$ and $C_B$ from the agent's final layer cell state at each of the additional 15 internal ticks performed by the agent when the agent is given 5 'thinking steps', averaged over 1000 episodes. </p> <h3 id="33---visualising-plan-formation">3.3 - Visualising Plan Formation</h3> <p>We can also investigate the question of whether the agent forms plans via some internal planning mechanism by qualitatively inspecting the manner in which the agent’s plans form. When visualising how the routes the agent plans to push boxes develop over the course of episodes, we observed a few recurring “plan-formation motifs”. These are described below. Note that, in the below examples, a blue arrow indicates that the agent expects to push a box off of a square in the associated direction. Additionally, note that we visualise the agent’s internal plan at each of the 3 computational ticks it performs each time step.</p> <ul> <li> <strong>Forward Planning</strong> - The agent frequently formulates its internal plan by iteratively extending planned routes forward from boxes. An example can be seen in Figure 8.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/projects/planning/motifs_for.gif" style="width: 50%; height: auto;" alt="An example episode"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 8: An example of an episode in which the agent iteratively constructs a plan by extending its plan forward from the lower-most box. </p> </div> <ul> <li> <strong>Backward Planning</strong> - The agent likewise often constructs its internal plan by iteratively extending planned routes backward from boxes. An example can be seen in Figure 9.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/projects/planning/motifs_back.gif" style="width: 50%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 9: An example of an episode in which the agent iteratively constructs a plan by extending its plan backward from the lower-right target. </p> </div> <ul> <li> <strong>Evaluative Planning</strong> - The agent sometimes (1) plans to push a box along a naively-appealing route to a target, (2) appears to evaluate the naively-appealing route and realise that pushing a box along it would make the level unsolvable, and (3) form an alternate, longer route connecting the box and target. An example can be seen in Figure 10.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/projects/planning/motifs_eval.gif" style="width: 50%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 10: An example of the agent appearing to evaluate, and subsequently modify, part of its plan. In this episode, the shortest path between the center-right box and center-most target is to push the box rightwards along the corridor. However, it is not feasible for the agent to push the box along this path, since doing so requires the agent to "get under the box" to push it up to the target. The agent cannot do this as pushing the box in this way blocks the corridor. </p> </div> <ul> <li> <strong>Adaptive Planning</strong> - the agent often initially plans to push multiple boxes to the same target before modifying its plan to push one of these boxes to an alternate target. An example can be seen in Figure 11.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/projects/planning/motifs_adapt.gif" style="width: 50%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 11: An example in which the agent initially plans to push two boxes to the same target (the left-most target) but modifies its plan such that one of the boxes is pushed to an alternate target. Note that, in this episode, the agent appears to realise that the upper-left box must be pushed to the left-most target as it, unlike the lower-left target, cannot be pushed to any other targets. </p> </div> <p>We think these motifs provide qualitative evidence that the agent forms plans using a planning algorithm that (i) is capable of evaluating plans, and that (ii) utilises a form of bidirectional search. This is interesting as the apparent use of bidirectional search indicates that the agent has learned to plan in a manner that is notably different from the forward planning algorithms commonly used in model-based RL<d-footnote>Most AlphaZero-style agents that engage in model-based planning rely on either MCTS or some other form of forward-rollout based planning</d-footnote>. We conjecture that the agent has learned to rely on bi-directional planning as it allows for efficient plan-formation in an environment such as Sokoban in which there are clear states to plan forward and backward from<d-footnote>Interestingly, one of the more capable RL agents not utilising deep learning also relies on a procedure that is similar to bidirectional planning<d-cite key="Culberson1997SokobanIP"></d-cite></d-footnote>.</p> <h3 id="34---planning-under-distribution-shift">3.4 - Planning Under Distribution Shift</h3> <p>If the agent does indeed form plans by performing a bidirectional, evaluative search, we would expect the agent to be able to continue to form coherent plans in levels drawn from different distributions to the levels it saw during training. This is because the agent’s learned search procedure would presumably be largely unaffected by distribution shift. In contrast, if the agent solely relied on memorised heuristics, we would expect the agent to be unable to coherently form plans in levels drawn from different distributions, as the memorised heuristics may no longer apply. Interestingly, the agent can indeed form coherent plans in levels drawn from different distributions. We now provide two examples of this.</p> <ul> <li> <strong>Blind Planning</strong> - The agent can frequently form plans to solve levels in levels in which the agent cannot see its own locations. An example can be seen in Figure 12.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/projects/planning/motifs_noag.gif" style="width: 50%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 12: An example of an episode in which the agent iteratively constructs a plan to push boxes to targets despite not being able to see its own location. </p> </div> <ul> <li> <strong>Generalised Planning</strong> - Despite being trained entirely on boxes with four boxes and four targets, the agent can succesfully form plans in levels with additional boxes and targets. An example can be seen in Figure 13.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/projects/planning/motifs_gen.gif" style="width: 50%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 13: An example of an episode in which the agent iteratively constructs a plan to push 6 boxes to 6 targets despite never having seen such a level during training. </p> </div> <h2 id="4---intervening-on-the-agents-plans">4 - Intervening on The Agent’s Plans</h2> <p>However, the evidence presented thus far only provides evidence that the agent constructs internal plans. It does not confirm that the agent’s apparent internal plans influence its behaviour in the way that would be expected if the agent were truly engaged in planning..</p> <p>As such, we performed intervention experiments to determine whether the agent’s representations of Agent Approach Direction and Box Push Direction influence the agent’s behaviour in the way that would be expected if these representations were indeed used for planning. Specifically, we performed interventions in two types of handcrafted levels:</p> <ul> <li>We designed levels in which the agent could take either a short or a long path to a region containing boxes and target. In these levels, we intervened to encourage the agent to take the long path.</li> <li>We designed levels in which the agent could push a box a short or a long route to a target. In these levels, we intervened to encourage the agent to push the box the long route.</li> </ul> <p>Our interventions consisted of adding the vectors learned by the probes to specific positions of the agent’s cell state with the aim of inducing the desired behaviour<d-footnote>Recall that a 1x1 probe projects activations along a vector $w_k \in \mathbb{R}^{32}$ to compute a logit for class $k$ of some multi-class concept $C$. We thus encourage the agent to represent square $(x,y)$ as class $k$ for concept $C$ by adding $w_k$ to position $(x,y)$ of the agent's cell state $g_{x,y}$:$$ g'_{x,y} \leftarrow g_{x,y} + w_k$$ If the agent indeed uses $C_A$ and $C_A$ for planning, altering the agent's square-level representations of these concepts ought to modify its internal plan and, subsequently, its long-term behavior.</d-footnote>. Our interventions were successful, with the agent being induced to take the long path in 98.8% of the former types of levels and 80.6% of the latter types of levels. Furthermore, when visualising the agent’s internal plans in levels with and without interventions, we can clearly see that the reason the agent’s behaviour changes after the intervention is because the intervention causes it to formulate an alternate internal plan. This would be expected if the agent indeed used its representations of Agent Approach Direction and Box Push Direction as part of an internal planning process. This is illustrated in Figures 14 and 15 below</p> <div style="text-align: center;"> <img src="/assets/img/projects/planning/interv_ag.gif" style="width: 80%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 14: An example of the effect of an intervention on the path that the agent plans to follow. Teal arrows correspond to squares that the agent expects to step onto, and the direction is expects to step onto them from. The left-most gif shows the agent's plan over the initial steps of an episode when no intervention is performed. The middle gif shows the squares intervened upon, with our interventions encouraging the agent to step onto the square with the white arrow, and discouraging the agent from stepping onto the squares with the white crosses. The right-most gif shows the agent's plan over the initial steps of an episode when the intervention is performed. </p> </div> <div style="text-align: center;"> <img src="/assets/img/projects/planning/interv_box.gif" style="width: 80%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 13: An example of the effect of an intervention on the route that the agent plans to push boxes. Purple arrows correspond to squares that the agent expects to push boxes off of, and the direction is expects to push them when it does so. The left-most gif shows the agent's plan over the initial steps of an episode when no intervention is performed. The middle gif shows the squares intervened upon, with our interventions encouraging the agent to push the box in the direction of the white arrow, and discouraging the agent from pushing boxes off of the squares with the white crosses. The right-most gif shows the agent's plan over the initial steps of an episode when the intervention is performed. </p> </div> <h2 id="5---discussion">5 - Discussion</h2> <p>What is the takeaway from all of this? Well, in seeming to plan via searching over potential future actions and their environmental impacts, the agent appears to have leaned to a plan in a manner analogous to model-based planning. However, this raises an obvious question: how can a nominally model-free agent have learned to plan in this way?</p> <p>We think the answer here is that the inductive bias of ConvLSTMs has encouraged the agent to learn to organise its internal representations in a manner that corresponds to a learned (implicit) environment model. Recall that the agent has learned a spatial correspondence between its cell states and the Sokoban grid such that the agent represents the spatially-localised concepts of Agent Approach Direction and Box Push Direction at the corresponding positions of its cell state. We contend this means that the agent’s 3D recurrent state can be seen as containing a learned “implicit” model of the environment. This is, of course, not a true world model in the sense that the agent is not explicitly approximating the full dynamics of the environment. However, the agent’s recurrent state does seemingly “do enough” to play the role of a world model in that it allows the agent to (i) formulate potential sequences of future actions and (ii) predict their environmental impacts.</p> <h2 id="6---conclusion">6 - Conclusion</h2> <p>Our work provides the first non-behavioural evidence that agents can learn to perform decision-time planning without either an explicit world model or an explicit planning algorithm. Specifically, we provide evidence that appears to indicate that a Sokoban-playing DRC agent internally performs a process with similarities to bi-directional search-based planning. This represents a further blurring of the classic distinction between model-based and model-free RL, and confirms that – at least with a specific architecture in a specific environment – model-free agents can learn to perform planning.</p> <p>However, our work leaves many questions unanswered. For instance, what are the conditions under which an agent can learn to plan without an explicit world model? For instance, as exemplified by its multiple internal recurrent ticks, the agent we study has a slightly atypical architecture. Likewise, the 3D nature of the agent’s ConvLSTM cell states means that the agent is especially well-suited to learning to plan in the context of Sokoban’s localised, grid-based transition dynamics. While we suspect our findings hold more broadly (see the Appendix to our paper and/or the Appendix to this blog post for more details), we leave it to future work to confirm this.</p> <h3 id="acknowledgements">Acknowledgements</h3> <p>This project would not have been possible without the amazing guidance and mentorship I have recieved from Stephen Chung, Usman Anwar, Adria Garriga-Alonso and David Krueger to who I am deeply grateful. I would also like to thank Alex Cloud for helpful feedback on this post.</p> <hr> <h2 id="appendix">Appendix</h2> <h3 id="a---planning-in-a-resnet-agent">A - Planning in a ResNet Agent</h3> <p>An obvious question to ask is whether the agent’s ability to plan is specific to the architecture of its ConvLSTM cell state. In an appendix to our paper, we provide evidence indicating that this is not the case. Specifically, we show that a 24-layer ResNet agent can learn to plan in Sokoban.</p> <p>As with the DRC agent, we train probes to predict the concepts ‘Agent Approach Direction’ and ‘Box Push Direction’ from the agent’s activations. The Macro F1 scores achieved by these probes are shown below.</p> <div style="text-align: center;"> <img src="/assets/img/projects/planning/resnet_f1s.png" style="width: 100%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 16: Macro F1s achieved by 1x1 and 3x3 probes when predicting (a) Agent Approach Direction and (b) Box Push Direction using the ResNet agent's activations after each of its 24 layers, or, for the baseline probes, using the observations. </p> </div> <p>Note that the 1x1 probes become iteratively more accurate over layers until a point at which the reverse trend begins. Specifically, the 1x1 probes for $C_B$ improve until about layer 10, whilst the probes for $C_A$ improve for longer until about layer 16. We hypothesize that this means that the ResNet agent is internally planning using spatially-localized concepts relating to box and agent movements, and that it is doing so by first determining how to move boxes, and then, afterward, reasoning about what that means for its own movements.</p> <p>Qualitative evidence of the agent iteratively refining its plan to push boxes over its initial layers is shown in the gifs below in Figure 18</p> <div class="row mt-4 mb-5"> <div class="col-sm-4"> <img src="/assets/img/projects/planning/resnet_box_1.gif" style="width: 100%;"> </div> <div class="col-sm-4"> <img src="/assets/img/projects/planning/resnet_box_2.gif" style="width: 100%;"> </div> <div class="col-sm-4"> <img src="/assets/img/projects/planning/resnet_box_3.gif" style="width: 100%;"> </div> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 18: The squares the ResNet agent plans to push boxes off of, over its initial 10 layers, as decoded by a 1x1 probe in three episodes. </p> </div> <h3 id="b---planning-in-mini-pacman">B - Planning in Mini Pacman</h3> <p>Another relevant question is whether the agent’s ability to plan is specific to the environment in which it is trained. In this section, we now provide preliminary results when investigating whether a DRC agent can learn to internally plan in a different environment: Mini PacMan.</p> <p>In Mini PacMan, an agent must navigate around walls in a grid-world and eat food. Initially, each non-wall square has food on, and levels end when the agent eats all food. However, the agent must also avoid ghosts which chase the agent. In each level, there are also ‘power pills’. When the agent steps onto a square with a power pill, ghosts flee, and the agent eats any ghosts it steps onto. An example of Mini PacMan is shown below.</p> <div style="text-align: center;"> <img src="/assets/img/projects/planning/pilleater_example.gif" style="width: 50%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 19: An example of a Mini PacMan episode. </p> </div> <p>We initially tried probing for the concept ‘Agent Approach Direction’ (CA) as in Sokoban but found little evidence of the agent representing it. After experimentation, however, we found probes to be able to decode the following concept from the agent’s cell state:</p> <ul> <li> <strong>Agent Approach Direction 16</strong>: This concept tracks which squares the agent will step off of, and which direction it will do so in, over the next 16 time steps.</li> <li> <strong>Agent Approach 16</strong>: This concept tracks which squares the agent will step off of over the next 16 time steps.</li> </ul> <p>We then probe the agent’s cell state for these concepts at each layer of the agent’s ConvLSTM. The Macro F1 scores achieved by these probes are shown below.</p> <div style="text-align: center;"> <img src="/assets/img/projects/planning/pacmanres.png" style="width: 100%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 20: Macro F1s achieved by 1x1 and 3x3 probes when predicting (a) Agent Approach Direction 16 and (b) Agent Approach 16 using the agent's cell state at each layer, or, for the baseline probes, using the observations. </p> </div> <p>We are unsure whether to interpret these results as indicating that (1) the agent possesses spatially-localized representations of the concept ‘Agent Approach 16’, (2) the agent posseses a representation of the concept ‘Agent Approach Direction 16’ distributed across adjacent positions of its cell state, or (3) the agent possesses a spatially-localized representation of a <em>correlate</em> of the concept ‘Agent Approach Direction 16’ that a 3x3 probe is able to pick up but a 1x1 probe is not.</p> <p>I lean towards the latter interpretation. This is because, when we visualise the predictions of a 3x3 probe trained to predict Agent Approach Direction 16, we see that the probe is able to pick up what appears to be another internal planning process.<d-footnote>Specifically, the agent appears to have learned to construct a plan by iteratively searching forward to different depths. I hypothesise that this means the `true' concept the agent plans in terms of is not Agent Approach Direction 16, but a concept that is correlated with it. I also hypothesise that the 3x3 probe is better able to pick up this true concept as it is more robust to the false positives and false negatives that training a probe to predict Agent Approach Direction 16 faces in such a scenario.</d-footnote> This is illustrated in the gifs below.<d-footnote>Note also that these decoded plans cannot really be explained by the agent internally representing merely which squares it plans to step onto and then the 3x3 probe guess the direction. This is because the probe correctly predicts the direction the agent will step on to a square even in cases where there are multiple feasible options.</d-footnote> Note that, in these gifs, I have adjusted the transparency of the arrows to match the confidence of the probe’s predictions.</p> <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 20px; text-align: center;"> <img src="/assets/img/projects/planning/pilleater_blogplanex_1.gif" style="width: 45%; height: auto;"> <img src="/assets/img/projects/planning/pilleater_blogplanex_2.gif" style="width: 45%; height: auto;"> <img src="/assets/img/projects/planning/pilleater_blogplanex_3.gif" style="width: 45%; height: auto;"> <img src="/assets/img/projects/planning/pilleater_blogplanex_4.gif" style="width: 45%; height: auto;"> </div> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 1em auto 0 auto;"> Figure 21: Four examples of the predictions of a 3x3 probe trained to predict Agent Approach Direction 16. The transparency of the arrows corresponds to the confidence of the probe’s predictions. </p> <p>An interesting observation here is that the agent appears to have learned an internal planning process that is completely different from the one it learns in Sokoban. Some notable features of this learned planning process are as follows:</p> <ul> <li>The agent seems to be primarily planning by searching <strong>forward</strong> from its current position.</li> <li>The agent’s search appears to be <strong>dynamic-horizon</strong> - that is, it appears to be searching forward to different depths over the course of the episode.</li> <li>The agent seems to frequently consider multiple plans before <strong>pruning</strong> most of them and selecting a single plan</li> <li>The agent’s <strong>confidence</strong> in its plan seems to meaningfuly vary. Specifically, the agent seems to be more confident about its short-term plans than its long-term plans, and seems to become more confident in its remaining plans after pruning most of its other plans.</li> </ul> <h3 id="c---planning-to-solve-mazes">C - Planning To Solve Mazes</h3> <p>Finally, I will outline some preliminary results regarding a DRC agent’s ability to solve mazes. These results were not included in the appendix due to its length, but I thought I would include them here as they are interesting.</p> <p>Specifically, I trained a DRC agent to solve mazes that varied in size between 5x5 and 25x25. An example of such a maze is shown below.</p> <div style="text-align: center;"> <img src="/assets/img/projects/planning/maze_ex.gif" style="width: 90%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 22: An example of a maze episode. </p> </div> <p>I then trained probes to predict which squares the agent would step onto when solving such mazes. Unsurprisingly given the above results, the probes were able to predict the concept with a high degree of accuracy, and are able to pick up on the agent’s internal planning process. This is illustrated in the gif below.</p> <div style="text-align: center;"> <img src="/assets/img/projects/planning/mazes_plan_id.gif" style="width: 90%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 23: An example of the predictions of a probe trained to predict what squares the agent plans to step onto. The squares the probe predicts the agent will step onto are marked with blue dots. </p> </div> <p>Now, it is perhaps interesting that the agent has learned an internal maze-solving algorithm that can be linearly decoded. However, what is much more interesting is that this algorithm can generalize far, far beyond the training distribution.</p> <p>Specifically, since the ConvLSTM backbone of the DRC agent is convolutional, we can apply every part of the agent up until its final MLP layer to an input of any size <d-footnote>The subsequent experimments were inspired by this observation being made in related work<d-cite key="garriga-alonso2024planning"></d-cite></d-footnote>. That is, we can feed observations of arbitrary sizes to the encoder and ConvLSTM backbone of the agent, and then apply our linear probes to the resulting cell states (which, as everything is convolutional, will be of the same size as the input) to decode the agent’s internal plan. Surprisingly, the agent maze-solving algorithm can generalize to mazes of sizes far beyond the training distribution. For instance, in the below gifs, the agent (when given sufficient test-time compute) internally forms a plans to solve mazes way beyond the size of the mazes in the training distribution.</p> <div style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap; text-align: center;"> <div style="flex: 0 1 45%;"> <img src="/assets/img/projects/planning/mazes_ex_ood_49_0.gif" style="width: 100%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; margin-top: 0.5em;"> Figure 24: The predictions of a probe trained to predict what squares the agent plans to step onto when the agent is given a 49x49 maze. </p> </div> <div style="flex: 0 1 45%;"> <img src="/assets/img/projects/planning/mazes_ex_ood_99_0.gif" style="width: 100%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; margin-top: 0.5em;"> Figure 25: The predictions of a probe trained to predict what squares the agent plans to step onto when the agent is given a 99x99 maze. </p> </div> </div> <div style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap; text-align: center;"> <div style="flex: 0 1 45%;"> <img src="/assets/img/projects/planning/mazes_ex_ood_149_0.gif" style="width: 100%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; margin-top: 0.5em;"> Figure 26: The predictions of a probe trained to predict what squares the agent plans to step onto when the agent is given a 149x149 maze. </p> </div> <div style="flex: 0 1 45%;"> <img src="/assets/img/projects/planning/mazes_ex_ood_199_0.gif" style="width: 100%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; margin-top: 0.5em;"> Figure 27: The predictions of a probe trained to predict what squares the agent plans to step onto when the agent is given a 199x199 maze. </p> </div> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/interpplanning.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Thomas Bush. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-home",title:"Home",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-research",title:"Research",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-improving-inflation-forecasts-with-bert",title:"Improving Inflation Forecasts With BERT",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/transfer-learning-inflation/"}},{id:"post-post-hoc-approaches-to-interpreting-reinforcement-learning-agents",title:"Post-Hoc Approaches to Interpreting Reinforcement Learning Agents",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/RLInterp/"}},{id:"post-a-very-brief-investigation-of-using-finetuning-to-interpret-wav2vec-2-0",title:"A Very Brief Investigation of Using Finetuning To Interpret Wav2Vec 2.0",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Finetuning-W2V/"}},{id:"post-basic-mcmc-pt-2-the-metropolis-hastings-algorithm",title:"Basic MCMC Pt. 2: The Metropolis-Hastings Algorithm",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Basic-MCMC/"}},{id:"post-bandit-algorithms-amp-the-exploration-exploitation-tradeoff",title:"Bandit Algorithms (&amp; The Exploration-Exploitation Tradeoff)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Bandit-Algorithms/"}},{id:"post-variational-dropout-in-recurrent-models",title:"Variational Dropout in Recurrent Models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Dropout-In-Recurrent-Models/"}},{id:"post-exploring-tradeoffs-between-safety-metrics-with-mnist",title:"Exploring Tradeoffs Between Safety Metrics with MNIST",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Exploring-Tradeoffs-Between-Safety-Metrics/"}},{id:"post-basic-mcmc-pt-1-an-intro-to-monte-carlo-methods",title:"Basic MCMC Pt. 1: An Intro to Monte Carlo Methods",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Monte-Carlo-Methods/"}},{id:"news-i-submitted-my-mphil-thesis",title:"I submitted my MPhil thesis!",description:"",section:"News"},{id:"projects-goal-interpretation-visualizations",title:"\ud83c\udfaf Goal Interpretation Visualizations",description:"",section:"Projects",handler:()=>{window.location.href="/projects/interpgoals/"}},{id:"projects-model-free-planning",title:"Model-Free Planning",description:"Interpreting planning in model-free RL",section:"Projects",handler:()=>{window.location.href="/projects/interpplanning/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%6F%75@%65%78%61%6D%70%6C%65.%63%6F%6D","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>