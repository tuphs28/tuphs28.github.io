<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Model-Free Planning | Thomas Bush </title> <meta name="author" content="Thomas Bush"> <meta name="description" content="Interpreting planning in model-free RL"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tuphs28.github.io/projects/interpplanning/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Model-Free Planning",
            "description": "Interpreting planning in model-free RL",
            "published": "December 30, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Thomas</span> Bush </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Research <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Model-Free Planning</h1> <p>Interpreting planning in model-free RL</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#0-tl-dr">0 - TL;DR</a> </div> <div> <a href="#1-introduction">1 - Introduction</a> </div> <div> <a href="#2-probing-for-planning-relevant-concepts">2 - Probing For Planning-Relevant Concepts</a> </div> <div> <a href="#3-does-the-agent-plan">3 - Does The Agent Plan?</a> </div> <div> <a href="#4-intervening-on-the-agent-s-plans">4 - Intervening on The Agent’s Plans</a> </div> <div> <a href="#5-conclusion">5 - Conclusion</a> </div> </nav> </d-contents> <p>In this post, I summarise my forthcoming paper in which we present evidence that a model-free reinforcement learning agent can learn to internally perform planning. Our post is organised as follows. In Section 0, I provide a high-level TL;DR. After this, in Section 1, I provide a brief introduction to our work. Sections 2, 3 and 4 then detail the three primary steps of our analysis. Finally, Section 5 concludes with some high-level takeaways.</p> <h2 id="0---tldr">0 - TL;DR</h2> <ul> <li>Within modern AI, “planning” is typically associated with agents that have access to an explicit model of their environment. This naturally raises a question: can an agent learn to plan without such a world model?</li> <li>Prior work introduced Deep Repeated ConvLSTM (DRC) agents<d-cite key="guez2019investigation"></d-cite>. Despite lacking an explicit world model, DRC agents exhibit planning-like behaviour<d-cite key="garriga-alonso2024planning"></d-cite>.</li> <li>In our paper, we provide evidence that strongly indicates that a Sokoban-playing DRC agent does indeed internally perform planning.</li> <li>We do this by using linear probes to locate representations of planning-relevant concepts within the agent’s activations. These concepts correspond to predictions made by the agent regarding the impact of its future actions on the environment.</li> <li>The agent seems to use these concepts to formulate internal plans using what appears to be a learned search-based planning algorithm.</li> <li>We demonstrate that these internal plans influence the long-term agent’s behaviour in the way that would be expected. For example, we intervene on the agent’s activations to cause it to formulate and execute specific plans.</li> </ul> <h2 id="1---introduction">1 - Introduction</h2> <p>In the context of modern artificial intelligence, “decision-time planning” – that is, the capacity of selecting immediate actions to perform by predicting and evaluating the consequences of different actions – is conventionally associated with model-based, AlphaZero-style agents. <d-footnote> Note that “planning” refers to two different phenomena in RL. Sometimes, “planning” refers to decision-time planning, that is, the capacity of selecting actions by predicting and evaluating the consequences of future actions. However, “planning” can also refer to background planning. Background planning refers to an agent learning a better policy and/or value function by interacting with a world model during training. A classic example of background planning is Dyna. </d-footnote><d-footnote>In past work, decision-time planning has usually been defined as the process of interacting with an explicit world model to select actions associated with good long-term consequences. However, this definition presupposes that an agent has a world model. Thus, we pragmatically introduce the above characterization of decision-time planning for the purposes of studying planning in model-free agents. </d-footnote> These agents predict and evaluate the consequences of different actions by interacting with an explicit model of their environment. <d-footnote> We understand an "explicit world model" as anything introduced for the purpose of approximating the dyanmics of an environment. This covers both simulators that agents explicitly interact with to predict the consequences of their actions (e.g. AlphaZero), and inductive biases whereby network topologies are structured to reflect the application of some a planning algorimth to a world model (e.g. MCTSNets<d-cite key="guez2018learning"></d-cite>). </d-footnote> However, this naturally raises a question: can an agent learn to plan without relying on an explicit world model?</p> <p>We investigate this question in the context of a Deep Repeated ConvLSTM (DRC) agent – a type of generic model-free agent introduced by Guez et al. (2019)<d-cite key="guez2019investigation"></d-cite> – that is trained to play the game of Sokoban. DRC agents are parameterized by a stack of ConvLSTM layers<d-cite key="shi2015convolutional"></d-cite> (i.e. LSTM layers with 3D recurrent states and convolutional connections) that perform multiple internal ticks of recurrent computation for each real time step.<d-footnote>At each time step $t$, a DRC agent passes the observed state $x_t$ through a convolutional encoder to produce an encoding $i_t \in \mathbb{R}^{H_0 \times W_0 \times G_0}$. This is then processed by $D$ ConvLSTM layers. At time $t$, the $d$-th ConvLSTM has a cell state $g_t^d \in \mathbb{R}^{H_d \times W_d \times G_d}$. Here, $H_d = W_d = 8$ and $G_d = 32$. Unlike standard recurrent networks, which perform a single tick of recurrent computation per time step, DRC agents perform $N$ ticks of recurrent computation per step. Here, $N=3$.</d-footnote> Specifically, the DRC agent we interpret performs three recurrent ticks for each time step in the environment.</p> <p>Sokoban is a deterministic, episodic environment in which an agent must navigate around an 8x8 grid to push four boxes onto four targets. The agent can move a box by stepping onto the square it inhabits. If, for example, the agent steps up onto a square containing a box, the box is pushed one square up. Sokoban allows agents to push boxes in such a way that levels become permanently unsolvable. It is hence a hard domain<d-footnote>Sokoban is PSPACE-complete.<d-cite key="Culberson1997SokobanIP"></d-cite></d-footnote> and is a common benchmark environment when studying planning<d-cite key="hamrick2020role"></d-cite> An illustration of an agent playing Sokoban can be seen in Figure 1 below.<d-footnote>We use a version of Sokoban in which the agent observes a symbolic representation of the environment. In this representation, each square of a Sokoban board is represented as a one-hot vector denoting which of 7 possible states that square is in. However, for ease of inspection, we present all figures using pixel representations of Sokoban. </d-footnote></p> <p align="center"> <img src="/assets/img/projects/planning/example_sokoban.gif" style="width: 30%; height: auto;"> </p> <p style="font-size: 0.75em; font-style: italic; text-align: center;"> Figure 1: Illustration of Sokoban </p> <p>Despite lacking an explicit world model, DRC agents have been shown to behave as though they are performing decision-time planning when playing Sokoban. For example, when trained to play Sokoban, DRC agents rival model-based agents like MuZero <d-cite key="chung2024predicting"></d-cite> and perform better when given additional ‘thinking’ time<d-cite key="garriga-alonso2024planning"></d-cite>. Yet, it was not previously known why DRC agents behave in this way: is this behaviour merely the result of complex learned heuristics, or do DRC agents truly learn to internally plan?</p> <p>In our work, we take a concept-based approach to interpreting a Sokoban-playing DRC agent and demonstrate that this agent is indeed internally performing planning. Specifically, we perform three steps of analysis:</p> <ol> <li>First, we use linear probes to decode representations of planning-relevant concepts from the agent’s activations (Section 2).</li> <li>Then, we investigate the manner in which these representations emerge at test-time. In doing so, we find qualitative evidence of the agent internally implementing a process that appears to be a form of search-based planning (Section 3).</li> <li>Finally, we confirm that these representations influence the agent’s behaviour in the manner that would be expected if they were used for planning. Specifically, we show that we can intervene on the agent’s representations to steer it to formulate and execute sub-optimal plans (Section 4).</li> </ol> <p>In performing our analysis, we provide the first non-behavioural evidence that it is possible for agents to learn to internally plan without relying on an explicit world model or planning algorithm.</p> <h2 id="2---probing-for-planning-relevant-concepts">2 - Probing For Planning-Relevant Concepts</h2> <p>How might an agent learn to plan in the absence of an explicit world model? We hypothesised that a natural way for such an agent to learn to plan would be for it to internally represent a collection of planning-relevant concepts. By planning-relevant concepts we mean concepts corresponding to potential future actions the agent could take, and concepts relating to the consequences of these future actions on the environment.</p> <p>Two aspects of Sokoban are dynamic: the location of the agent, and the locations of the boxes. As such, we hypothesise that planning-capable agent in Sokoban would learn the following two concepts relating to individual squares of the 8x8 Sokoban board<d-footnote>In this paper, we specifically consider `multi-class' concepts, which can formally be defined as mappings from input states (or parts of input states) to some fixed classes.</d-footnote><d-footnote>Both of the concepts we study map each grid square of the agent's observed Sokoban board to the classes $\{\texttt{UP}, \texttt{DOWN}, \texttt{LEFT}, \texttt{RIGHT}, \texttt{NEVER}\}$. The directional classes correspond to the agent's movement directions. If the next time the agent steps onto a specific square, the agent steps onto that square from the left, the concept $C_A$ would map this square to the class $\texttt{LEFT}$. If the next time the agent pushes a box off of specific square, the box is pushed to the left, the concept $C_B$ would map this square to the class $\texttt{LEFT}$. Finally, the class $\texttt{NEVER}$ corresponds to the agent not stepping onto or pushing a box off of a square again for the remainder of the episode</d-footnote>:</p> <ul> <li> <strong>Agent Approach Direction (\(C_A\))</strong>: A concept that captures (i) whether an agent will step onto a square at any point in the future; if the agent will step onto a square this concept also captures (ii) which direction the agent will step onto this square from.</li> <li> <strong>Box Push Direction (\(C_B\)):</strong> A concept that captures (i) whether a box will be pushed off of this square at any point in the future; if a box will be pushed off of a square, this concept also captures (ii) which direction this box will be pushed.</li> </ul> <p>We use linear probes – that is, linear classifiers trained to predict these concepts using the agent’s internal activations – to determine whether the Sokoban-playing DRC agent we investigate indeed represents these concepts<d-footnote>Linear probes are just standard linear classifiers. So, when predicting linear probe will compute a logit $l_k= w^T_kg$ for each class $k$ by projecting the associated activations $g \in \mathbb{R}^d$ along a class-specific vector $w_k \in \mathbb{R}^d$.</d-footnote>. Specifically, we hypothesise that the agent will learn a spatial correspondence between its 3D recurrent state and the Sokoban board. As such, we train linear probes that predict the concepts \(C_A\) and \(C_B\) for each square \((x,y)\) of a Sokoban board using the agent’s cell state activations at position \((x,y)\). We call these “1x1 probes”. We also trained probes that recieve predict concepts for each square \((x,y)\) using a 3x3 patch of the agent’s cell state activations around position \((x,y)\). We call this latter type of probes “3x3 probes”. Finally, as a baseline we trained 1x1 and 3x3 probes to predict square level concepts when recieving the agent’s observation as input.</p> <p>We measure the extent to which linear probes can accurately decode these concepts from the agent’s cell state using the Macro F1 score they achieve. <d-footnote>The Macro-F1 score is a multi-class generalisation of the F1 score. To calculate it, you calculate F1 scores when viewing each class as the positive class and then take the unweighted mean of these class-specific F1 scores. The F1 score for any binary classification task is the harmonic mean of the precision and recall. </d-footnote> The Macro F1 scores of 1x1 and 3x3 probes are shown below.</p> <p align="center"> <img src="/assets/img/projects/planning/proberes.png" style="width: 90%; height: auto;"> </p> <p style="font-size: 0.75em; font-style: italic; text-align: center;"> Figure 2: Macro F1s achieved by 1x1 and 3x3 probes when predicting (a) Agent Approach Direction and (b) Box Push Direction using the agent's cell state at each layer, or, for the baseline probes, using the observation. Error bars show ±1 standard deviation. </p> <p>Our linear probes are able to accurately predict both (1) agent approach direction and (2) box push direction for squares of Sokoban boards. For instance, when probing the agent’s cell state at all layers for these concepts, all macro F1 scores are greater than 0.8 (for agent approach direction) and 0.86 (for box push direction). In contrast, probes trained to predict agent approach direction and box push direction based on the agent’s observation of the Sokoban board, only get macro F1s of 0.2 and 0.25. We take this as evidence that the agent indeed (linearly) represents the two hypothesised planning-relevant concepts. Similarly, we take the relatively minimal increase in performance when moving from 1x1 to 3x3 probes as evidence that the agent indeed represents these concepts at localised spatial positons of its cell state.</p> <h2 id="3---does-the-agent-plan">3 - Does The Agent Plan?</h2> <p>So, the Deep Repeated ConvLSTM (DRC) agent internally represents the aformentioned planning-relevant, square-level concepts. How, then, does the agent use these representations to engage in planning?</p> <h3 id="31---internal-plans">3.1 - Internal Plans</h3> <p>We find that the agent uses its internal representations of Agent Approach Direction and Box Push Direction for each individual Sokoban square to formulate coherent planned paths to take around the Sokoban board, and to predict the consequences of taking these paths on the locations of boxes. That is, when using 1x1 probes to predict the agent’s representations of \(C_A\) and \(C_B\) for entire observed Sokoban boards, we find that the agent uses these concepts to internally formulate (1) planned paths to move along and (2) planned routes to push boxes along. Examples (a)-(c) in the below figure provide examples of the agent forming “internal plans” in this way in three example levels.</p> <p align="center"> <img src="/assets/img/projects/planning/planning_ex.png" style="width: 100%; height: auto;"> </p> <p style="font-size: 0.75em; font-style: italic; text-align: center;"> Figure 3: Examples of internal plans decoded from the agent’s cell state by a probe. Teal and purple arrows respectively indicate that a probe applied to the agent’s cell state decodes that the agent expects to next step on to, or push a box off, a square from the associated direction. </p> <p>This, however, raises a question: how does the agent arrive at these internal plans? Is the agent merely remembering past sequences of actions that leads to good outcomes – that is, is it performing some form of heuristic-based lookup - or has the agent learned something more akin to a generalisable planning algorithm?</p> <h3 id="32---iterative-plan-refinement">3.2 - Iterative Plan Refinement</h3> <p>One way to go about answering this question is to look at what happens if we force the agent to pause and “think” prior to acting at the start of episodes. If the agent is merely performing something akin to heuristic-based lookup we would not expect the agent’s internal plan to necessarily get any more accurate when given extra “thinking time”. In contrast, if the agent were indeed performing some form of iterative planning, this is exactly what we would expect.</p> <p>To test this, we forced the agent to remain stationary and not perform any actions for the first 5 steps of 1000 episodes. Since the agent performs 3 internal ticks of computation for each real time step, this corresponds to giving the agent 15 additional computational ticks of “thinking time” before it has to act in these episodes. We then used 1x1 probes to decode the agent’s internal plan (in terms of both \(C_A\) and \(C_B\)) at each tick, and measured the correctness of the agent’s plan at each tick by measuring the macro F1 when using that plan to predict the agent’s actual future interactions with the environment. Figure 4 below shows that, as would be expected if the agent planned via an iterative search, the agent’s plans iteratively improve over the course of the 15 extra internal ticks of computation it performs when forced to remain still prior to acting. This is all to say that, when given extra time to think, the agent seems to iteratively refine its internal plan despite nothing in its training objective explicitly encouraging this!</p> <p align="center"> <img src="/assets/img/projects/planning/planrefinement.png" style="width: 70%; height: auto;"> </p> <p style="font-size: 0.75em; font-style: italic; text-align: center;"> Figure 4: Macro F1 when using 1x1 probes to decode $C_A$ and $C_B$ from the agent’s final layer cell state at each of the additional 15 internal ticks performed by the agent when the agent is given 5 ‘thinking steps’, averaged over 1000 episodes. </p> <h3 id="33---visualising-plan-formation">3.3 - Visualising Plan Formation</h3> <p>We can also investigate the question of whether the agent forms plans via some internal planning mechanism by qualitatively inspecting the manner in which the agent’s plans form. When visualising how the routes the agent plans to push boxes develop over the course of episodes, we observed a few recurring “plan-formation motifs”. These are described below. Note that, in the below examples, a blue arrow indicates that the agent expects to push a box off of a square in the associated direction. Additionally, note that we visualise the agent’s internal plan at each of the 3 computational ticks it performs each time step.</p> <ul> <li> <strong>Forward Planning</strong> - The agent frequently formulates its internal plan by iteratively extending planned routes forward from boxes. An example can be seen in Figure 5.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/projects/planning/motifs_for.gif" style="width: 50%; height: auto;" alt="An example episode"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 5: An example of an episode in which the agent iteratively constructs a plan by extending its plan forward from the lower-most box. </p> </div> <ul> <li> <strong>Backward Planning</strong> - The agent likewise often constructs its internal plan by iteratively extending planned routes backward from boxes. An example can be seen in Figure 6.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/projects/planning/motifs_back.gif" style="width: 50%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 6: An example of an episode in which the agent iteratively constructs a plan by extending its plan backward from the lower-right target. </p> </div> <ul> <li> <strong>Evaluative Planning</strong> - The agent sometimes (1) plans to push a box along a naively-appealing route to a target, (2) appears to evaluate the naively-appealing route and realise that pushing a box along it would make the level unsolvable, and (3) form an alternate, longer route connecting the box and target. An example can be seen in Figure 7.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/projects/planning/motifs_eval.gif" style="width: 50%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 7: An example of the agent appearing to evaluate, and subsequently modify, part of its plan. In this episode, the shortest path between the center-right box and center-most target is to push the box rightwards along the corridor. However, it is not feasible for the agent to push the box along this path, since doing so requires the agent to "get under the box" to push it up to the target. The agent cannot do this as pushing the box in this way blocks the corridor. </p> </div> <ul> <li> <strong>Adaptive Planning</strong> - the agent often initially plans to push multiple boxes to the same target before modifying its plan to push one of these boxes to an alternate target. An example can be seen in Figure 8.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/projects/planning/motifs_adapt.gif" style="width: 50%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 8: An example in which the agent initially plans to push two boxes to the same target (the left-most target) but modifies its plan such that one of the boxes is pushed to an alternate target. Note that, in this episode, the agent appears to realise that the upper-left box must be pushed to the left-most target as it, unlike the lower-left target, cannot be pushed to any other targets. </p> </div> <p>We think these motifs provide qualitative evidence that the agent forms plans using a planning algorithm that (i) is capable of evaluating plans, and that (ii) utilises a form of bidirectional search. This is interesting as the apparent use of bidirectional search indicates that the agent has learned to plan in a manner that is notably different from the forward planning algorithms commonly used in model-based RL<d-footnote>Most AlphaZero-style agents that engage in model-based planning rely on either MCTS or some other form of forward-rollout based planning</d-footnote>. We conjecture that the agent has learned to rely on bi-directional planning as it allows for efficient plan-formation in an environment such as Sokoban in which there are clear states to plan forward and backward from<d-footnote>Interestingly, one of the more capable RL agents not utilising deep learning also relies on a procedure that is similar to bidirectional planning<d-cite key="Culberson1997SokobanIP"></d-cite></d-footnote>.</p> <h3 id="34---planning-under-distribution-shift">3.4 - Planning Under Distribution Shift</h3> <p>If the agent does indeed form plans by performing a bidirectional, evaluative search, we would expect the agent to be able to continue to form coherent plans in levels drawn from different distributions to the levels it saw during training. This is because the agent’s learned search procedure would presumably be largely unaffected by distribution shift. In contrast, if the agent solely relied on memorised heuristics, we would expect the agent to be unable to coherently form plans in levels drawn from different distributions, as the memorised heuristics may no longer apply. Interestingly, the agent can indeed form coherent plans in levels drawn from different distributions. We now provide two examples of this.</p> <ul> <li> <strong>Blind Planning</strong> - The agent can frequently form plans to solve levels in levels in which the agent cannot see its own locations. An example can be seen in Figure 9.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/projects/planning/motifs_noag.gif" style="width: 50%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 9: An example of an episode in which the agent iteratively constructs a plan to push boxes to targets despite not being able to see its own location. </p> </div> <ul> <li> <strong>Generalised Planning</strong> - Despite being trained entirely on boxes with four boxes and four targets, the agent can succesfully form plans in levels with additional boxes and targets. An example can be seen in Figure 10.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/projects/planning/motifs_gen.gif" style="width: 50%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 10: An example of an episode in which the agent iteratively constructs a plan to push 6 boxes to 6 targets despite never having seen such a level during training. </p> </div> <h2 id="4---intervening-on-the-agents-plans">4 - Intervening on The Agent’s Plans</h2> <p>However, the evidence presented thus far only provides evidence that the agent constructs internal plans. It does not confirm that the agent’s apparent internal plans influence its behaviour in the way that would be expected if the agent were truly engaged in planning..</p> <p>As such, we performed intervention experiments to determine whether the agent’s representations of Agent Approach Direction and Box Push Direction influence the agent’s behaviour in the way that would be expected if these representations were indeed used for planning. Specifically, we performed two types of interventions in handcrafted levels:</p> <ul> <li>In levels where the agent could take either a short or a long path to a region containing boxes and targets, we intervened to encourage the agent to take the long path.</li> <li>In levels where the agent could push a box a short or a long route to a target, we intervened to encourage the agent to push the box the long route.</li> </ul> <p>Our interventions consisted of adding the vectors learned by the probes to specific positions of the agent’s cell state with the aim of inducing the desired behaviour<d-footnote>Recall that a 1x1 probe projects activations along a vector $w_k \in \mathbb{R}^{32}$ to compute a logit for class $k$ of some multi-class concept $C$. We thus encourage the agent to represent square $(x,y)$ as class $k$ for concept $C$ by adding $w_k$ to position $(x,y)$ of the agent's cell state $g_{x,y}$:$$ g'_{x,y} \leftarrow g_{x,y} + w_k$$. If the agent indeed uses $C_A$ and $C_A$ for planning, altering the agent's square-level representations of these concepts ought to modify its internal plan and, subsequently, its long-term behavior.</d-footnote>. Our interventions were successful, with the agent being induced to take the long path in 98.8% of the former types of levels and 80.6% of the latter types of levels. Furthermore, when visualising the agent’s internal plans in levels with and without interventions, we can clearly see that the reason the agent’s behaviour changes after the intervention is because the intervention causes it to formulate an alternate internal plan. This would be expected if the agent indeed used its representations of Agent Approach Direction and Box Push Direction as part of an internal planning process. This is illustrated in Figures 11 and 12 below</p> <div style="text-align: center;"> <img src="/assets/img/projects/planning/interv_ag.gif" style="width: 80%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 11: An example of the effect of an intervention on the path that the agent plans to follow. Teal arrows correspond to squares that the agent expects to step onto, and the direction is expects to step onto them from. The left-most gif shows the agent's plan over the initial steps of an episode when no intervention is performed. The middle gif shows the squares intervened upon, with our interventions encouraging the agent to step onto the square with the white arrow, and discouraging the agent from stepping onto the squares with the white crosses. The right-most gif shows the agent's plan over the initial steps of an episode when the intervention is performed. </p> </div> <div style="text-align: center;"> <img src="/assets/img/projects/planning/interv_box.gif" style="width: 80%; height: auto;"> <p style="font-size: 0.75em; font-style: italic; text-align: center; width: 90%; margin: 0 auto;"> Figure 12: An example of the effect of an intervention on the route that the agent plans to push boxes. Purple arrows correspond to squares that the agent expects to push boxes off of, and the direction is expects to push them when it does so. The left-most gif shows the agent's plan over the initial steps of an episode when no intervention is performed. The middle gif shows the squares intervened upon, with our interventions encouraging the agent to push the box in the direction of the white arrow, and discouraging the agent from pushing boxes off of the squares with the white crosses. The right-most gif shows the agent's plan over the initial steps of an episode when the intervention is performed. </p> </div> <h1 id="5---conclusion">5 - Conclusion</h1> <p>Our work provides the first non-behavioural evidence that agents can learn to perform decision-time planning without either an explicit world model or an explicit planning algorithm. Specifically, we provide evidence that appears to indicate that the agent we study internally performs a process with similarities to bi-directional search based planning. In seeming to plan via searching over potential future actions and their environmental impacts, the agent appears to have leaned to a plan in a manner analogous to model-based planning.</p> <p>However, this raises an obvious question: how can a nominally model-free agent have learned to plan in this way? We think the answer here is that the inductive bias of ConvLSTMs has encouraged the agent to learn to organise its internal representations in a manner that corresponds to a learned (implicit) environment model. Recall that the agent has learned a spatial correspondence between its cell states and the Sokoban grid such that the agent represents the spatially-localised concepts of Agent Approach Direction and Box Push Direction at the corresponding positions of its cell state. We contend this means that the agent’s 3D recurrent state can be seen as containing a learned “implicit” model of the environment. This is, of course, not a true world model in the sense that the agent is not explicitly approximating the full dynamics of the environment. However, the agent’s recurrent state does seemingly play the role of a world model in that it allows the agent to (i) formulate potential sequences of future actions and (ii) predict their environmental impacts.</p> <h3 id="acknowledgements">Acknowledgements</h3> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/interpplanning.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Thomas Bush. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-home",title:"Home",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-research",title:"Research",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-improving-inflation-forecasts-with-bert",title:"Improving Inflation Forecasts With BERT",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/transfer-learning-inflation/"}},{id:"post-post-hoc-approaches-to-interpreting-reinforcement-learning-agents",title:"Post-Hoc Approaches to Interpreting Reinforcement Learning Agents",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/RLInterp/"}},{id:"post-a-very-brief-investigation-of-using-finetuning-to-interpret-wav2vec-2-0",title:"A Very Brief Investigation of Using Finetuning To Interpret Wav2Vec 2.0",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Finetuning-W2V/"}},{id:"post-basic-mcmc-pt-2-the-metropolis-hastings-algorithm",title:"Basic MCMC Pt. 2: The Metropolis-Hastings Algorithm",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Basic-MCMC/"}},{id:"post-bandit-algorithms-amp-the-exploration-exploitation-tradeoff",title:"Bandit Algorithms (&amp; The Exploration-Exploitation Tradeoff)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Bandit-Algorithms/"}},{id:"post-variational-dropout-in-recurrent-models",title:"Variational Dropout in Recurrent Models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Dropout-In-Recurrent-Models/"}},{id:"post-exploring-tradeoffs-between-safety-metrics-with-mnist",title:"Exploring Tradeoffs Between Safety Metrics with MNIST",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Exploring-Tradeoffs-Between-Safety-Metrics/"}},{id:"post-basic-mcmc-pt-1-an-intro-to-monte-carlo-methods",title:"Basic MCMC Pt. 1: An Intro to Monte Carlo Methods",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Monte-Carlo-Methods/"}},{id:"news-i-submitted-my-mphil-thesis",title:"I submitted my MPhil thesis!",description:"",section:"News"},{id:"projects-model-free-planning",title:"Model-Free Planning",description:"Interpreting planning in model-free RL",section:"Projects",handler:()=>{window.location.href="/projects/interpplanning/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%6F%75@%65%78%61%6D%70%6C%65.%63%6F%6D","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>